<!DOCTYPE html>
<html lang=" en ">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Convolution Neural Network : A Modular Perspective | State of the art</title>
<meta name="generator" content="Jekyll v4.1.1">
<meta property="og:title" content="Convolution Neural Network : A Modular Perspective">
<meta name="author" content="Christopher Olah">
<meta property="og:locale" content="en_US">
<meta name="description" content="Source : https://colah.github.io/posts/2014-07-Conv-Nets-Modular/">
<meta property="og:description" content="Source : https://colah.github.io/posts/2014-07-Conv-Nets-Modular/">
<link rel="canonical" href="http://hmthanh.github.io/blog/deep-learning/2019/04/25/ConvNets-Modular.html">
<meta property="og:url" content="http://hmthanh.github.io/blog/deep-learning/2019/04/25/ConvNets-Modular.html">
<meta property="og:site_name" content="State of the art">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-04-25T07:04:17+07:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Convolution Neural Network : A Modular Perspective">
<script type="application/ld+json">
{"headline":"Convolution Neural Network : A Modular Perspective","dateModified":"2019-04-25T07:04:17+07:00","datePublished":"2019-04-25T07:04:17+07:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://hmthanh.github.io/blog/deep-learning/2019/04/25/ConvNets-Modular.html"},"url":"http://hmthanh.github.io/blog/deep-learning/2019/04/25/ConvNets-Modular.html","author":{"@type":"Person","name":"Christopher Olah"},"description":"Source : https://colah.github.io/posts/2014-07-Conv-Nets-Modular/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css">
<link type="application/atom+xml" rel="alternate" href="http://hmthanh.github.io/blog/feed.xml" title="State of the art">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <main class="page-content" aria-label="Content">
    <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

            <header class="post-header">
                <h1 class="post-title p-name" itemprop="name headline" id="title">Convolution Neural Network : A Modular Perspective</h1>
                <a class="back" href="/blog/"><svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512"><path d="M223.7 239l136-136c9.4-9.4 24.6-9.4 33.9 0l22.6 22.6c9.4 9.4 9.4 24.6 0 33.9L319.9 256l96.4 96.4c9.4 9.4 9.4 24.6 0 33.9L393.7 409c-9.4 9.4-24.6 9.4-33.9 0l-136-136c-9.5-9.4-9.5-24.6-.1-34zm-192 34l136 136c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9L127.9 256l96.4-96.4c9.4-9.4 9.4-24.6 0-33.9L201.7 103c-9.4-9.4-24.6-9.4-33.9 0l-136 136c-9.5 9.4-9.5 24.6-.1 34z"></path></svg>Back Home</a>
                <p class="post-meta">
                    <time class="dt-published" datetime="2019-04-25T07:04:17+07:00" itemprop="datePublished">Apr 25, 2019
            </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Christopher Olah</span></span></p>
                    <div style="clear:both"></div>
            </header>
            

            <div class="post-content e-content" itemprop="articleBody">
                <p>Source : <a href="https://colah.github.io/posts/2014-07-Conv-Nets-Modular/">https://colah.github.io/posts/2014-07-Conv-Nets-Modular/</a></p>

<!--more-->

<ul class="table-of-content" id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#structure-of-convolutional-neural-networks" id="markdown-toc-structure-of-convolutional-neural-networks">Structure of Convolutional Neural Networks</a></li>
  <li><a href="#results-of-convolutional-neural-networks" id="markdown-toc-results-of-convolutional-neural-networks">Results of Convolutional Neural Networks</a></li>
  <li><a href="#formalizing-convolutional-neural-networks" id="markdown-toc-formalizing-convolutional-neural-networks">Formalizing Convolutional Neural Networks</a></li>
  <li><a href="#acknowledgments" id="markdown-toc-acknowledgments">Acknowledgments</a></li>
</ul>

<h1 id="introduction">Introduction</h1>

<!-- Trong vài năm gần đây, mạng neural networks đã có kết quả đột phá trên rất nhiều vấn đề nhận dạng mẫu như thị giác máy tính, nhận dạng giọng nói. Một trong những yếu tố cơ bản dẫn đến kết quả này là
một loại đặc biệt của neural network được gọi là *convolution neural network* (mạng neural tích chập) .

Cơ bản mà nói, convolution neural network có thể được xem như một loại mạng neural mà sử dụng rất nhiều bản sao chép giống hệt nhau của cùng một neuron.
Điều này cho phép network này có thể có nhiều neurons và thực hiện tính toán mô hình lớn trong khi vẫn giữ được số lượng tham số thực sự - giá trị mô tả cách mà những neuron có - mà cần được học không thiên vị.
-->

<p>In the last few years, deep neural networks have lead to breakthrough results on a variety of pattern recognition problems, such as computer vision and voice recognition. One of the essential components leading to these results has been a special kind of neural network called a convolutional neural network.</p>

<p>At its most basic, convolutional neural networks can be thought of as a kind of neural network that uses many identical copies of the same neuron<sup><a href="#e675:fn:1" class="footnote" id="e675:fn-back:1">1</a></sup>. This allows the network to have lots of neurons and express computationally large models while keeping the number of actual parameters – the values describing how neurons behave – that need to be learned fairly small.</p>

<p style="width: 100%;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-9x5-Conv2Conv2.png" alt="Conv2Conv2"></p>

<p class="center"><em>A 2D Convolutional Neural Network</em></p>

<p><br>
This trick of having multiple copies of the same neuron is roughly analogous to the abstraction of functions in mathematics and computer science. When programming, we write a function once and use it in many places – not writing the same code a hundred times in different places makes it faster to program, and results in fewer bugs. Similarly, a convolutional neural network can learn a neuron once and use it in many places, making it easier to learn the model and reducing error.</p>

<!-- Kỹ thuật để sao chép nhiều lần của cùng một neuron được hiểu đại khác tương tự như quá trình trừu tượng hóa một hàm trong toán học và khoa học máy tính. Khi lập trình, chúng ta viết hàm một lần và sử dụng chúng ở nhiều nơi - chúng ta không viết cùng một đoạn code hàng trăm lần khác nhau
khiến cho chúng nhanh hơn để lập trình và kết quả ít lỗi. Tương tự, một mạng neurol tích chập có thể học một neuron một lần và sử dụng
nó ở nhiều nơi, làm cho chúng dễ dàng hơn để học mô hình và giảm thiểu lỗi.

# Cấu trúc của những mạng neurol tích chập (Convolution neural networks)
-->

<h1 id="structure-of-convolutional-neural-networks">Structure of Convolutional Neural Networks</h1>

<p>Suppose you want a neural network to look at audio samples and predict whether a human is speaking or not. Maybe you want to do more analysis if someone is speaking.</p>

<p>You get audio samples at different points in time. The samples are evenly spaced.</p>

<p style="width: 100%;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-xs.png" alt="Conv-9-xs"></p>

<p>The simplest way to try and classify them with a neural network is to just connect them all to a fully-connected layer.
There are a bunch of different neurons, and every input connects to every neuron.</p>

<p style="width: 90%; padding: 20px 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-F.png" alt="Conv-9-F"></p>

<p style="width: 100%; padding: 20px 10px;" class="center"></p>

<p>A more sophisticated approach notices a kind of <em>symmetry</em> in the properties it’s useful to look for in the data. We care a lot about local properties of the data: What frequency of sounds are there around a given time? Are they increasing or decreasing? And so on.</p>

<p>We care about the same properties at all points in time. It’s useful to know the frequencies at the beginning, it’s useful to know the frequencies in the middle, and it’s also useful to know the frequencies at the end. Again, note that these are local properties, in that we only need to look at a small window of the audio sample in order to determine them.</p>

<p>So, we can create a group of neurons, $A$, that look at small time segments of our data<sup><a href="#e675:fn:2" class="footnote" id="e675:fn-back:2">2</a></sup> $A$ looks at all such segments, computing certain features.
Then, the output of this convolutional layer is fed into a fully-connected layer, $F$.</p>

<p style="width: 90%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-Conv2.png" alt="Conv-9-Conv2"></p>

<p>In the above example, $A$ only looked at segments consisting of two points. This isn’t realistic. Usually, a convolution layer’s window would be much larger.</p>

<p>In the following example, $A$ looks at 3 points.
That isn’t realistic either – sadly, it’s tricky to visualize $A$ connecting to lots of points.</p>

<p style="width: 90%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-Conv3.png" alt="Conv-9-Conv3"></p>

<p>One very nice property of convolutional layers is that they’re composable. You can feed the output of one convolutional layer into another. With each layer, the network can detect higher-level, more abstract features.</p>

<p>In the following example, we have a new group of neurons, $B$. $B$ is used to create another convolutional layer stacked on top of the previous one.</p>

<p style="width: 90%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-Conv2Conv2.png" alt="Conv-9-Conv2Conv2"></p>

<p>Convolutional layers are often interweaved with pooling layers. In particular, there is a kind of layer called a max-pooling layer that is extremely popular.</p>

<p>Often, from a high level perspective, we don’t care about the precise point in time a feature is present. If a shift in frequency occurs slightly earlier or later, does it matter?</p>

<p>A max-pooling layer takes the maximum of features over small blocks of a previous layer. The output tells us if a feature was present in a region of the previous layer, but not precisely where.</p>

<p>Max-pooling layers kind of “zoom out”. They allow later convolutional layers to work on larger sections of the data, because a small patch after the pooling layer corresponds to a much larger patch before it. They also make us invariant to some very small transformations of the data.</p>

<p style="width: 90%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-Conv2Max2Conv2.png" alt="Conv-9-Conv2Max2Conv2"></p>

<p><br></p>

<p style="width: 40%; padding: 20 10px; float:right;"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-unit.png" alt="Conv2-unit"></p>

<p>In our previous examples, we’ve used 1-dimensional convolutional layers. However, convolutional layers can work on higher-dimensional data as well. In fact, the most famous successes of convolutional neural networks are applying 2D convolutional neural networks to recognizing images.</p>

<p>In a 2-dimensional convolutional layer, instead of looking at segments, $A$ will now look at patches.</p>

<p>For each patch, $A$ will compute features. For example, it might learn to detect the presence of an edge. Or it might learn to detect a texture. Or perhaps a contrast between two colors.</p>

<p style="width: 80%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-9x5-Conv2.png" alt="Conv2-9x5-Conv2"></p>

<p>In the previous example, we fed the output of our convolutional layer into a fully-connected layer. But we can also compose two convolutional layers, as we did in the one dimensional case.</p>

<p style="width: 80%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-9x5-Conv2Conv2.png" alt="Conv2-9x5-Conv2Conv2"></p>
<p><br></p>

<p>We can also do max pooling in two dimensions. Here, we take the maximum of features over a small patch.</p>

<p>What this really boils down to is that, when considering an entire image, we don’t care about the exact position of an edge, down to a pixel. It’s enough to know where it is to within a few pixels.</p>

<p style="width:80%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-9x5-Conv2Max2Conv2.png" alt="Conv2-9x5-Conv2Max2Conv2"></p>
<p><br></p>

<p>Three-dimensional convolutional networks are also sometimes used, for data like videos or volumetric data (eg. 3D medical scans). However, they are not very widely used, and much harder to visualize.</p>

<p>Now, we previously said that $A$ was a group of neurons. We should be a bit more precise about this: what is $A$ exactly?</p>

<p>In traditional convolutional layers, $A$ is a bunch of neurons in parallel, that all get the same inputs and compute different features.</p>

<p>For example, in a 2-dimensional convolutional layer, one neuron might detect horizontal edges, another might detect vertical edges, and another might detect green-red color contrasts.</p>

<p style="width: 80%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-A.png" alt="Conv-A"></p>

<p>That said, in the recent paper ‘Network in Network’ (<a href="https://arxiv.org/abs/1312.4400">Lin et al. (2013)</a>), a new “Mlpconv” layer is proposed.
In this model, $A$ would have multiple layers of neurons, with the final layer outputting higher level features for the region. In the paper, the model achieves some very impressive results, setting new state of the art on a number of benchmark datasets.</p>

<p style="width: 80%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-A-NIN.png" alt="Conv-A-NIN"></p>

<p>That said, for the purposes of this post, we will focus on standard convolutional layers. There’s already enough for us to consider there!</p>

<h1 id="results-of-convolutional-neural-networks">Results of Convolutional Neural Networks</h1>

<p>Earlier, we alluded to recent breakthroughs in computer vision using convolutional neural networks. Before we go on, I’d like to briefly discuss some of these results as motivation.</p>

<p>In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton blew existing image classification results out of the water (<a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">Krizehvsky et al. (2012)</a>).</p>

<p>Their progress was the result of combining together a bunch of different pieces.
They used GPUs to train a very large, deep, neural network.
They used a new kind of neuron (ReLUs) and a new technique to reduce a problem called ‘overfitting’ (DropOut).
They used a very large dataset with lots of image categories (<a href="http://www.image-net.org/">ImageNet</a>). And, of course, it was a convolutional neural network.</p>

<p>Their architecture, illustrated below, was very deep.
It has 5 convolutional layers<sup><a href="#e675:fn:3" class="footnote" id="e675:fn-back:3">3</a></sup>, with pooling interspersed, and three fully-connected layers.
The early layers are split over the two GPUs.</p>

<p style="width: 80%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/KSH-arch.png" alt="KSH-arch"></p>

<p style="padding: 20 10px;" class="center"><em><a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">From Krizehvsky et al. (2012)</a></em></p>

<p>They trained their network to classify images into a thousand different categories.</p>

<p>Randomly guessing, one would guess the correct answer 0.1% of the time.
Krizhevsky, et al.’s model is able to give the right answer 63% of the time.
Further, one of the top 5 answers it gives is right 85% of the time!</p>

<p style="padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/KSH-results.png" alt="KSH-results"></p>

<p class="center"><em>Top: 4 correctly classified examples.
Bottom: 4 incorrectly classified examples.
Each example has an image, followed by its label, followed by the top 5 guesses with probabilities.
From <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">Krizehvsky et al. (2012)</a></em></p>
<p><br></p>

<p>Even some of its errors seem pretty reasonable to me!</p>

<p>We can also examine what the first layer of the network learns to do.</p>

<p>Recall that the convolutional layers were split between the two GPUs. Information doesn’t go back and forth each layer, so the split sides are disconnected in a real way. It turns out that, every time the model is run, the two sides specialize.</p>

<p style="padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/KSH-filters.png" alt="KSH-filters"></p>

<p class="center"><em>Filters learned by the first convolutional layer. The top half corresponds to the layer on one GPU, the bottom on the other. From <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">Krizehvsky et al. (2012)</a></em></p>
<p><br></p>

<p>Neurons in one side focus on black and white, learning to detect edges of different orientations and sizes.
Neurons on the other side specialize on color and texture, detecting color contrasts and patterns<sup><a href="#e675:fn:4" class="footnote" id="e675:fn-back:4">4</a></sup>.
Remember that the neurons are <em>randomly</em> initialized. No human went and set them to be edge detectors, or to split in this way. It arose simply from training the network to classify images.</p>

<p>These remarkable results (and other exciting results around that time) were only the beginning. They were quickly followed by a lot of other work testing modified approaches and gradually improving the results, or applying them to other areas. And, in addition to the neural networks community, many in the computer vision community have adopted deep convolutional neural networks.</p>

<p>Convolutional neural networks are an essential tool in computer vision and modern pattern recognition.</p>

<h1 id="formalizing-convolutional-neural-networks">Formalizing Convolutional Neural Networks</h1>

<p>Consider a 1-dimensional convolutional layer with inputs ${x_n}$ and outputs  ${y_n}$</p>

<p style="padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-Conv2-XY.png" alt="Conv-9-Conv2-XY"></p>

<p>It’s relatively easy to describe the outputs in terms of the inputs:</p>

\[y_n = A(x_n,x_{n+1},...)\]

<p>For example, in the above:</p>

\[y_0 = A(x_0,x_1)\]

\[y_1 = A(x_1,x_2)\]

<p>Similarly, if we consider a 2-dimensional convolutional layer, with inputs ${x_n,m}$ and outputs ${y_n,m}$:</p>

<p style="width: 80%; padding: 20 10px;" class="center"><img src="/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-5x5-Conv2-XY.png" alt="Conv2-5x5-Conv2-XY"></p>
<p><br></p>

<p>We can, again, write down the outputs in terms of the inputs:</p>

\[y_{n,m} = A\begin{pmatrix}
           x_{n, m}, &amp; x_{n+1, m} &amp; ..., \\
           x_{n, m+1}, &amp; x_{n+1, m+1}, &amp; ..., \\
           &amp; ...,
           \end{pmatrix}\]

<p>For example:</p>

\[y_{0, 0} = A\begin{pmatrix}
    x_{0, 0}, &amp;x_{1, 0}, \\
    x_{0,1}, &amp; x_{1, 1}
    \end{pmatrix}\]

\[y_{1, 0} = A\begin{pmatrix}
    x_{1, 0}, &amp; x_{2, 0}, \\
    x_{1,1}, &amp; x_{2, 1}
    \end{pmatrix}\]

<p>If one combines this with the equation for $A(x)$,</p>

\[A(x) = \sigma(Wx + b)\]

<p>one has everything they need to implement a convolutional neural network, at least in theory.</p>

<p>In practice, this is often not best way to think about convolutional neural networks.
There is an alternative formulation, in terms of a mathematical operation called <em>convolution</em>, that is often more helpful.</p>

<p>The convolution operation is a powerful tool. In mathematics, it comes up in diverse contexts, ranging from the study of partial differential equations to probability theory. In part because of its role in PDEs, convolution is very important in the physical sciences. It also has an important role in many applied areas, like computer graphics and signal processing.</p>

<p>For,[^1] us, convolution will provide a number of benefits. Firstly, it will allow us to create much more efficient implementations of convolutional layers than the naive perspective might suggest. Secondly, it will remove a lot of messiness from our formulation, handling all the bookkeeping presently showing up in the indexing of xs – the present formulation may not seem messy yet, but that’s only because we haven’t got into the tricky cases yet. Finally, convolution will give us a significantly different perspective for reasoning about convolutional layers.</p>

<blockquote>
  <p>I admire the elegance of your method of computation; it must be nice to ride through these fields upon the horse of true mathematics while the like of us have to make our way laboriously on foot.  — Albert Einstein</p>
</blockquote>

<h1 id="acknowledgments">Acknowledgments</h1>

<ol class="footnotelist">
<li id="e675:fn:1" class="footnotebody" value="1">
        It should be noted that not all neural networks that use multiple copies of the same neuron are convolutional neural networks. Convolutional neural networks are just one type of neural network that uses the more general trick, weight-tying. Other kinds of neural network that do this are recurrent neural networks and recursive neural networks.
   <a href="#e675:fn-back:1" class="backlink">⏎</a>
</li>
<li id="e675:fn:2" class="footnotebody" value="2">
      Groups of neurons, like $A$, that appear in multiple places are sometimes called modules, and networks that use them are sometimes called modular neural networks
   <a href="#e675:fn-back:2" class="backlink">⏎</a>
</li>
<li id="e675:fn:3" class="footnotebody" value="3">
     Groups of neurons, like $A$, that appear in multiple places are sometimes called modules, and networks that use them are sometimes called modular neural networks
  <a href="#e675:fn-back:3" class="backlink">⏎</a>
</li>
<li id="e675:fn:4" class="footnotebody" value="4">
       They also test using 7 in the paper.
    <a href="#e675:fn-back:4" class="backlink">⏎</a>
</li>
</ol>

            </div>
<a class="u-url" href="/blog/deep-learning/2019/04/25/ConvNets-Modular.html" hidden></a>
        </article>
        

        <div class="page-navigation">
          
          <a class="prev" href="/blog/2018/06/24/attention-attention.html"><svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512"><path d="M223.7 239l136-136c9.4-9.4 24.6-9.4 33.9 0l22.6 22.6c9.4 9.4 9.4 24.6 0 33.9L319.9 256l96.4 96.4c9.4 9.4 9.4 24.6 0 33.9L393.7 409c-9.4 9.4-24.6 9.4-33.9 0l-136-136c-9.5-9.4-9.5-24.6-.1-34zm-192 34l136 136c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9L127.9 256l96.4-96.4c9.4-9.4 9.4-24.6 0-33.9L201.7 103c-9.4-9.4-24.6-9.4-33.9 0l-136 136c-9.5 9.4-9.5 24.6-.1 34z"></path></svg> Attention? Attention!</a>  
          <a class="next" href="/blog/deep-learning/2019/04/25/recipe-for-training-neural-network.html">A Recipe for Training Neural Networks <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34zm192-34l-136-136c-9.4-9.4-24.6-9.4-33.9 0l-22.6 22.6c-9.4 9.4-9.4 24.6 0 33.9l96.4 96.4-96.4 96.4c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l136-136c9.4-9.2 9.4-24.4 0-33.8z"></path></svg></a> 
      </div>
    
    </div>
</main>
<div class="scroll-top"><a href="#title" class="nav-top">Top <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 320 512"><path d="M177 255.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 351.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 425.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1zm-34-192L7 199.7c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l96.4-96.4 96.4 96.4c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9l-136-136c-9.2-9.4-24.4-9.4-33.8 0z"></path></svg></a></div>
<footer class="site-footer h-card">
    <data class="u-url" href="/blog/"></data>

    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div><ul class="social-media-list">
<li><a href="http://hmthanh.github.io/" target="_blank"><svg class="svg-icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 496 512"><path d="M336.5 160C322 70.7 287.8 8 248 8s-74 62.7-88.5 152h177zM152 256c0 22.2 1.2 43.5 3.3 64h185.3c2.1-20.5 3.3-41.8 3.3-64s-1.2-43.5-3.3-64H155.3c-2.1 20.5-3.3 41.8-3.3 64zm324.7-96c-28.6-67.9-86.5-120.4-158-141.6 24.4 33.8 41.2 84.7 50 141.6h108zM177.2 18.4C105.8 39.6 47.8 92.1 19.3 160h108c8.7-56.9 25.5-107.8 49.9-141.6zM487.4 192H372.7c2.1 21 3.3 42.5 3.3 64s-1.2 43-3.3 64h114.6c5.5-20.5 8.6-41.8 8.6-64s-3.1-43.5-8.5-64zM120 256c0-21.5 1.2-43 3.3-64H8.6C3.2 212.5 0 233.8 0 256s3.2 43.5 8.6 64h114.6c-2-21-3.2-42.5-3.2-64zm39.5 96c14.5 89.3 48.7 152 88.5 152s74-62.7 88.5-152h-177zm159.3 141.6c71.4-21.2 129.4-73.7 158-141.6h-108c-8.8 56.9-25.6 107.8-50 141.6zM19.3 352c28.6 67.9 86.5 120.4 158 141.6-24.4-33.8-41.2-84.7-50-141.6h-108z"></path></svg><span class="username">Website</span></a></li>
<li><a href="mailto:hmthanhgm@gmail.com"><svg class="svg-icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 512 512"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg> <span class="username">Email</span></a></li>
<li><a href="https://www.facebook.com/hmthanhgm" target="_blank"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#facebook"></use></svg> <span class="username">Facebook</span></a></li>
<li><a href="https://github.com/hmthanh" target="_blank"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg>
    <span class="username">Github</span></a></li>
<li><a href="https://www.twitter.com/hmthanhgm" target="_blank"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">Twitter</span></a></li>
<li>
        <a href="/blog/%20/feed.xml">RSS</a>
    </li>

</ul></div>
            <p>© 2020 Minh-Thanh</p>
        </div>
    </div>
</footer>
            <script>
                window.onload = function () {
                    var script = document.createElement('script');
                    var firstScript = document.getElementsByTagName('script')[0];
                    script.type = 'text/javascript';
                    script.async = true;
                    script.src = '/blog/sw-register.js?v=' + Date.now();
                    firstScript.parentNode.insertBefore(script, firstScript);
                };
            </script>
            </body>


</html>
