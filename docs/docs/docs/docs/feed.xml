<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/blog/" rel="alternate" type="text/html" /><updated>2020-11-03T01:50:46+07:00</updated><id>http://localhost:4000/blog/feed.xml</id><title type="html">Minh-Thanh’s Blog</title><subtitle>✨ _Your ambition outweighed your talent !_ ✨</subtitle><entry><title type="html">Simple tutorial using VIM</title><link href="http://localhost:4000/blog/tutorial/2020/11/01/simple-vim-tutorial.html" rel="alternate" type="text/html" title="Simple tutorial using VIM" /><published>2020-11-01T11:04:17+07:00</published><updated>2020-11-01T11:04:17+07:00</updated><id>http://localhost:4000/blog/tutorial/2020/11/01/simple-vim-tutorial</id><content type="html" xml:base="http://localhost:4000/blog/tutorial/2020/11/01/simple-vim-tutorial.html">&lt;p&gt;Download VIM tại : &lt;a href=&quot;https://www.vim.org/download.php&quot;&gt;VIM&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Vim có hai chế độ&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;insert&lt;/code&gt; mode (để chèn) : ấn &lt;code&gt;i&lt;/code&gt; để chuyển qua chế độ insert&lt;/li&gt;
      &lt;li&gt;&lt;code&gt;normal&lt;/code&gt; mode  (để di chuyển và điều khiển trong văn bản) : ấn &lt;code&gt;Esc&lt;/code&gt; để chuyển qua chế độ normal&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Vim chuyển cơ bản&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Di chuyển sang trái &amp;lt;- hoặc sang phải -&amp;gt; trong văn bản : ấn &lt;code&gt;h&lt;/code&gt; sang trái, ấn &lt;code&gt;l&lt;/code&gt; sang phải&lt;/li&gt;
      &lt;li&gt;Di chuyển lên xuống từng dòng : ấn &lt;code&gt;j&lt;/code&gt; đi xuống, ấn &lt;code&gt;k&lt;/code&gt; đi lên&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Di chuyển theo từ&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Để di chuyển đến từ tiếp theo : &lt;code&gt;w&lt;/code&gt; đến đầu từ, &lt;code&gt;e&lt;/code&gt; đến cuối từ&lt;/li&gt;
      &lt;li&gt;Để di chuyển đến từ phía trước : &lt;code&gt;b&lt;/code&gt; di chuyển đến đầu từ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Di chuyển sử dụng số&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Sử dụng số trước các phím di chuyển (&lt;code&gt;w&lt;/code&gt;, &lt;code&gt;e&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;) để đi đến số từ mong muốn
Ví dụ : &lt;code&gt;3w&lt;/code&gt; để di chuyển đến vị trí đầu tiên sau 3 từ tiếp theo&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Sử dụng số trước &lt;code&gt;l&lt;/code&gt; để di chuyển đến ký tự (letter) lần
Ví dụ : &lt;code&gt;3l&lt;/code&gt; di chuyển đến 3 ký tự tiếp theo&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Chèn nhiều lần trong chế độ &lt;code&gt;norrmal&lt;/code&gt; mode&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Để chèn các kỹ tự lặp lại nhiều lần, ta sử dụng &lt;code&gt;[number]i[]&lt;/code&gt; rồi &lt;code&gt;Esc&lt;/code&gt;
Ví dụ &lt;code&gt;30iabc&lt;/code&gt; &lt;code&gt;Esc&lt;/code&gt; sẽ sinh ra &lt;code&gt;abcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcab&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tìm kiếm một ký tự&lt;/p&gt;

    &lt;p&gt;huoưg&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Minh-Thanh Hoang</name></author><category term="tutorial" /><summary type="html">Download VIM tại : VIM Vim có hai chế độ insert mode (để chèn) : ấn i để chuyển qua chế độ insert normal mode (để di chuyển và điều khiển trong văn bản) : ấn Esc để chuyển qua chế độ normal Vim chuyển cơ bản Di chuyển sang trái &amp;lt;- hoặc sang phải -&amp;gt; trong văn bản : ấn h sang trái, ấn l sang phải Di chuyển lên xuống từng dòng : ấn j đi xuống, ấn k đi lên Di chuyển theo từ Để di chuyển đến từ tiếp theo : w đến đầu từ, e đến cuối từ Để di chuyển đến từ phía trước : b di chuyển đến đầu từ Di chuyển sử dụng số Sử dụng số trước các phím di chuyển (w, e, b) để đi đến số từ mong muốn Ví dụ : 3w để di chuyển đến vị trí đầu tiên sau 3 từ tiếp theo Sử dụng số trước l để di chuyển đến ký tự (letter) lần Ví dụ : 3l di chuyển đến 3 ký tự tiếp theo Chèn nhiều lần trong chế độ norrmal mode Để chèn các kỹ tự lặp lại nhiều lần, ta sử dụng [number]i[] rồi Esc Ví dụ 30iabc Esc sẽ sinh ra abcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcab Tìm kiếm một ký tự huoưg</summary></entry><entry><title type="html">The Transformer Family</title><link href="http://localhost:4000/blog/2020/04/07/the-transformer-family.html" rel="alternate" type="text/html" title="The Transformer Family" /><published>2020-04-07T19:00:00+07:00</published><updated>2020-04-07T19:00:00+07:00</updated><id>http://localhost:4000/blog/2020/04/07/the-transformer-family</id><content type="html" xml:base="http://localhost:4000/blog/2020/04/07/the-transformer-family.html">&lt;blockquote&gt;
  &lt;p&gt;Inspired by recent progress on various enhanced versions of Transformer models, this post presents how the vanilla Transformer can be improved for longer-term attention span, less memory and computation consumption, RL task solving, etc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;It has been almost two years since my last post on &lt;a href=&quot;/blog/blog/2018/06/24/attention-attention.html&quot;&gt;attention&lt;/a&gt;. Recent progress on new and enhanced versions of Transformer motivates me to write another post on this specific topic, focusing on how the vanilla Transformer can be improved for longer-term attention span, less memory and computation consumption, RL task solving and more.&lt;/p&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#notations&quot; id=&quot;markdown-toc-notations&quot;&gt;Notations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#attention-and-self-attention&quot; id=&quot;markdown-toc-attention-and-self-attention&quot;&gt;Attention and Self-Attention&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#multi-head-self-attention&quot; id=&quot;markdown-toc-multi-head-self-attention&quot;&gt;Multi-Head Self-Attention&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#transformer&quot; id=&quot;markdown-toc-transformer&quot;&gt;Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#adaptive-computation-time-act&quot; id=&quot;markdown-toc-adaptive-computation-time-act&quot;&gt;Adaptive Computation Time (ACT)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#improved-attention-span&quot; id=&quot;markdown-toc-improved-attention-span&quot;&gt;Improved Attention Span&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#longer-attention-span-transformer-xl&quot; id=&quot;markdown-toc-longer-attention-span-transformer-xl&quot;&gt;Longer Attention Span (Transformer-XL)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#adaptive-attention-span&quot; id=&quot;markdown-toc-adaptive-attention-span&quot;&gt;Adaptive Attention Span&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#localized-attention-span-image-transformer&quot; id=&quot;markdown-toc-localized-attention-span-image-transformer&quot;&gt;Localized Attention Span (Image Transformer)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#less-time-and-memory-cost&quot; id=&quot;markdown-toc-less-time-and-memory-cost&quot;&gt;Less Time and Memory Cost&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#sparse-attention-matrix-factorization-sparse-transformers&quot; id=&quot;markdown-toc-sparse-attention-matrix-factorization-sparse-transformers&quot;&gt;Sparse Attention Matrix Factorization (Sparse Transformers)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#locality-sensitive-hashing-reformer&quot; id=&quot;markdown-toc-locality-sensitive-hashing-reformer&quot;&gt;Locality-Sensitive Hashing (Reformer)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#make-it-recurrent-universal-transformer&quot; id=&quot;markdown-toc-make-it-recurrent-universal-transformer&quot;&gt;Make it Recurrent (Universal Transformer)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#stabilization-for-rl-gtrxl&quot; id=&quot;markdown-toc-stabilization-for-rl-gtrxl&quot;&gt;Stabilization for RL (GTrXL)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot; id=&quot;markdown-toc-reference&quot;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;notations&quot;&gt;Notations&lt;/h3&gt;

&lt;table class=&quot;info&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Symbol&lt;/th&gt;
      &lt;th&gt;Meaning&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$$d$$&lt;/td&gt;
      &lt;td&gt;The model size / hidden state dimension / positional encoding size.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$h$$&lt;/td&gt;
      &lt;td&gt;The number of heads in multi-head attention layer.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$L$$&lt;/td&gt;
      &lt;td&gt;The segment length of input sequence.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{X} \in \mathbb{R}^{L \times d}$$&lt;/td&gt;
      &lt;td&gt;The input sequence where each element has been mapped into an embedding vector of shape $$d$$, same as the model size.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{W}^k \in \mathbb{R}^{d \times d_k}$$&lt;/td&gt;
      &lt;td&gt;The key weight matrix.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{W}^q \in \mathbb{R}^{d \times d_k}$$&lt;/td&gt;
      &lt;td&gt;The query weight matrix.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{W}^v \in \mathbb{R}^{d \times d_v}$$&lt;/td&gt;
      &lt;td&gt;The value weight matrix. Often we have $$d_k = d_v = d$$.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{W}^k_i, \mathbf{W}^q_i \in \mathbb{R}^{d \times d_k/h}; \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$$&lt;/td&gt;
      &lt;td&gt;The weight matrices per head.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$$&lt;/td&gt;
      &lt;td&gt;The output weight matrix.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{Q} = \mathbf{X}\mathbf{W}^q \in \mathbb{R}^{L \times d_k}$$&lt;/td&gt;
      &lt;td&gt;The query embedding inputs.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{K} = \mathbf{X}\mathbf{W}^k \in \mathbb{R}^{L \times d_k}$$&lt;/td&gt;
      &lt;td&gt;The key embedding inputs.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{V} = \mathbf{X}\mathbf{W}^v \in \mathbb{R}^{L \times d_v}$$&lt;/td&gt;
      &lt;td&gt;The value embedding inputs.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$S_i$$&lt;/td&gt;
      &lt;td&gt;A collection of key positions for the $$i$$-th query $$\mathbf{q}_i$$ to attend to.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{A} \in \mathbb{R}^{L \times L}$$&lt;/td&gt;
      &lt;td&gt;The self-attention matrix between a input sequence of lenght $$L$$ and itself. $$\mathbf{A} = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$$.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$a_{ij} \in \mathbf{A}$$&lt;/td&gt;
      &lt;td&gt;The scalar attention score between query $$\mathbf{q}_i$$ and key $$\mathbf{k}_j$$.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\mathbf{P} \in \mathbb{R}^{L \times d}$$&lt;/td&gt;
      &lt;td&gt;position encoding matrix, where the $$i$$-th row $$\mathbf{p}_i$$ is the positional encoding for input $$\mathbf{x}_i$$.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;attention-and-self-attention&quot;&gt;Attention and Self-Attention&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Attention&lt;/em&gt; is a mechanism in the neural network that a model can learn to make predictions by selectively attending to a given set of data. The amount of attention is quantified by learned weights and thus the output is usually formed as a weighted average.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Self-attention&lt;/em&gt; is a type of attention mechanism where the model makes prediction for one part of a data sample using other parts of the observation about the same sample. Conceptually, it feels quite similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-local_means&quot;&gt;non-local means&lt;/a&gt;. Also note that self-attention is permutation-invariant; in other words, it is an operation on sets.&lt;/p&gt;

&lt;p&gt;There are various forms of attention / self-attention, Transformer (&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Vaswani et al., 2017&lt;/a&gt;) relies on the &lt;em&gt;scaled dot-product attention&lt;/em&gt;: given a query matrix \(\mathbf{Q}\), a key matrix \(\mathbf{K}\) and a value matrix \(\mathbf{V}\), the output is a weighted sum of the value vectors, where the weight assigned to each value slot is determined by the dot-product of the query with the corresponding key:&lt;/p&gt;

\[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q} {\mathbf{K}}^\top}{\sqrt{d_k}})\mathbf{V}\]

&lt;p&gt;And for a query and a key vector \(\mathbf{q}_i, \mathbf{k}_j \in \mathbb{R}^d\) (row vectors in query and key matrices), we have a scalar score:&lt;/p&gt;

\[a_{ij} = \text{softmax}(\frac{\mathbf{q}_i {\mathbf{k}_j}^\top}{\sqrt{d_k}})
= \frac{\exp(\mathbf{q}_i {\mathbf{k}_j}^\top)}{ \sqrt{d_k} \sum_{r \in S_i} \exp(\mathbf{q}_i {\mathbf{k}_r}^\top) }\]

&lt;p&gt;where \(S_i\) is a collection of key positions for the \(i\)-th query to attend to.&lt;/p&gt;

&lt;p&gt;See my old &lt;a href=&quot;/blog/blog/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms&quot;&gt;post&lt;/a&gt; for other types of attention if interested.&lt;/p&gt;

&lt;h2 id=&quot;multi-head-self-attention&quot;&gt;Multi-Head Self-Attention&lt;/h2&gt;

&lt;p&gt;The &lt;em&gt;multi-head self-attention&lt;/em&gt; module is a key component in Transformer. Rather than only computing the attention once, the multi-head mechanism splits the inputs into smaller chunks and then computes the scaled dot-product attention over each subspace in parallel. The independent attention outputs are simply concatenated and linearly transformed into expected dimensions.&lt;/p&gt;

\[\begin{aligned}
\text{MultiHeadAttention}(\mathbf{X}_q, \mathbf{X}_k, \mathbf{X}_v) &amp;amp;= [\text{head}_1; \dots; \text{head}_h] \mathbf{W}^o \\ 
\text{where head}_i &amp;amp;= \text{Attention}(\mathbf{X}_q\mathbf{W}^q_i, \mathbf{X}_k\mathbf{W}^k_i, \mathbf{X}_v\mathbf{W}^v_i)
\end{aligned}\]

&lt;p&gt;where \([.;.]\) is a concatenation operation. \(\mathbf{W}^q_i, \mathbf{W}^k_i \in \mathbb{R}^{d \times d_k/h}, \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}\) are weight matrices to map input embeddings of size \(L \times d\) into query, key and value matrices. And \(\mathbf{W}^o \in \mathbb{R}^{d_v \times d}\) is the output linear transformation. All the weights should be learned during training.&lt;/p&gt;

&lt;p style=&quot;width: 30%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/multi-head-attention.png&quot; alt=&quot;Multi-head scaled dot-product attention&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 1. Illustration of the multi-head scaled dot-product attention mechanism. (Image source: Figure 2 in &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;transformer&quot;&gt;Transformer&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Transformer&lt;/strong&gt; (which will be referred to as “vanilla Transformer” to distinguish it from other enhanced versions; &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;) model has an encoder-decoder architecture, as commonly used in many &lt;a href=&quot;/blog/blog/2018/06/24/attention-attention.html#born-for-translation&quot;&gt;NMT&lt;/a&gt; models. Later decoder-only Transformer was shown to achieve great performance in language modeling tasks, like in &lt;a href=&quot;/blog&quot;&gt;GPT and BERT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encoder-Decoder Architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;encoder&lt;/strong&gt; generates an attention-based representation with capability to locate a specific piece of information from a large context. It consists of a stack of 6 identity modules, each containing two submodules, a &lt;em&gt;multi-head self-attention&lt;/em&gt; layer and a &lt;em&gt;point-wise&lt;/em&gt; fully connected feed-forward network. By point-wise, it means that it applies the same linear transformation (with same weights) to each element in the sequence. This can also be viewed as a convolutional layer with filter size 1. Each submodule has a residual connection and layer normalization. All the submodules output data of the same dimension \(d\).&lt;/p&gt;

&lt;p&gt;The function of Transformer &lt;strong&gt;decoder&lt;/strong&gt; is to retrieve information from the encoded representation. The architecture is quite similar to the encoder, except that the decoder contains two multi-head attention submodules instead of one in each identical repeating module. The first multi-head attention submodule is &lt;em&gt;masked&lt;/em&gt; to prevent positions from attending to the future.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/transformer.png&quot; alt=&quot;Transformer&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 2. The architecture of the vanilla Transformer model. (Image source: &lt;a href=&quot;/blog/blog/2018/06/24/attention-attention.html#full-architecture&quot;&gt;Figure 17&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Positional Encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Because self-attention operation is permutation invariant, it is important to use proper &lt;strong&gt;positional encoding&lt;/strong&gt;to provide &lt;em&gt;order information&lt;/em&gt; to the model. The positional encoding \(\mathbf{P} \in \mathbb{R}^{L \times d}\) has the same dimension as the input embedding, so it can be added on the input directly. The vanilla Transformer considered two types of encodings:&lt;/p&gt;

&lt;p&gt;(1) &lt;em&gt;Sinusoidal positional encoding&lt;/em&gt; is defined as follows, given the token position \(i=1,\dots,L\) and the dimension \(\delta=1,\dots,d\):&lt;/p&gt;

\[\text{PE}(i,\delta) = 
\begin{cases}
\sin(\frac{i}{10000^{2\delta'/d}}) &amp;amp; \text{if } \delta = 2\delta'\\
\cos(\frac{i}{10000^{2\delta'/d}}) &amp;amp; \text{if } \delta = 2\delta' + 1\\
\end{cases}\]

&lt;p&gt;In this way each dimension of the positional encoding corresponds to a sinusoid of different wavelengths in different dimensions, from \(2\pi\) to \(10000 \cdot 2\pi\).&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/sinoidual-positional-encoding.png&quot; alt=&quot;Transformer&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 3. Sinusoidal positional encoding with \(L=32\) and \(d=128\). The value is between -1 (black) and 1 (white) and the value 0 is in gray.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;(2) &lt;em&gt;Learned positional encoding&lt;/em&gt;, as its name suggested, assigns each element with a learned column vector which encodes its &lt;em&gt;absolute&lt;/em&gt; position (&lt;a href=&quot;https://arxiv.org/abs/1705.03122&quot;&gt;Gehring, et al. 2017&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Quick Follow-ups&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Following the vanilla Transformer, &lt;a href=&quot;https://arxiv.org/abs/1808.04444&quot;&gt;Al-Rfou et al. (2018)&lt;/a&gt; added a set of auxiliary losses to enable training a deep Transformer model on character-level language modeling which outperformed LSTMs. Several types of auxiliary tasks are used:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Instead of producing only one prediction at the sequence end, every &lt;em&gt;immediate position&lt;/em&gt; is also asked to make a correct prediction, forcing the model to predict given smaller contexts (e.g. first couple tokens at the beginning of a context window).&lt;/li&gt;
  &lt;li&gt;Each intermediate Transformer layer is used for making predictions as well. Lower layers are weighted to contribute less and less to the total loss as training progresses.&lt;/li&gt;
  &lt;li&gt;Each position in the sequence can predict multiple targets, i.e. two or more predictions of the future tokens.&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/transformer-aux-losses.png&quot; alt=&quot;Transformer&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 4. Auxiliary prediction tasks used in deep Transformer for character-level language modeling. (Image source: &lt;a href=&quot;https://arxiv.org/abs/1808.04444&quot;&gt;Al-Rfou et al. (2018)&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;adaptive-computation-time-act&quot;&gt;Adaptive Computation Time (ACT)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Adaptive Computation Time&lt;/strong&gt; (short for &lt;strong&gt;ACT&lt;/strong&gt;; &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;&gt;Graves, 2016&lt;/a&gt;) is a mechanism for dynamically deciding how many computational steps are needed in a recurrent neural network. Here is a cool &lt;a href=&quot;https://distill.pub/2016/augmented-rnns/#adaptive-computation-time&quot;&gt;tutorial&lt;/a&gt; on ACT from distill.pub.&lt;/p&gt;

&lt;p&gt;Let’s say, we have a RNN model \(\mathcal{R}\) composed of input weights \(W_x\), a parametric state transition function \(\mathcal{S}(.)\), a set of output weights \(W_y\) and an output bias \(b_y\). Given an input sequence \((x_1, \dots, x_L)\), the output sequence \((y_1, \dots, y_L)\) is computed by:&lt;/p&gt;

\[s_t = \mathcal{S}(s_{t-1}, W_x x_t), \quad y_t = W_y s_t + b_y\quad\text{for }t=1, \dots, L\]

&lt;p&gt;ACT enables the above RNN setup to perform a variable number of steps at each input element. Multiple computational steps lead to a sequence of intermediate states \((s_t^1, \dots, s_t^{N(t)})\) and outputs \((y_t^1, \dots, y_t^{N(t)})\) — they all share the same state transition function \(\mathcal{S}(.)\), as well as the same output weights \(W_y\) and bias \(b_y\):&lt;/p&gt;

\[\begin{aligned}
s_t^0 &amp;amp;= s_{t-1} \\
s_t^n &amp;amp;= \mathcal{S}(s_{t}^{n-1}, x_t^n) = \mathcal{S}(s_{t}^{n-1}, x_t + \delta_{n,1}) \text{ for } n=1, \dots, N(t)\\
y_t^n &amp;amp;= W_y s_t^n + b_y
\end{aligned}\]

&lt;p&gt;where \(\delta_{n,1}\) is a binary flag indicating whether the input step has been incremented.&lt;/p&gt;

&lt;p&gt;The number of steps \(N(t)\) is determined by an extra sigmoidal halting unit \(h\), with associated weight matrix \(W_h\) and bias \(b_h\), outputting a halting probability \(p_t^n\) at immediate step \(n\) for \(t\)-th input element:&lt;/p&gt;

\[h_t^n = \sigma(W_h s_t^n + b_h)\]

&lt;p&gt;In order to allow the computation to halt after a single step, ACT introduces a small constant \(\epsilon\) (e.g. 0.01), so that whenever the cumulative probability goes above \(1-\epsilon\), the computation stops.&lt;/p&gt;

\[\begin{aligned}
N(t) &amp;amp;= \min(\min\{n': \sum_{n=1}^{n'} h_t^n \geq 1 -\epsilon\}, M) \\
p_t^n &amp;amp;= \begin{cases}
h_t^n &amp;amp; \text{if }n &amp;lt; N(t) \\
R(t) = 1 - \sum_{n=1}^{N(t)-1} h_t^n &amp;amp; \text{if }n= N(t)\\
\end{cases}
\end{aligned}\]

&lt;p&gt;where \(M\) is an upper limit for the number of immediate steps allowed.&lt;/p&gt;

&lt;p&gt;The final state and output are mean-field updates:&lt;/p&gt;

\[s_t = \sum_{n=1}^{N(t)} p_t^n s_t^n,\quad y_t = \sum_{n=1}^{N(t)} p_t^n y_t^n\]

&lt;p style=&quot;width: 85%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/ACT-computation-graph.png&quot; alt=&quot;ACT computation graph&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 5. The computation graph of a RNN with ACT mechanism. (Image source: &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;&gt;Graves, 2016&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To avoid unnecessary pondering over each input, ACT adds a &lt;em&gt;ponder cost&lt;/em&gt; \(\mathcal{P}(x) = \sum_{t=1}^L N(t) + R(t)\)  in the loss function to encourage a smaller number of intermediate computational steps.&lt;/p&gt;

&lt;h2 id=&quot;improved-attention-span&quot;&gt;Improved Attention Span&lt;/h2&gt;

&lt;p&gt;The goal of improving attention span is to make the context that can be used in self-attention longer, more efficient and flexible.&lt;/p&gt;

&lt;h3 id=&quot;longer-attention-span-transformer-xl&quot;&gt;Longer Attention Span (Transformer-XL)&lt;/h3&gt;

&lt;p&gt;The vanilla Transformer has a fixed and limited attention span. The model can only attend to other elements in the same segments during each update step and no information can flow across separated fixed-length segments.&lt;/p&gt;

&lt;p&gt;This &lt;em&gt;context segmentation&lt;/em&gt; causes several issues:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The model cannot capture very long term dependencies.&lt;/li&gt;
  &lt;li&gt;It is hard to predict the first few tokens in each segment given no or thin context.&lt;/li&gt;
  &lt;li&gt;The evaluation is expensive. Whenever the segment is shifted  to the right by one, the new segment is re-processed from scratch, although there are a lot of overlapped tokens.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Transformer-XL&lt;/strong&gt; (&lt;a href=&quot;https://arxiv.org/abs/1901.02860&quot;&gt;Dai et al., 2019&lt;/a&gt;; “XL” means “extra long”) solves the context segmentation problem with two main modifications:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Reusing hidden states between segments.&lt;/li&gt;
  &lt;li&gt;Adopting a new positional encoding that is suitable for reused states.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Hidden State Reuse&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The recurrent connection between segments is introduced into the model by continuously using the hidden states from the previous segments.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/transformer-XL-training.png&quot; alt=&quot;Training phrase of Transformer-XL&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 6. A comparison between the training phrase of vanilla Transformer &amp;amp; Transformer-XL with a segment length 4. (Image source: left part of Figure 2 in &lt;a href=&quot;https://arxiv.org/abs/1901.02860&quot;&gt;Dai et al., 2019&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let’s label the hidden state of the \(n\)-th layer for the \((\tau + 1)\)-th segment in the model as \(\mathbf{h}_{\tau+1}^{(n)} \in \mathbb{R}^{L \times d}\). In addition to the hidden state of the last layer for the same segment \(\mathbf{h}_{\tau+1}^{(n-1)}\), it also depends on the hidden state of the same layer for the previous segment \(\mathbf{h}_{\tau}^{(n)}\).  By incorporating information from the previous hidden states, the model extends the attention span much longer in the past, over multiple segments.&lt;/p&gt;

\[\begin{aligned}
\color{red}{\widetilde{\mathbf{h}}_{\tau+1}^{(n-1)}} &amp;amp;= [\text{stop-gradient}(\mathbf{h}_{\tau}^{(n-1)}) \circ \mathbf{h}_{\tau+1}^{(n-1)}] \\
\mathbf{Q}_{\tau+1}^{(n)} &amp;amp;= \mathbf{h}_{\tau+1}^{(n-1)}\mathbf{W}^q \\
\mathbf{K}_{\tau+1}^{(n)} &amp;amp;= \color{red}{\widetilde{\mathbf{h}}_{\tau+1}^{(n-1)}} \mathbf{W}^k \\
\mathbf{V}_{\tau+1}^{(n)} &amp;amp;= \color{red}{\widetilde{\mathbf{h}}_{\tau+1}^{(n-1)}} \mathbf{W}^v \\
\mathbf{h}_{\tau+1}^{(n)} &amp;amp;= \text{transformer-layer}(\mathbf{Q}_{\tau+1}^{(n)}, \mathbf{K}_{\tau+1}^{(n)}, \mathbf{V}_{\tau+1}^{(n)})
\end{aligned}\]

&lt;p&gt;Note that both key and value rely on the extended hidden state, while the query only consumes hidden state at current step. The concatenation operation \([. \circ .]\) is along the sequence length dimension.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relative Positional Encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to work with this new form of attention span, Transformer-XL proposed a new type of positional encoding. If using the same approach by vanilla Transformer and encoding the absolute position, the previous and current segments will be assigned with the same encoding, which is undesired.&lt;/p&gt;

&lt;p&gt;To keep the positional information flow coherently across segments, Transformer-XL encodes the &lt;em&gt;relative&lt;/em&gt; position instead, as it could be sufficient enough to know the position offset for making good predictions, i.e. \(i-j\), between one key vector \(\mathbf{k}_{\tau, j}\) and its query \(\mathbf{q}_{\tau, i}\).&lt;/p&gt;

&lt;p&gt;If omitting the scalar \(1/\sqrt{d_k}\) and the normalizing term in softmax but including positional encodings, we can write the attention score between query at position \(i\) and key at position \(j\) as:&lt;/p&gt;

\[\begin{aligned}
a_{ij} 
&amp;amp;= \mathbf{q}_i {\mathbf{k}_j}^\top = (\mathbf{x}_i + \mathbf{p}_i)\mathbf{W}^q ((\mathbf{x}_j + \mathbf{p}_j)\mathbf{W}^k)^\top \\
&amp;amp;= \mathbf{x}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{x}_j^\top + \mathbf{x}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{p}_j^\top + \mathbf{p}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{x}_j^\top + \mathbf{p}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{p}_j^\top
\end{aligned}\]

&lt;p&gt;Transformer-XL reparameterizes the above four terms as follows:&lt;/p&gt;

\[a_{ij}^\text{rel} = 
\underbrace{ \mathbf{x}_i\mathbf{W}^q \color{blue}{ {\mathbf{W}_E^k}^\top } \mathbf{x}_j^\top }_\text{content-based addressing} + 
\underbrace{ \mathbf{x}_i\mathbf{W}^q \color{blue}{ {\mathbf{W}_R^k}^\top } \color{green}{\mathbf{r}_{i-j}^\top} }_\text{content-dependent positional bias} + 
\underbrace{ \color{red}{\mathbf{u}} \color{blue}{ {\mathbf{W}_E^k}^\top } \mathbf{x}_j^\top }_\text{global content bias} + 
\underbrace{ \color{red}{\mathbf{v}} \color{blue}{ {\mathbf{W}_R^k}^\top } \color{green}{\mathbf{r}_{i-j}^\top} }_\text{global positional bias}\]

&lt;ul&gt;
  &lt;li&gt;Replace \(\mathbf{p}_j\) with relative positional encoding \(\mathbf{r}_{i-j} \in \mathbf{R}^{d}\);&lt;/li&gt;
  &lt;li&gt;Replace \(\mathbf{p}_i\mathbf{W}^q\) with two trainable parameters \(\mathbf{u}\) (for content) and \(\mathbf{v}\) (for location) in two different terms;&lt;/li&gt;
  &lt;li&gt;Split \(\mathbf{W}^k\) into two matrices, \(\mathbf{W}^k_E\) for content information and \(\mathbf{W}^k_R\) for location information.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adaptive-attention-span&quot;&gt;Adaptive Attention Span&lt;/h3&gt;

&lt;p&gt;One key advantage of Transformer is the capability of capturing long-term dependencies. Depending on the context, the model may prefer to attend further sometime than others; or one attention head may had different attention pattern from the other. If the attention span could adapt its length flexibly and only attend further back when needed, it would help reduce both computation and memory cost to support longer maximum context size in the model.&lt;/p&gt;

&lt;p&gt;This is the motivation for &lt;strong&gt;Adaptive Attention Span&lt;/strong&gt;. &lt;a href=&quot;https://arxiv.org/abs/1905.07799&quot;&gt;Sukhbaatar, et al., (2019)&lt;/a&gt; proposed a self-attention mechanism that seeks an optimal attention span. They hypothesized that different attention heads might assign scores differently within the same context window (See Fig. 7) and thus the optimal span would be trained separately per head.&lt;/p&gt;

&lt;p style=&quot;width: 70%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/attention-per-head.png&quot; alt=&quot;Attention per head&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 7. Two attention heads in the same model, A &amp;amp; B, assign attention differently within the same context window. Head A attends more to the recent tokens, while head B look further back into the past uniformly. (Image source: &lt;a href=&quot;https://arxiv.org/abs/1905.07799&quot;&gt;Sukhbaatar, et al. 2019&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Given the \(i\)-th token, we need to compute the attention weights between this token and other keys at positions \(j \in S_i\), where \(S_i\) defineds the \(i\)-th token’s context window.&lt;/p&gt;

\[\begin{aligned}
e_{ij} &amp;amp;= \mathbf{q}_i {\mathbf{k}_j}^\top \\ 
a_{ij} &amp;amp;= \text{softmax}(e_{ij}) = \frac{\exp(e_{ij})}{\sum_{r=i-s}^{i-1} \exp(e_{ir})} \\
\mathbf{y}_i &amp;amp;= \sum_{r=i-s}^{i-1}a_{ir}\mathbf{v}_r = \sum_{r=i-s}^{i-1}a_{ir}\mathbf{x}_r\mathbf{W}^v
\end{aligned}\]

&lt;p&gt;A &lt;em&gt;soft mask function&lt;/em&gt; \(m_z\) is added to control for an effective adjustable attention span, which maps the distance between query and key into a [0, 1] value. \(m_z\) is parameterized by \(z \in [0, s]\) and \(z\) is to be learned:&lt;/p&gt;

\[m_z(x) = \text{clamp}(\frac{1}{R}(R+z-x), 0, 1)\]

&lt;p&gt;where \(R\) is a hyper-parameter which defines the softness of \(m_z\).&lt;/p&gt;

&lt;p style=&quot;width: 55%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/soft-masking-function.png&quot; alt=&quot;Soft masking function&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 8. The soft masking function used in the adaptive attention span. (Image source: &lt;a href=&quot;https://arxiv.org/abs/1905.07799&quot;&gt;Sukhbaatar, et al. 2019&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The soft mask function is applied to the softmax elements in the attention weights:&lt;/p&gt;

\[a_{ij} = \frac{m_z(i-j)\exp(s_{ij})}{\sum_{r=i-s}^{i-1}m_z(i-r) \exp(s_{ir})}\]

&lt;p&gt;In the above equation, \(z\) is differentiable so it is trained jointly with other parts of the model. Parameters \(z^{(i)}, i=1, \dots, h\) are learned &lt;em&gt;separately per head&lt;/em&gt;. Moreover, the loss function has an extra L1 penalty on \(\sum_{i=1}^h z^{(i)}\).&lt;/p&gt;

&lt;p&gt;Using &lt;a href=&quot;#adaptive-computation-time-act&quot;&gt;Adaptive Computation Time&lt;/a&gt;, the approach can be further enhanced to have flexible attention span length, adaptive to the current input dynamically. The span parameter \(z_t\) of an attention head at time \(t\) is a sigmoidal function, \(z_t = S \sigma(\mathbf{v} \cdot \mathbf{x}_t +b)\), where the vector \(\mathbf{v}\) and the bias scalar \(b\) are learned jointly with other parameters.&lt;/p&gt;

&lt;p&gt;In the experiments of Transformer with adaptive attention span, &lt;a href=&quot;https://arxiv.org/abs/1905.07799&quot;&gt;Sukhbaatar, et al. (2019)&lt;/a&gt; found a general tendency that lower layers do not require very long attention spans, while a few attention heads in higher layers may use exceptionally long spans. Adaptive attention span also helps greatly reduce the number of FLOPS, especially in a big model with many attention layers and a large context length.&lt;/p&gt;

&lt;h3 id=&quot;localized-attention-span-image-transformer&quot;&gt;Localized Attention Span (Image Transformer)&lt;/h3&gt;

&lt;p&gt;The original, also the most popular, use case for Transformer is to do language modeling. The text sequence is one-dimensional in a clearly defined chronological order and thus the attention span grows linearly with increased context size.&lt;/p&gt;

&lt;p&gt;However, if we want to use Transformer on images, it is unclear how to define the scope of context or the order. &lt;strong&gt;Image Transformer&lt;/strong&gt; (&lt;a href=&quot;https://arxiv.org/abs/1802.05751&quot;&gt;Parmer, et al 2018&lt;/a&gt;) embraces a formulation of image generation similar to sequence modeling within the Transformer framework. Additionally, Image Transformer restricts the self-attention span to only &lt;em&gt;local&lt;/em&gt; neighborhoods, so that the model can scale up to process more images in parallel and keep the likelihood loss tractable.&lt;/p&gt;

&lt;p&gt;The encoder-decoder architecture remains for image-conditioned generation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The encoder generates a contextualized, per-pixel-channel representation of the source image;&lt;/li&gt;
  &lt;li&gt;The decoder &lt;em&gt;autoregressively&lt;/em&gt; generates an output image, one channel per pixel at each time step.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s label the representation of the current pixel to be generated as the query \(\mathbf{q}\). Other positions whose representations will be used for computing \(\mathbf{q}\) are key vector \(\mathbf{k}_1, \mathbf{k}_2, \dots\) and they together form a memory matrix \(\mathbf{M}\). The scope of \(\mathbf{M}\) defines the context window for pixel query \(\mathbf{q}\).&lt;/p&gt;

&lt;p&gt;Image Transformer introduced two types of localized \(\mathbf{M}\), as illustrated below.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/image-transformer-attention.png&quot; alt=&quot;Attention patterns in Image Transformer&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 9. Illustration of 1D and 2D attention span for visual inputs in Image Transformer. The black line marks a query block and the cyan outlines the actual attention span for pixel q. (Image source: Figure 2 in &lt;a href=&quot;https://arxiv.org/abs/1802.05751&quot;&gt;Parmer et al, 2018&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;(1) &lt;em&gt;1D Local Attention&lt;/em&gt;: The input image is flattened in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Raster_scan#Scanning_pattern&quot;&gt;raster scanning&lt;/a&gt; order, that is, from left to right and top to bottom. The linearized image is then partitioned into non-overlapping query blocks. The context window consists of pixels in the same query block as \(\mathbf{q}\) and a fixed number of additional pixels generated before this query block.&lt;/p&gt;

&lt;p&gt;(2) &lt;em&gt;2D Local Attention&lt;/em&gt;: The image is partitioned into multiple non-overlapping rectangular query blocks. The query pixel can attend to all others in the same memory blocks. To make sure the pixel at the top-left corner can also have a valid context window, the memory block is extended to the top, left and right by a fixed amount, respectively.&lt;/p&gt;

&lt;h2 id=&quot;less-time-and-memory-cost&quot;&gt;Less Time and Memory Cost&lt;/h2&gt;

&lt;p&gt;This section introduces several improvements made on Transformer to reduce the computation time and memory consumption.&lt;/p&gt;

&lt;h3 id=&quot;sparse-attention-matrix-factorization-sparse-transformers&quot;&gt;Sparse Attention Matrix Factorization (Sparse Transformers)&lt;/h3&gt;

&lt;p&gt;The compute and memory cost of the vanilla Transformer grows quadratically with sequence length and thus it is hard to be applied on very long sequences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sparse Transformer&lt;/strong&gt; (&lt;a href=&quot;https://arxiv.org/abs/1904.10509&quot;&gt;Child et al., 2019&lt;/a&gt;) introduced &lt;em&gt;factorized self-attention&lt;/em&gt;, through sparse matrix factorization, making it possible to train dense attention networks with hundreds of layers on sequence length up to 16,384, which would be infeasible on modern hardware otherwise.&lt;/p&gt;

&lt;p&gt;Given a set of attention connectivity pattern \(\mathcal{S} = \{S_1, \dots, S_n\}\), where each \(S_i\) records a set of key positions that the \(i\)-th query vector attends to.&lt;/p&gt;

\[\begin{aligned}
\text{Attend}(\mathbf{X}, \mathcal{S}) &amp;amp;= \Big( a(\mathbf{x}_i, S_i) \Big)_{i \in \{1, \dots, L\}} \\
\text{ where } a(\mathbf{x}_i, S_i) &amp;amp;= \text{softmax}\Big(\frac{(\mathbf{x}_i \mathbf{W}^q)(\mathbf{x}_j \mathbf{W}^k)_{j \in S_i}^\top}{\sqrt{d_k}}\Big) (\mathbf{x}_j \mathbf{W}^v)_{j \in S_i}
\end{aligned}\]

&lt;p&gt;Note that although the size of \(S_i\) is not fixed, \(a(\mathbf{x}_i, S_i)\) is always of size \(d_v\) and thus \(\text{Attend}(\mathbf{X}, \mathcal{S}) \in \mathbb{R}^{L \times d_v}\).&lt;/p&gt;

&lt;p&gt;In anto-regressive models, one attention span is defined as \(S_i = \{j: j \leq i\}\) as it allows each token to attend to all the positions in the past.&lt;/p&gt;

&lt;p&gt;In factorized self-attention, the set \(S_i\) is decomposed into a &lt;em&gt;tree&lt;/em&gt; of dependencies, such that for every pair of \((i, j)\) where \(j \leq i\), there is a path connecting \(i\) back to \(j\) and \(i\) can attend to \(j\) either directly or indirectly.&lt;/p&gt;

&lt;p&gt;Precisely, the set \(S_i\) is divided into \(p\) &lt;em&gt;non-overlapping&lt;/em&gt; subsets, where the \(m\)-th subset is denoted as \(A^{(m)}_i \subset S_i, m = 1,\dots, p\). Therefore the path between the output position \(i\) and any \(j\) has a maximum length \(p + 1\). For example, if \((j, a, b, c, \dots, i)\) is a path of indices between \(i\) and \(j\), we would have \(j \in A_a^{(1)}, a \in A_b^{(2)}, b \in A_c^{(3)}, \dots\), so on and so forth.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sparse Factorized Attention&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Sparse Transformer proposed two types of fractorized attention. It is easier to understand the concepts as illustrated in Fig. 10 with 2D image inputs as examples.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/sparse-attention.png&quot; alt=&quot;Sparse attention&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 10. The top row illustrates the attention connectivity patterns in (a) Transformer, (b) Sparse Transformer with strided attention, and (c) Sparse Transformer with fixed attention. The bottom row contains corresponding self-attention connectivity matrices. Note that the top and bottom rows are not in the same scale. (Image source: &lt;a href=&quot;https://arxiv.org/abs/1904.10509&quot;&gt;Child et al., 2019&lt;/a&gt; + a few of extra annotations.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;(1) &lt;em&gt;Strided&lt;/em&gt; attention with stride \(\ell \sim \sqrt{n}\). This works well with image data as the structure is aligned with strides. In the image case, each pixel would attend to all the previous \(\ell\) pixels in the raster scanning order (naturally cover the entire width of the image) and then those pixels attend to others in the same column (defined by another attention connectivity subset).&lt;/p&gt;

\[\begin{aligned}
A_i^{(1)} &amp;amp;= \{ t, t+1, \dots, i\} \text{, where } t = \max(0, i - \ell) \\
A_i^{(2)} &amp;amp;= \{j: (i-j) \mod \ell = 0\}
\end{aligned}\]

&lt;p&gt;(2) &lt;em&gt;Fixed&lt;/em&gt; attention. A small set of tokens summarize previous locations and propagate that information to all future locations.&lt;/p&gt;

\[\begin{aligned}
A_i^{(1)} &amp;amp;= \{j: \lfloor \frac{j}{\ell} \rfloor = \lfloor \frac{i}{\ell} \rfloor \} \\
A_i^{(2)} &amp;amp;= \{j: j \mod \ell \in \{\ell-c, \dots, \ell-1\} \}
\end{aligned}\]

&lt;p&gt;where \(c\) is a hyperparameter. If \(c=1\), it restricts the representation whereas many depend on a few positions. The paper chose \(c\in \{ 8, 16, 32 \}\) for \(\ell \in \{ 128, 256 \}\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use Factorized Self-Attention in Transformer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are three ways to use sparse factorized attention patterns in Transformer architecture:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;One attention type per residual block and then interleave them, &lt;br /&gt;
\(\text{attention}(\mathbf{X}) = \text{Attend}(\mathbf{X}, A^{(n \mod p)}) \mathbf{W}^o\), where \(n\) is the index of the current residual block.&lt;/li&gt;
  &lt;li&gt;Set up a single head which attends to locations that all the factorized heads attend to, &lt;br /&gt;
\(\text{attention}(\mathbf{X}) = \text{Attend}(\mathbf{X}, \cup_{m=1}^p A^{(m)}) \mathbf{W}^o\).&lt;/li&gt;
  &lt;li&gt;Use a multi-head attention mechanism, but different from vanilla Transformer, each head might adopt a pattern presented above, 1 or 2. =&amp;gt; This option often performs the best.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sparse Transformer also proposed a set of changes so as to train the Transformer up to hundreds of layers, including gradient checkpointing, recomputing attention &amp;amp; FF layers during the backward pass, mixed precision training, efficient block-sparse implementation, etc. Please check the &lt;a href=&quot;https://arxiv.org/abs/1904.10509&quot;&gt;paper&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&quot;locality-sensitive-hashing-reformer&quot;&gt;Locality-Sensitive Hashing (Reformer)&lt;/h3&gt;

&lt;p&gt;The improvements proposed by the &lt;strong&gt;Reformer&lt;/strong&gt; model (&lt;a href=&quot;https://arxiv.org/abs/2001.04451&quot;&gt;Kitaev, et al. 2020&lt;/a&gt;) aim to solve the following pain points in Transformer:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Memory in a model with \(N\) layers is \(N\)-times larger than in a single-layer model because we need to store activations for back-propagation.&lt;/li&gt;
  &lt;li&gt;The intermediate FF layers are often quite large.&lt;/li&gt;
  &lt;li&gt;The attention matrix on sequences of length \(L\) often requires \(O(L^2)\) in both memory and time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reformer proposed two main changes:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Replace the dot-product attention with &lt;em&gt;locality-sensitive hashing (LSH) attention&lt;/em&gt;, reducing the complexity from \(O(L^2)\) to \(O(L\log L)\).&lt;/li&gt;
  &lt;li&gt;Replace the standard residual blocks with &lt;em&gt;reversible residual layers&lt;/em&gt;, which allows storing activations only once during training instead of \(N\) times (i.e. proportional to the number of layers).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a name=&quot;LSH&quot;&gt;&lt;/a&gt;&lt;strong&gt;Locality-Sensitive Hashing Attention&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In \(\mathbf{Q} \mathbf{K}^\top\) part of the &lt;a href=&quot;#attention-and-self-attention&quot;&gt;attention formula&lt;/a&gt;, we are only interested in the largest elements as only large elements contribute a lot after softmax. For each query \(\mathbf{q}_i \in \mathbf{Q}\), we are looking for row vectors in \(\mathbf{K}\) closest to \(\mathbf{q}_i\). In order to find nearest neighbors quickly in high-dimensional space, Reformer incorporates &lt;a href=&quot;https://en.wikipedia.org/wiki/Locality-sensitive_hashing&quot;&gt;Locality-Sensitive Hashing (LSH)&lt;/a&gt; into its attention mechanism.&lt;/p&gt;

&lt;p&gt;A hashing scheme \(x \mapsto h(x)\) is &lt;em&gt;locality-sensitive&lt;/em&gt; if it preserves the distancing information between data points, such that close vectors obtain similar hashes while distant vectors have very different ones. The Reformer adopts a hashing scheme as such, given a fixed random matrix \(\mathbf{R} \in \mathbb{R}^{d \times b/2}\) (where \(b\) is a hyperparam), the hash function is \(h(x) = \arg\max([xR; −xR])\).&lt;/p&gt;

&lt;!-- If we omit the scalar in self-attention and summarize the denominator into a normalizing term $$Z(.)$$, an normal attention output looks as follows:

$$
\mathbf{o}_i = \sum_{j \in S_i} \exp(\mathbf{q}_i \cdot \mathbf{k}_j - Z(i, S_i)) \mathbf{v}_j \text{, where } S_i = \{j: j \leq i\}
$$ 
--&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/LSH-attention-matrix.png&quot; alt=&quot;LSH attention matrix&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 11. Illustration of Locality-Sensitive Hashing (LSH) attention. (Image source: right part of Figure 1 in &lt;a href=&quot;https://arxiv.org/abs/2001.04451&quot;&gt;Kitaev, et al. 2020&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In LSH attention, a query can only attend to positions in the same hashing bucket, \(S_i = \{j: h(\mathbf{q}_i) = h(\mathbf{k}_j)\}\). It is carried out in the following process, as illustrated in Fig. 11:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(a) The attention matrix for full attention is often sparse.&lt;/li&gt;
  &lt;li&gt;(b) Using LSH, we can sort the keys and queries to be aligned according to their hash buckets.&lt;/li&gt;
  &lt;li&gt;(c) Set \(\mathbf{Q} = \mathbf{K}\) (precisely \(\mathbf{k}_j = \mathbf{q}_j / \|\mathbf{q}_j\|\)), so that there are equal numbers of keys and queries in one bucket, easier for batching. Interestingly, this “shared-QK” config does not affect the performance of the Transformer.&lt;/li&gt;
  &lt;li&gt;(d) Apply batching where chunks of \(m\) consecutive queries are grouped together.&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;width: 75%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/LSH-attention.png&quot; alt=&quot;LSH attention&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 12. The LSH attention consists of 4 steps: bucketing, sorting, chunking, and attention computation. (Image source: left part of Figure 1 in &lt;a href=&quot;https://arxiv.org/abs/2001.04451&quot;&gt;Kitaev, et al. 2020&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reversible Residual Network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Another improvement by Reformer is to use &lt;em&gt;reversible residual layers&lt;/em&gt; (&lt;a href=&quot;https://arxiv.org/abs/1707.04585&quot;&gt;Gomez et al. 2017&lt;/a&gt;). The motivation for reversible residual network is to design the architecture in a way that activations at any given layer can be recovered from the activations at the following layer, using only the model parameters. Hence, we can save memory by recomputing the activation during backprop rather than storing all the activations.&lt;/p&gt;

&lt;p&gt;Given a layer \(x \mapsto y\), the normal residual layer does \(y = x + F(x)\), but the reversible layer splits both input and output into pairs \((x_1, x_2) \mapsto (y_1, y_2)\) and then executes the following:&lt;/p&gt;

\[y_1 = x_1 + F(x_2),\; y_2 = x_2 + G(y_1)\]

&lt;p&gt;and reversing is easy:&lt;/p&gt;

\[x_2 = y_2 - G(y_1), \; x_1 = y_1 − F(x_2)\]

&lt;p&gt;Reformer applies the same idea to Transformer by combination attention (\(F\)) and feed-forward layers (\(G\)) within a reversible net block:&lt;/p&gt;

\[Y_1 = X_1 + \text{Attention}(X_2), \; Y_2 = X_2 + \text{FeedForward}(Y_1)\]

&lt;p&gt;The memory can be further reduced by chunking the feed-forward computation:
\(Y_2 = [Y_2^{(1)}; \dots; Y_2^{(c)}] = [X_2^{(1)} + \text{FeedForward}(Y_1^{(1)}); \dots; X_2^{(c)} + \text{FeedForward}(Y_1^{(c)})]\)&lt;/p&gt;

&lt;p&gt;The resulting reversible Transformer does not need to store activation in every layer.&lt;/p&gt;

&lt;h2 id=&quot;make-it-recurrent-universal-transformer&quot;&gt;Make it Recurrent (Universal Transformer)&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Universal Transformer&lt;/strong&gt; (&lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;&gt;Dehghani, et al. 2019&lt;/a&gt;) combines self-attention in Transformer with the recurrent mechanism in RNN, aiming to benefit from both a long-term global receptive field of Transformer and learned inductive biases of RNN.&lt;/p&gt;

&lt;p&gt;Rather than going through a fixed number of layers, Universal Transformer dynamically adjusts the number of steps using &lt;a href=&quot;#adaptive-computation-time-act&quot;&gt;adaptive computation time&lt;/a&gt;. If we fix the number of steps, an Universal Transformer is equivalent to a multi-layer Transformer with shared parameters across layers.&lt;/p&gt;

&lt;p&gt;On a high level, the universal transformer can be viewed as a recurrent function for learning the hidden state representation per token. The recurrent function evolves in parallel across token positions and the information between positions is shared through self-attention.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/universal-transformer-loop.png&quot; alt=&quot;Universal Transformer Recurrent Step&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 13. How the Universal Transformer refines a set of hidden state representations repeatedly for every position in parallel. (Image source: Figure 1 in &lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;&gt;Dehghani, et al. 2019&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Given an input sequence of length \(L\), Universal Transformer iteratively updates the representation \(\mathbf{H}^t \in \mathbb{R}^{L \times d}\) at step \(t\) for an adjustable number of steps. At step 0, \(\mathbf{H}^0\) is initialized to be same as the input embedding matrix. All the positions are processed in parallel in the multi-head self-attention mechanism and then go through a recurrent transition function.&lt;/p&gt;

\[\begin{aligned}
\mathbf{A}^t &amp;amp;= \text{LayerNorm}(\mathbf{H}^{t-1} + \text{MultiHeadAttention}(\mathbf{H}^{t-1} + \mathbf{P}^t) \\
\mathbf{H}^t &amp;amp;= \text{LayerNorm}(\mathbf{A}^{t-1} + \text{Transition}(\mathbf{A}^t))
\end{aligned}\]

&lt;p&gt;where \(\text{Transition}(.)\) is either a &lt;a href=&quot;https://arxiv.org/abs/1610.02357&quot;&gt;separable convolution&lt;/a&gt; or a fully-connected neural network that consists of two position-wise (i.e. applied to each row of \(\mathbf{A}^t\) individually) affine transformation + one ReLU.&lt;/p&gt;

&lt;p&gt;The positional encoding \(\mathbf{P}^t\) uses sinusoidal position signal but with an additional time dimension:&lt;/p&gt;

\[\text{PE}(i, t, \delta) = 
\begin{cases}
\sin(\frac{i}{10000^{2\delta'/d}}) \oplus \sin(\frac{t}{10000^{2\delta'/d}}) &amp;amp; \text{if } \delta = 2\delta'\\
\cos(\frac{i}{10000^{2\delta'/d}}) \oplus \cos(\frac{t}{10000^{2\delta'/d}}) &amp;amp; \text{if } \delta = 2\delta' + 1\\
\end{cases}\]

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/universal-transformer.png&quot; alt=&quot;Universal Transformer&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 14. A simplified illustration of Universal Transformer. The encoder and decoder share the same basic recurrent structure. But the decoder also attends to final encoder representation \(\mathbf{H}^T\). (Image source: Figure 2 in &lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;&gt;Dehghani, et al. 2019&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the adaptive version of Universal Transformer, the number of recurrent steps \(T\) is dynamically determined by &lt;a href=&quot;#adaptive-computation-time-act&quot;&gt;ACT&lt;/a&gt;. Each position is equipped with a dynamic ACT halting mechanism. Once a per-token recurrent block halts, it stops taking more recurrent updates but simply copies the current value to the next step until all the blocks halt or until the model reaches a maximum step limit.&lt;/p&gt;

&lt;h2 id=&quot;stabilization-for-rl-gtrxl&quot;&gt;Stabilization for RL (GTrXL)&lt;/h2&gt;

&lt;p&gt;The self-attention mechanism avoids compressing the whole past into a fixed-size hidden state and does not suffer from vanishing or exploding gradients as much as RNNs. Reinforcement Learning tasks can for sure benefit from these traits. &lt;em&gt;However&lt;/em&gt;, it is quite difficult to train Transformer even in supervised learning, let alone in the RL context. It could be quite challenging to stabilize and train a LSTM agent by itself, after all.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Gated Transformer-XL&lt;/strong&gt; (&lt;strong&gt;GTrXL&lt;/strong&gt;; &lt;a href=&quot;https://arxiv.org/abs/1910.06764&quot;&gt;Parisotto, et al. 2019&lt;/a&gt;) is one attempt to use Transformer for RL. GTrXL succeeded in stabilizing training with two changes on top of &lt;a href=&quot;#longer-attention-span-transformer-xl&quot;&gt;Transformer-XL&lt;/a&gt;:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The layer normalization is only applied on the input stream in a residual module, but NOT on the shortcut stream. A key benefit to this reordering is to allow the original input to flow from the first to last layer.&lt;/li&gt;
  &lt;li&gt;The residual connection is replaced with a GRU-style (Gated Recurrent Unit; &lt;a href=&quot;https://arxiv.org/abs/1412.3555&quot;&gt;Chung et al., 2014&lt;/a&gt;) &lt;em&gt;gating&lt;/em&gt; mechanism.&lt;/li&gt;
&lt;/ol&gt;

\[\begin{aligned}
r &amp;amp;= \sigma(W_r^{(l)} y + U_r^{(l)} x) \\
z &amp;amp;= \sigma(W_z^{(l)} y + U_z^{(l)} x - b_g^{(l)}) \\
\hat{h} &amp;amp;= \tanh(W_g^{(l)} y + U_g^{(l)} (r \odot x)) \\
g^{(l)}(x, y) &amp;amp;= (1-z)\odot x + z\odot \hat{h}
\end{aligned}\]

&lt;p&gt;The gating function parameters are explicitly initialized to be close to an identity map - this is why there is a \(b_g\) term. A \(b_g &amp;gt; 0\) greatly helps with the learning speedup.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/gated-transformer-XL.png&quot; alt=&quot;GTrXL&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fig. 15. Comparison of the model architecture of Transformer-XL, Transformer-XL with the layer norm reordered, and Gated Transformer-XL. (Image source: Figure 1 in &lt;a href=&quot;https://arxiv.org/abs/1910.06764&quot;&gt;Parisotto, et al. 2019&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Cited as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{weng2020transformer,
  title   = &quot;The Transformer Family&quot;,
  author  = &quot;Weng, Lilian&quot;,
  journal = &quot;lilianweng.github.io/lil-log&quot;,
  year    = &quot;2020&quot;,
  url     = &quot;https://lilianweng.github.io/lil-log/2020/03/27/the-transformer-family.html&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;[1] Ashish Vaswani, et al. &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;“Attention is all you need.”&lt;/a&gt; NIPS 2017.&lt;/p&gt;

&lt;p&gt;[2] Rami Al-Rfou, et al. &lt;a href=&quot;https://arxiv.org/abs/1808.04444&quot;&gt;“Character-level language modeling with deeper self-attention.”&lt;/a&gt; AAAI 2019.&lt;/p&gt;

&lt;p&gt;[3] Olah &amp;amp; Carter, &lt;a href=&quot;http://doi.org/10.23915/disti&quot;&gt;“Attention and Augmented Recurrent Neural Networks”&lt;/a&gt;, Distill, 2016.&lt;/p&gt;

&lt;p&gt;[4] Sainbayar Sukhbaatar, et al. &lt;a href=&quot;https://arxiv.org/abs/1905.07799&quot;&gt;“Adaptive Attention Span in Transformers”&lt;/a&gt;. ACL 2019.&lt;/p&gt;

&lt;p&gt;[5] Rewon Child, et al. &lt;a href=&quot;https://arxiv.org/abs/1904.10509&quot;&gt;“Generating Long Sequences with Sparse Transformers”&lt;/a&gt; arXiv:1904.10509 (2019).&lt;/p&gt;

&lt;p&gt;[6] Nikita Kitaev, et al. &lt;a href=&quot;https://arxiv.org/abs/2001.04451&quot;&gt;“Reformer: The Efficient Transformer”&lt;/a&gt; ICLR 2020.&lt;/p&gt;

&lt;p&gt;[7] Alex Graves. (“Adaptive Computation Time for Recurrent Neural Networks”)[https://arxiv.org/abs/1603.08983]&lt;/p&gt;

&lt;p&gt;[8] Niki Parmar, et al. &lt;a href=&quot;https://arxiv.org/abs/1802.05751&quot;&gt;“Image Transformer”&lt;/a&gt; ICML 2018.&lt;/p&gt;

&lt;p&gt;[9] Zihang Dai, et al. &lt;a href=&quot;https://arxiv.org/abs/1901.02860&quot;&gt;“Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.”&lt;/a&gt; ACL 2019.&lt;/p&gt;

&lt;p&gt;[10] Aidan N. Gomez, et al. &lt;a href=&quot;https://arxiv.org/abs/1707.04585&quot;&gt;“The Reversible Residual Network: Backpropagation Without Storing Activations”&lt;/a&gt; NIPS 2017.&lt;/p&gt;

&lt;p&gt;[11] Mostafa Dehghani, et al. &lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;&gt;“Universal Transformers”&lt;/a&gt; ICLR 2019.&lt;/p&gt;

&lt;p&gt;[12] Emilio Parisotto, et al. &lt;a href=&quot;https://arxiv.org/abs/1910.06764&quot;&gt;“Stabilizing Transformers for Reinforcement Learning”&lt;/a&gt; arXiv:1910.06764 (2019).&lt;/p&gt;

&lt;p&gt;Source : &lt;a href=&quot;https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html&quot;&gt;Lilian&lt;/a&gt;&lt;/p&gt;</content><author><name>Lilian</name></author><category term="attention" /><category term="transformer" /><category term="reinforcement-learning" /><summary type="html">Inspired by recent progress on various enhanced versions of Transformer models, this post presents how the vanilla Transformer can be improved for longer-term attention span, less memory and computation consumption, RL task solving, etc.</summary></entry><entry><title type="html">Python is Cool</title><link href="http://localhost:4000/blog/python/2019/12/27/python-is-cool.html" rel="alternate" type="text/html" title="Python is Cool" /><published>2019-12-27T11:04:17+07:00</published><updated>2019-12-27T11:04:17+07:00</updated><id>http://localhost:4000/blog/python/2019/12/27/python-is-cool</id><content type="html" xml:base="http://localhost:4000/blog/python/2019/12/27/python-is-cool.html">&lt;h2 id=&quot;1-lambda-map-filter-reduce&quot;&gt;1. Lambda, map, filter, reduce&lt;/h2&gt;
&lt;p&gt;The lambda keyword is used to create inline functions. The functions&lt;code&gt;square_fn&lt;/code&gt; and &lt;code&gt;square_ld&lt;/code&gt; below are identical.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def square_fn(x):
    return x * x

square_ld = lambda x: x * x

for i in range(10):
    assert square_fn(i) == square_ld(i)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Its quick declaration makes &lt;code&gt;lambda&lt;/code&gt; functions ideal for use in callbacks, and when functions are to be passed as arguments to other functions. They are especially useful when used in conjunction with functions like &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, and &lt;code&gt;reduce&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;map(fn, iterable)&lt;/code&gt; applies the &lt;code&gt;fn&lt;/code&gt; to all elements of the &lt;code&gt;iterable&lt;/code&gt; (e.g. list, set, dictionary, tuple, string) and returns a map object.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;nums = [1/3, 333/7, 2323/2230, 40/34, 2/3]
nums_squared = [num * num for num in nums]
print(nums_squared)

==&amp;gt; [0.1111111, 2263.04081632, 1.085147, 1.384083, 0.44444444]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the same as calling using &lt;code&gt;map&lt;/code&gt; with a callback function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;nums_squared_1 = map(square_fn, nums)
nums_squared_2 = map(lambda x: x * x, nums)
print(list(nums_squared_1))

==&amp;gt; [0.1111111, 2263.04081632, 1.085147, 1.384083, 0.44444444]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also use &lt;code&gt;map&lt;/code&gt; with more than one iterable. For example, if you want to calculate the mean squared error of a simple linear function &lt;code&gt;f(x) = ax + b&lt;/code&gt; with the true label &lt;code&gt;labels&lt;/code&gt;, these two methods are equivalent:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;a, b = 3, -0.5
xs = [2, 3, 4, 5]
labels = [6.4, 8.9, 10.9, 15.3]

# Method 1: using a loop
errors = []
for i, x in enumerate(xs):
    errors.append((a * x + b - labels[i]) ** 2)
result1 = sum(errors) ** 0.5 / len(xs)

# Method 2: using map
diffs = map(lambda x, y: (a * x + b - y) ** 2, xs, labels)
result2 = sum(diffs) ** 0.5 / len(xs)

print(result1, result2)

==&amp;gt; 0.35089172119045514 0.35089172119045514
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that objects returned by &lt;code&gt;map&lt;/code&gt; and &lt;code&gt;filter&lt;/code&gt; are iterators, which means that their values aren’t stored but generated as needed. After you’ve called &lt;code&gt;sum(diffs)&lt;/code&gt;, &lt;code&gt;diffs&lt;/code&gt; becomes empty. If you want to keep all elements in &lt;code&gt;diffs&lt;/code&gt;, convert it to a list using &lt;code&gt;list(diffs)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;filter(fn, iterable)&lt;/code&gt; works the same way as &lt;code&gt;map&lt;/code&gt;, except that &lt;code&gt;fn&lt;/code&gt; returns a boolean value and &lt;code&gt;filter&lt;/code&gt; returns all the elements of the &lt;code&gt;iterable&lt;/code&gt; for which the &lt;code&gt;fn&lt;/code&gt; returns True.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;bad_preds = filter(lambda x: x &amp;gt; 0.5, errors)
print(list(bad_preds))

==&amp;gt; [0.8100000000000006, 0.6400000000000011]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;reduce(fn, iterable, initializer)&lt;/code&gt; is used when we want to iteratively apply an operator to all elements in a list. For example, if we want to calculate the product of all elements in a list:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;product = 1
for num in nums:
    product *= num
print(product)

==&amp;gt; 12.95564683272412
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is equivalent to:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from functools import reduce
product = reduce(lambda x, y: x * y, nums)
print(product)

==&amp;gt; 12.95564683272412
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;note-on-the-performance-of-lambda-functions&quot;&gt;Note on the performance of lambda functions&lt;/h3&gt;

&lt;p&gt;Lambda functions are meant for one time use. Each time &lt;code&gt;lambda x: dosomething(x)&lt;/code&gt; is called, the function has to be created, which hurts the performance if you call &lt;code&gt;lambda x: dosomething(x)&lt;/code&gt; multiple times (e.g. when you pass it inside &lt;code&gt;reduce&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;When you assign a name to the lambda function as in &lt;code&gt;fn = lambda x: dosomething(x)&lt;/code&gt;, its performance is slightly slower than the same function defined using &lt;code&gt;def&lt;/code&gt;, but the difference is negligible. See &lt;a href=&quot;https://stackoverflow.com/questions/26540885/lambda-is-slower-than-function-call-in-python-why&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Even though I find lambdas cool, I personally recommend using named functions when you can for the sake of clarity.&lt;/p&gt;

&lt;h2 id=&quot;2-list-manipulation&quot;&gt;2. List manipulation&lt;/h2&gt;
&lt;p&gt;Python lists are super cool.&lt;/p&gt;

&lt;h3 id=&quot;21-unpacking&quot;&gt;2.1 Unpacking&lt;/h3&gt;
&lt;p&gt;We can unpack a list by each element like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;elems = [1, 2, 3, 4]
a, b, c, d = elems
print(a, b, c, d)

==&amp;gt; 1 2 3 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also unpack a list like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;a, *new_elems, d = elems
print(a)
print(new_elems)
print(d)

==&amp;gt; 1
    [2, 3]
    4
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;22-slicing&quot;&gt;2.2 Slicing&lt;/h3&gt;
&lt;p&gt;We know that we can reverse a list using &lt;code&gt;[::-1]&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;elems = list(range(10))
print(elems)

==&amp;gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

print(elems[::-1])

==&amp;gt; [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The syntax &lt;code&gt;[x:y:z]&lt;/code&gt; means &quot;take every &lt;code&gt;z&lt;/code&gt;th element of a list from index &lt;code&gt;x&lt;/code&gt; to index &lt;code&gt;y&lt;/code&gt;&quot;. When &lt;code&gt;z&lt;/code&gt; is negative, it indicates going backwards. When &lt;code&gt;x&lt;/code&gt; isn’t specified, it defaults to the first element of the list in the direction you are traversing the list. When &lt;code&gt;y&lt;/code&gt; isn’t specified, it defaults to the last element of the list. So if we want to take every 2th element of a list, we use &lt;code&gt;[::2]&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;evens = elems[::2]
print(evens)

reversed_evens = elems[-2::-2]
print(reversed_evens)

==&amp;gt; [0, 2, 4, 6, 8]
    [8, 6, 4, 2, 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also use slicing to delete all the even numbers in the list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;del elems[::2]
print(elems)

==&amp;gt; [1, 3, 5, 7, 9]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;23-insertion&quot;&gt;2.3 Insertion&lt;/h3&gt;
&lt;p&gt;We can change the value of an element in a list to another value.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;elems = list(range(10))
elems[1] = 10
print(elems)

==&amp;gt; [0, 10, 2, 3, 4, 5, 6, 7, 8, 9]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we want to replace the element at an index with multiple elements, e.g. replace the value &lt;code&gt;1&lt;/code&gt; with 3 values &lt;code&gt;20, 30, 40&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;elems = list(range(10))
elems[1:2] = [20, 30, 40]
print(elems)

==&amp;gt; [0, 20, 30, 40, 2, 3, 4, 5, 6, 7, 8, 9]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we want to insert 3 values &lt;code&gt;0.2, 0.3, 0.5&lt;/code&gt; between element at index 0 and element at index 1:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;elems = list(range(10))
elems[1:1] = [0.2, 0.3, 0.5]
print(elems)

==&amp;gt; [0, 0.2, 0.3, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;24-flattening&quot;&gt;2.4 Flattening&lt;/h3&gt;
&lt;p&gt;We can flatten a list of lists using &lt;code&gt;sum&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;list_of_lists = [[1], [2, 3], [4, 5, 6]]
sum(list_of_lists, [])

==&amp;gt; [1, 2, 3, 4, 5, 6]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we have nested lists, we can recursively flatten it. That’s another beauty of lambda functions – we can use it in the same line as its creation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;nested_lists = [[1, 2], [[3, 4], [5, 6], [[7, 8], [9, 10], [[11, [12, 13]]]]]]
flatten = lambda x: [y for l in x for y in flatten(l)] if type(x) is list else [x]
flatten(nested_lists)

# This line of code is from
# https://github.com/sahands/python-by-example/blob/master/python-by-example.rst#flattening-lists
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;25-list-vs-generator&quot;&gt;2.5 List vs generator&lt;/h3&gt;
&lt;p&gt;To illustrate the difference between a list and a generator, let’s look at an example of creating n-grams out of a list of tokens.&lt;/p&gt;

&lt;p&gt;One way to create n-grams is to use a sliding window.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;tokens = ['i', 'want', 'to', 'go', 'to', 'school']

def ngrams(tokens, n):
    length = len(tokens)
    grams = []
    for i in range(length - n + 1):
        grams.append(tokens[i:i+n])
    return grams

print(ngrams(tokens, 3))

==&amp;gt; [['i', 'want', 'to'],
     ['want', 'to', 'go'],
     ['to', 'go', 'to'],
     ['go', 'to', 'school']]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the above example, we have to store all the n-grams at the same time. If the text has m tokens, then the memory requirement is &lt;code&gt;O(nm)&lt;/code&gt;, which can be problematic when m is large.&lt;/p&gt;

&lt;p&gt;Instead of using a list to store all n-grams, we can use a generator that generates the next n-gram when it’s asked for. This is known as lazy evaluation. We can make the function &lt;code&gt;ngrams&lt;/code&gt; returns a generator using the keyword &lt;code&gt;yield&lt;/code&gt;. Then the memory requirement is &lt;code&gt;O(m+n)&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def ngrams(tokens, n):
    length = len(tokens)
    for i in range(length - n + 1):
        yield tokens[i:i+n]

ngrams_generator = ngrams(tokens, 3)
print(ngrams_generator)

==&amp;gt; &amp;lt;generator object ngrams at 0x1069b26d0&amp;gt;

for ngram in ngrams_generator:
    print(ngram)

==&amp;gt; ['i', 'want', 'to']
    ['want', 'to', 'go']
    ['to', 'go', 'to']
    ['go', 'to', 'school']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another way to generate n-grams is to use slices to create lists: &lt;code&gt;[0, 1, ..., -n]&lt;/code&gt;, &lt;code&gt;[1, 2, ..., -n+1]&lt;/code&gt;, …, &lt;code&gt;[n-1, n, ..., -1]&lt;/code&gt;, and then &lt;code&gt;zip&lt;/code&gt; them together.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def ngrams(tokens, n):
    length = len(tokens)
    slices = (tokens[i:length-n+i+1] for i in range(n))
    return zip(*slices)

ngrams_generator = ngrams(tokens, 3)
print(ngrams_generator)

==&amp;gt; &amp;lt;zip object at 0x1069a7dc8&amp;gt; # zip objects are generators

for ngram in ngrams_generator:
    print(ngram)

==&amp;gt; ('i', 'want', 'to')
    ('want', 'to', 'go')
    ('to', 'go', 'to')
    ('go', 'to', 'school')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that to create slices, we use &lt;code&gt;(tokens[...] for i in range(n))&lt;/code&gt; instead of &lt;code&gt;[tokens[...] for i in range(n)]&lt;/code&gt;. &lt;code&gt;[]&lt;/code&gt; is the normal list comprehension that returns a list. &lt;code&gt;()&lt;/code&gt; returns a generator.&lt;/p&gt;

&lt;h2 id=&quot;3-classes-and-magic-methods&quot;&gt;3. Classes and magic methods&lt;/h2&gt;
&lt;p&gt;In Python, magic methods are prefixed and suffixed with the double underscore &lt;code&gt;__&lt;/code&gt;, also known as dunder. The most wellknown magic method is probably &lt;code&gt;__init__&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Node:
    &quot;&quot;&quot; A struct to denote the node of a binary tree.
    It contains a value and pointers to left and right children.
    &quot;&quot;&quot;
    def __init__(self, value, left=None, right=None):
        self.value = value
        self.left = left
        self.right = right
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we try to print out a Node object, however, it’s not very interpretable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;root = Node(5)
print(root) # &amp;lt;__main__.Node object at 0x1069c4518&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ideally, when user prints out a node, we want to print out the node’s value and the values of its children if it has children. To do so, we use the magic method &lt;code&gt;__repr__&lt;/code&gt;, which must return a printable object, like string.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Node:
    &quot;&quot;&quot; A struct to denote the node of a binary tree.
    It contains a value and pointers to left and right children.
    &quot;&quot;&quot;
    def __init__(self, value, left=None, right=None):
        self.value = value
        self.left = left
        self.right = right

    def __repr__(self):
        strings = [f'value: {self.value}']
        strings.append(f'left: {self.left.value}' if self.left else 'left: None')
        strings.append(f'right: {self.right.value}' if self.right else 'right: None')
        return ', '.join(strings)

left = Node(4)
root = Node(5, left)
print(root) # value: 5, left: 4, right: None
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We’d also like to compare two nodes by comparing their values. To do so, we overload the operator &lt;code&gt;==&lt;/code&gt; with &lt;code&gt;__eq__&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt; with &lt;code&gt;__lt__&lt;/code&gt;, and &lt;code&gt;&amp;gt;=&lt;/code&gt; with &lt;code&gt;__ge__&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Node:
    &quot;&quot;&quot; A struct to denote the node of a binary tree.
    It contains a value and pointers to left and right children.
    &quot;&quot;&quot;
    def __init__(self, value, left=None, right=None):
        self.value = value
        self.left = left
        self.right = right

    def __eq__(self, other):
        return self.value == other.value

    def __lt__(self, other):
        return self.value &amp;lt; other.value

    def __ge__(self, other):
        return self.value &amp;gt;= other.value


left = Node(4)
root = Node(5, left)
print(left == root) # False
print(left &amp;lt; root) # True
print(left &amp;gt;= root) # False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For a comprehensive list of supported magic methods &lt;a href=&quot;https://www.tutorialsteacher.com/python/magic-methods-in-python&quot;&gt;here&lt;/a&gt; or see the official Python documentation &lt;a href=&quot;https://docs.python.org/3/reference/datamodel.html#special-method-names&quot;&gt;here&lt;/a&gt; (slightly harder to read).&lt;/p&gt;

&lt;p&gt;Some of the methods that I highly recommend:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;__len__&lt;/code&gt;: to overload the &lt;code&gt;len()&lt;/code&gt; function.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;__str__&lt;/code&gt;: to overload the &lt;code&gt;str()&lt;/code&gt; function.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;__iter__&lt;/code&gt;: if you want to your objects to be iterators. This also allows you to call &lt;code&gt;next()&lt;/code&gt; on your object.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For classes like Node where we know for sure all the attributes they can support (in the case of Node, they are &lt;code&gt;value&lt;/code&gt;, &lt;code&gt;left&lt;/code&gt;, and &lt;code&gt;right&lt;/code&gt;), we might want to use &lt;code&gt;__slots__&lt;/code&gt; to denote those values for both performance boost and memory saving. For a comprehensive understanding of pros and cons of &lt;code&gt;__slots__&lt;/code&gt;, see this &lt;a href=&quot;https://stackoverflow.com/a/28059785/5029595&quot;&gt;absolutely amazing answer by Aaron Hall on StackOverflow&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Node:
    &quot;&quot;&quot; A struct to denote the node of a binary tree.
    It contains a value and pointers to left and right children.
    &quot;&quot;&quot;
    __slots__ = ('value', 'left', 'right')
    def __init__(self, value, left=None, right=None):
        self.value = value
        self.left = left
        self.right = right
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;4-local-namespace-objects-attributes&quot;&gt;4. local namespace, object’s attributes&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;locals()&lt;/code&gt; function returns a dictionary containing the variables defined in the local namespace.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Model1:
    def __init__(self, hidden_size=100, num_layers=3, learning_rate=3e-4):
        print(locals())
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.learning_rate = learning_rate

model1 = Model1()

==&amp;gt; {'learning_rate': 0.0003, 'num_layers': 3, 'hidden_size': 100, 'self': &amp;lt;__main__.Model1 object at 0x1069b1470&amp;gt;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All attributes of an object are stored in its &lt;code&gt;__dict__&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;print(model1.__dict__)

==&amp;gt; {'hidden_size': 100, 'num_layers': 3, 'learning_rate': 0.0003}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that manually assigning each of the arguments to an attribute can be quite tiring when the list of the arguments is large. To avoid this, we can directly assign the list of arguments to the object’s &lt;code&gt;__dict__&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Model2:
    def __init__(self, hidden_size=100, num_layers=3, learning_rate=3e-4):
        params = locals()
        del params['self']
        self.__dict__ = params

model2 = Model2()
print(model2.__dict__)

==&amp;gt; {'learning_rate': 0.0003, 'num_layers': 3, 'hidden_size': 100}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be especially convenient when the object is initiated using the catch-all &lt;code&gt;**kwargs&lt;/code&gt;, though the use of &lt;code&gt;**kwargs&lt;/code&gt; should be reduced to the minimum.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;class Model3:
    def __init__(self, **kwargs):
        self.__dict__ = kwargs

model3 = Model3(hidden_size=100, num_layers=3, learning_rate=3e-4)
print(model3.__dict__)

==&amp;gt; {'hidden_size': 100, 'num_layers': 3, 'learning_rate': 0.0003}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;5-wild-import&quot;&gt;5. Wild import&lt;/h2&gt;
&lt;p&gt;Often, you run into this wild import &lt;code&gt;*&lt;/code&gt; that looks something like this:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;file.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;    from parts import *
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is irresponsible because it will import everything in module, even the imports of that module. For example, if &lt;code&gt;parts.py&lt;/code&gt; looks like this:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;parts.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy
import tensorflow

class Encoder:
    ...

class Decoder:
    ...

class Loss:
    ...

def helper(*args, **kwargs):
    ...

def utils(*args, **kwargs):
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since &lt;code&gt;parts.py&lt;/code&gt; doesn’t have &lt;code&gt;__all__&lt;/code&gt; specified, &lt;code&gt;file.py&lt;/code&gt; will import Encoder, Decoder, Loss, utils, helper together with numpy and tensorflow.&lt;/p&gt;

&lt;p&gt;If we intend that only Encoder, Decoder, and Loss are ever to be imported and used in another module, we should specify that in &lt;code&gt;parts.py&lt;/code&gt; using the &lt;code&gt;__all__&lt;/code&gt; keyword.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;parts.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; __all__ = ['Encoder', 'Decoder', 'Loss']
import numpy
import tensorflow

class Encoder:
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, if some user irresponsibly does a wild import with &lt;code&gt;parts&lt;/code&gt;, they can only import Encoder, Decoder, Loss. Personally, I also find &lt;code&gt;__all__&lt;/code&gt; helpful as it gives me an overview of the module.&lt;/p&gt;

&lt;h2 id=&quot;6-decorator-to-time-your-functions&quot;&gt;6. Decorator to time your functions&lt;/h2&gt;
&lt;p&gt;It’s often useful to know how long it takes a function to run, e.g. when you need to compare the performance of two algorithms that do the same thing. One naive way is to call &lt;code&gt;time.time()&lt;/code&gt; at the begin and end of each function and print out the difference.&lt;/p&gt;

&lt;p&gt;For example: compare two algorithms to calculate the n-th Fibonacci number, one uses memoization and one doesn’t.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def fib_helper(n):
    if n &amp;lt; 2:
        return n
    return fib_helper(n - 1) + fib_helper(n - 2)

def fib(n):
    &quot;&quot;&quot; fib is a wrapper function so that later we can change its behavior
    at the top level without affecting the behavior at every recursion step.
    &quot;&quot;&quot;
    return fib_helper(n)

def fib_m_helper(n, computed):
    if n in computed:
        return computed[n]
    computed[n] = fib_m_helper(n - 1, computed) + fib_m_helper(n - 2, computed)
    return computed[n]

def fib_m(n):
    return fib_m_helper(n, {0: 0, 1: 1})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s make sure that &lt;code&gt;fib&lt;/code&gt; and &lt;code&gt;fib_m&lt;/code&gt; are functionally equivalent.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;for n in range(20):
    assert fib(n) == fib_m(n)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import time

start = time.time()
fib(30)
print(f'Without memoization, it takes {time.time() - start:7f} seconds.')

==&amp;gt; Without memoization, it takes 0.267569 seconds.

start = time.time()
fib_m(30)
print(f'With memoization, it takes {time.time() - start:.7f} seconds.')

==&amp;gt; With memoization, it takes 0.0000713 seconds.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to time multiple functions, it can be a drag having to write the same code over and over again. It’d be nice to have a way to specify how to change any function in the same way. In this case would be to call time.time() at the beginning and the end of each function, and print out the time difference.&lt;/p&gt;

&lt;p&gt;This is exactly what decorators do. They allow programmers to change the behavior of a function or class. Here’s an example to create a decorator &lt;code&gt;timeit&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def timeit(fn): 
    # *args and **kwargs are to support positional and named arguments of fn
    def get_time(*args, **kwargs): 
        start = time.time() 
        output = fn(*args, **kwargs)
        print(f&quot;Time taken in {fn.__name__}: {time.time() - start:.7f}&quot;)
        return output  # make sure that the decorator returns the output of fn
    return get_time 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add the decorator &lt;code&gt;@timeit&lt;/code&gt; to your functions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;@timeit
def fib(n):
    return fib_helper(n)

@timeit
def fib_m(n):
    return fib_m_helper(n, {0: 0, 1: 1})

fib(30)
fib_m(30)

==&amp;gt; Time taken in fib: 0.2787242
==&amp;gt; Time taken in fib_m: 0.0000138
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;7-caching-with-functoolslru_cache&quot;&gt;7. Caching with @functools.lru_cache&lt;/h2&gt;
&lt;p&gt;Memoization is a form of cache: we cache the previously calculated Fibonacci numbers so that we don’t have to calculate them again.&lt;/p&gt;

&lt;p&gt;Caching is such an important technique that Python provides a built-in decorator to give your function the caching capacity. If you want &lt;code&gt;fib_helper&lt;/code&gt; to reuse the previously calculated Fibonacci numbers, you can just add the decorator &lt;code&gt;lru_cache&lt;/code&gt; from &lt;code&gt;functools&lt;/code&gt;. &lt;code&gt;lru&lt;/code&gt; stands for “least recently used”. For more information on cache, see &lt;a href=&quot;https://docs.python.org/3/library/functools.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import functools

@functools.lru_cache()
def fib_helper(n):
    if n &amp;lt; 2:
        return n
    return fib_helper(n - 1) + fib_helper(n - 2)

@timeit
def fib(n):
    &quot;&quot;&quot; fib is a wrapper function so that later we can change its behavior
    at the top level without affecting the behavior at every recursion step.
    &quot;&quot;&quot;
    return fib_helper(n)

fib(50)
fib_m(50)

==&amp;gt; Time taken in fib: 0.0000412
==&amp;gt; Time taken in fib_m: 0.0000281
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Source : &lt;a href=&quot;https://github.com/chiphuyen/python-is-cool&quot;&gt;Huyen Chip&lt;/a&gt;&lt;/p&gt;</content><author><name>Huyen Chip</name></author><category term="python" /><summary type="html">1. Lambda, map, filter, reduce The lambda keyword is used to create inline functions. The functionssquare_fn and square_ld below are identical. def square_fn(x): return x * x square_ld = lambda x: x * x for i in range(10): assert square_fn(i) == square_ld(i) Its quick declaration makes lambda functions ideal for use in callbacks, and when functions are to be passed as arguments to other functions. They are especially useful when used in conjunction with functions like map, filter, and reduce. map(fn, iterable) applies the fn to all elements of the iterable (e.g. list, set, dictionary, tuple, string) and returns a map object. nums = [1/3, 333/7, 2323/2230, 40/34, 2/3] nums_squared = [num * num for num in nums] print(nums_squared) ==&amp;gt; [0.1111111, 2263.04081632, 1.085147, 1.384083, 0.44444444] This is the same as calling using map with a callback function. nums_squared_1 = map(square_fn, nums) nums_squared_2 = map(lambda x: x * x, nums) print(list(nums_squared_1)) ==&amp;gt; [0.1111111, 2263.04081632, 1.085147, 1.384083, 0.44444444] You can also use map with more than one iterable. For example, if you want to calculate the mean squared error of a simple linear function f(x) = ax + b with the true label labels, these two methods are equivalent: a, b = 3, -0.5 xs = [2, 3, 4, 5] labels = [6.4, 8.9, 10.9, 15.3] # Method 1: using a loop errors = [] for i, x in enumerate(xs): errors.append((a * x + b - labels[i]) ** 2) result1 = sum(errors) ** 0.5 / len(xs) # Method 2: using map diffs = map(lambda x, y: (a * x + b - y) ** 2, xs, labels) result2 = sum(diffs) ** 0.5 / len(xs) print(result1, result2) ==&amp;gt; 0.35089172119045514 0.35089172119045514 Note that objects returned by map and filter are iterators, which means that their values aren’t stored but generated as needed. After you’ve called sum(diffs), diffs becomes empty. If you want to keep all elements in diffs, convert it to a list using list(diffs). filter(fn, iterable) works the same way as map, except that fn returns a boolean value and filter returns all the elements of the iterable for which the fn returns True. bad_preds = filter(lambda x: x &amp;gt; 0.5, errors) print(list(bad_preds)) ==&amp;gt; [0.8100000000000006, 0.6400000000000011] reduce(fn, iterable, initializer) is used when we want to iteratively apply an operator to all elements in a list. For example, if we want to calculate the product of all elements in a list: product = 1 for num in nums: product *= num print(product) ==&amp;gt; 12.95564683272412 This is equivalent to: from functools import reduce product = reduce(lambda x, y: x * y, nums) print(product) ==&amp;gt; 12.95564683272412 Note on the performance of lambda functions Lambda functions are meant for one time use. Each time lambda x: dosomething(x) is called, the function has to be created, which hurts the performance if you call lambda x: dosomething(x) multiple times (e.g. when you pass it inside reduce). When you assign a name to the lambda function as in fn = lambda x: dosomething(x), its performance is slightly slower than the same function defined using def, but the difference is negligible. See here. Even though I find lambdas cool, I personally recommend using named functions when you can for the sake of clarity. 2. List manipulation Python lists are super cool. 2.1 Unpacking We can unpack a list by each element like this: elems = [1, 2, 3, 4] a, b, c, d = elems print(a, b, c, d) ==&amp;gt; 1 2 3 4 We can also unpack a list like this: a, *new_elems, d = elems print(a) print(new_elems) print(d) ==&amp;gt; 1 [2, 3] 4 2.2 Slicing We know that we can reverse a list using [::-1]. elems = list(range(10)) print(elems) ==&amp;gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] print(elems[::-1]) ==&amp;gt; [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] The syntax [x:y:z] means &quot;take every zth element of a list from index x to index y&quot;. When z is negative, it indicates going backwards. When x isn’t specified, it defaults to the first element of the list in the direction you are traversing the list. When y isn’t specified, it defaults to the last element of the list. So if we want to take every 2th element of a list, we use [::2]. evens = elems[::2] print(evens) reversed_evens = elems[-2::-2] print(reversed_evens) ==&amp;gt; [0, 2, 4, 6, 8] [8, 6, 4, 2, 0] We can also use slicing to delete all the even numbers in the list. del elems[::2] print(elems) ==&amp;gt; [1, 3, 5, 7, 9] 2.3 Insertion We can change the value of an element in a list to another value. elems = list(range(10)) elems[1] = 10 print(elems) ==&amp;gt; [0, 10, 2, 3, 4, 5, 6, 7, 8, 9] If we want to replace the element at an index with multiple elements, e.g. replace the value 1 with 3 values 20, 30, 40: elems = list(range(10)) elems[1:2] = [20, 30, 40] print(elems) ==&amp;gt; [0, 20, 30, 40, 2, 3, 4, 5, 6, 7, 8, 9] If we want to insert 3 values 0.2, 0.3, 0.5 between element at index 0 and element at index 1: elems = list(range(10)) elems[1:1] = [0.2, 0.3, 0.5] print(elems) ==&amp;gt; [0, 0.2, 0.3, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9] 2.4 Flattening We can flatten a list of lists using sum. list_of_lists = [[1], [2, 3], [4, 5, 6]] sum(list_of_lists, []) ==&amp;gt; [1, 2, 3, 4, 5, 6] If we have nested lists, we can recursively flatten it. That’s another beauty of lambda functions – we can use it in the same line as its creation. nested_lists = [[1, 2], [[3, 4], [5, 6], [[7, 8], [9, 10], [[11, [12, 13]]]]]] flatten = lambda x: [y for l in x for y in flatten(l)] if type(x) is list else [x] flatten(nested_lists) # This line of code is from # https://github.com/sahands/python-by-example/blob/master/python-by-example.rst#flattening-lists 2.5 List vs generator To illustrate the difference between a list and a generator, let’s look at an example of creating n-grams out of a list of tokens. One way to create n-grams is to use a sliding window. tokens = ['i', 'want', 'to', 'go', 'to', 'school'] def ngrams(tokens, n): length = len(tokens) grams = [] for i in range(length - n + 1): grams.append(tokens[i:i+n]) return grams print(ngrams(tokens, 3)) ==&amp;gt; [['i', 'want', 'to'], ['want', 'to', 'go'], ['to', 'go', 'to'], ['go', 'to', 'school']] In the above example, we have to store all the n-grams at the same time. If the text has m tokens, then the memory requirement is O(nm), which can be problematic when m is large. Instead of using a list to store all n-grams, we can use a generator that generates the next n-gram when it’s asked for. This is known as lazy evaluation. We can make the function ngrams returns a generator using the keyword yield. Then the memory requirement is O(m+n). def ngrams(tokens, n): length = len(tokens) for i in range(length - n + 1): yield tokens[i:i+n] ngrams_generator = ngrams(tokens, 3) print(ngrams_generator) ==&amp;gt; &amp;lt;generator object ngrams at 0x1069b26d0&amp;gt; for ngram in ngrams_generator: print(ngram) ==&amp;gt; ['i', 'want', 'to'] ['want', 'to', 'go'] ['to', 'go', 'to'] ['go', 'to', 'school'] Another way to generate n-grams is to use slices to create lists: [0, 1, ..., -n], [1, 2, ..., -n+1], …, [n-1, n, ..., -1], and then zip them together. def ngrams(tokens, n): length = len(tokens) slices = (tokens[i:length-n+i+1] for i in range(n)) return zip(*slices) ngrams_generator = ngrams(tokens, 3) print(ngrams_generator) ==&amp;gt; &amp;lt;zip object at 0x1069a7dc8&amp;gt; # zip objects are generators for ngram in ngrams_generator: print(ngram) ==&amp;gt; ('i', 'want', 'to') ('want', 'to', 'go') ('to', 'go', 'to') ('go', 'to', 'school') Note that to create slices, we use (tokens[...] for i in range(n)) instead of [tokens[...] for i in range(n)]. [] is the normal list comprehension that returns a list. () returns a generator. 3. Classes and magic methods In Python, magic methods are prefixed and suffixed with the double underscore __, also known as dunder. The most wellknown magic method is probably __init__. class Node: &quot;&quot;&quot; A struct to denote the node of a binary tree. It contains a value and pointers to left and right children. &quot;&quot;&quot; def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right When we try to print out a Node object, however, it’s not very interpretable. root = Node(5) print(root) # &amp;lt;__main__.Node object at 0x1069c4518&amp;gt; Ideally, when user prints out a node, we want to print out the node’s value and the values of its children if it has children. To do so, we use the magic method __repr__, which must return a printable object, like string. class Node: &quot;&quot;&quot; A struct to denote the node of a binary tree. It contains a value and pointers to left and right children. &quot;&quot;&quot; def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right def __repr__(self): strings = [f'value: {self.value}'] strings.append(f'left: {self.left.value}' if self.left else 'left: None') strings.append(f'right: {self.right.value}' if self.right else 'right: None') return ', '.join(strings) left = Node(4) root = Node(5, left) print(root) # value: 5, left: 4, right: None We’d also like to compare two nodes by comparing their values. To do so, we overload the operator == with __eq__, &amp;lt; with __lt__, and &amp;gt;= with __ge__. class Node: &quot;&quot;&quot; A struct to denote the node of a binary tree. It contains a value and pointers to left and right children. &quot;&quot;&quot; def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right def __eq__(self, other): return self.value == other.value def __lt__(self, other): return self.value &amp;lt; other.value def __ge__(self, other): return self.value &amp;gt;= other.value left = Node(4) root = Node(5, left) print(left == root) # False print(left &amp;lt; root) # True print(left &amp;gt;= root) # False For a comprehensive list of supported magic methods here or see the official Python documentation here (slightly harder to read). Some of the methods that I highly recommend: __len__: to overload the len() function. __str__: to overload the str() function. __iter__: if you want to your objects to be iterators. This also allows you to call next() on your object. For classes like Node where we know for sure all the attributes they can support (in the case of Node, they are value, left, and right), we might want to use __slots__ to denote those values for both performance boost and memory saving. For a comprehensive understanding of pros and cons of __slots__, see this absolutely amazing answer by Aaron Hall on StackOverflow. class Node: &quot;&quot;&quot; A struct to denote the node of a binary tree. It contains a value and pointers to left and right children. &quot;&quot;&quot; __slots__ = ('value', 'left', 'right') def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right 4. local namespace, object’s attributes The locals() function returns a dictionary containing the variables defined in the local namespace. class Model1: def __init__(self, hidden_size=100, num_layers=3, learning_rate=3e-4): print(locals()) self.hidden_size = hidden_size self.num_layers = num_layers self.learning_rate = learning_rate model1 = Model1() ==&amp;gt; {'learning_rate': 0.0003, 'num_layers': 3, 'hidden_size': 100, 'self': &amp;lt;__main__.Model1 object at 0x1069b1470&amp;gt;} All attributes of an object are stored in its __dict__. print(model1.__dict__) ==&amp;gt; {'hidden_size': 100, 'num_layers': 3, 'learning_rate': 0.0003} Note that manually assigning each of the arguments to an attribute can be quite tiring when the list of the arguments is large. To avoid this, we can directly assign the list of arguments to the object’s __dict__. class Model2: def __init__(self, hidden_size=100, num_layers=3, learning_rate=3e-4): params = locals() del params['self'] self.__dict__ = params model2 = Model2() print(model2.__dict__) ==&amp;gt; {'learning_rate': 0.0003, 'num_layers': 3, 'hidden_size': 100} This can be especially convenient when the object is initiated using the catch-all **kwargs, though the use of **kwargs should be reduced to the minimum. class Model3: def __init__(self, **kwargs): self.__dict__ = kwargs model3 = Model3(hidden_size=100, num_layers=3, learning_rate=3e-4) print(model3.__dict__) ==&amp;gt; {'hidden_size': 100, 'num_layers': 3, 'learning_rate': 0.0003} 5. Wild import Often, you run into this wild import * that looks something like this: file.py from parts import * This is irresponsible because it will import everything in module, even the imports of that module. For example, if parts.py looks like this: parts.py import numpy import tensorflow class Encoder: ... class Decoder: ... class Loss: ... def helper(*args, **kwargs): ... def utils(*args, **kwargs): ... Since parts.py doesn’t have __all__ specified, file.py will import Encoder, Decoder, Loss, utils, helper together with numpy and tensorflow. If we intend that only Encoder, Decoder, and Loss are ever to be imported and used in another module, we should specify that in parts.py using the __all__ keyword. parts.py __all__ = ['Encoder', 'Decoder', 'Loss'] import numpy import tensorflow class Encoder: ... Now, if some user irresponsibly does a wild import with parts, they can only import Encoder, Decoder, Loss. Personally, I also find __all__ helpful as it gives me an overview of the module. 6. Decorator to time your functions It’s often useful to know how long it takes a function to run, e.g. when you need to compare the performance of two algorithms that do the same thing. One naive way is to call time.time() at the begin and end of each function and print out the difference. For example: compare two algorithms to calculate the n-th Fibonacci number, one uses memoization and one doesn’t. def fib_helper(n): if n &amp;lt; 2: return n return fib_helper(n - 1) + fib_helper(n - 2) def fib(n): &quot;&quot;&quot; fib is a wrapper function so that later we can change its behavior at the top level without affecting the behavior at every recursion step. &quot;&quot;&quot; return fib_helper(n) def fib_m_helper(n, computed): if n in computed: return computed[n] computed[n] = fib_m_helper(n - 1, computed) + fib_m_helper(n - 2, computed) return computed[n] def fib_m(n): return fib_m_helper(n, {0: 0, 1: 1}) Let’s make sure that fib and fib_m are functionally equivalent. for n in range(20): assert fib(n) == fib_m(n) import time start = time.time() fib(30) print(f'Without memoization, it takes {time.time() - start:7f} seconds.') ==&amp;gt; Without memoization, it takes 0.267569 seconds. start = time.time() fib_m(30) print(f'With memoization, it takes {time.time() - start:.7f} seconds.') ==&amp;gt; With memoization, it takes 0.0000713 seconds. If you want to time multiple functions, it can be a drag having to write the same code over and over again. It’d be nice to have a way to specify how to change any function in the same way. In this case would be to call time.time() at the beginning and the end of each function, and print out the time difference. This is exactly what decorators do. They allow programmers to change the behavior of a function or class. Here’s an example to create a decorator timeit. def timeit(fn): # *args and **kwargs are to support positional and named arguments of fn def get_time(*args, **kwargs): start = time.time() output = fn(*args, **kwargs) print(f&quot;Time taken in {fn.__name__}: {time.time() - start:.7f}&quot;) return output # make sure that the decorator returns the output of fn return get_time Add the decorator @timeit to your functions. @timeit def fib(n): return fib_helper(n) @timeit def fib_m(n): return fib_m_helper(n, {0: 0, 1: 1}) fib(30) fib_m(30) ==&amp;gt; Time taken in fib: 0.2787242 ==&amp;gt; Time taken in fib_m: 0.0000138 7. Caching with @functools.lru_cache Memoization is a form of cache: we cache the previously calculated Fibonacci numbers so that we don’t have to calculate them again. Caching is such an important technique that Python provides a built-in decorator to give your function the caching capacity. If you want fib_helper to reuse the previously calculated Fibonacci numbers, you can just add the decorator lru_cache from functools. lru stands for “least recently used”. For more information on cache, see here. import functools @functools.lru_cache() def fib_helper(n): if n &amp;lt; 2: return n return fib_helper(n - 1) + fib_helper(n - 2) @timeit def fib(n): &quot;&quot;&quot; fib is a wrapper function so that later we can change its behavior at the top level without affecting the behavior at every recursion step. &quot;&quot;&quot; return fib_helper(n) fib(50) fib_m(50) ==&amp;gt; Time taken in fib: 0.0000412 ==&amp;gt; Time taken in fib_m: 0.0000281 Source : Huyen Chip</summary></entry><entry><title type="html">Convolution Neural Network : A Modular Perspective</title><link href="http://localhost:4000/blog/deep-learning/2019/04/25/ConvNets-Modular.html" rel="alternate" type="text/html" title="Convolution Neural Network : A Modular Perspective" /><published>2019-04-25T07:04:17+07:00</published><updated>2019-04-25T07:04:17+07:00</updated><id>http://localhost:4000/blog/deep-learning/2019/04/25/ConvNets-Modular</id><content type="html" xml:base="http://localhost:4000/blog/deep-learning/2019/04/25/ConvNets-Modular.html">&lt;p&gt;Source : &lt;a href=&quot;https://colah.github.io/posts/2014-07-Conv-Nets-Modular/&quot;&gt;https://colah.github.io/posts/2014-07-Conv-Nets-Modular/&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#structure-of-convolutional-neural-networks&quot; id=&quot;markdown-toc-structure-of-convolutional-neural-networks&quot;&gt;Structure of Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#results-of-convolutional-neural-networks&quot; id=&quot;markdown-toc-results-of-convolutional-neural-networks&quot;&gt;Results of Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#formalizing-convolutional-neural-networks&quot; id=&quot;markdown-toc-formalizing-convolutional-neural-networks&quot;&gt;Formalizing Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#acknowledgments&quot; id=&quot;markdown-toc-acknowledgments&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;!-- Trong vài năm gần đây, mạng neural networks đã có kết quả đột phá trên rất nhiều vấn đề nhận dạng mẫu như thị giác máy tính, nhận dạng giọng nói. Một trong những yếu tố cơ bản dẫn đến kết quả này là
một loại đặc biệt của neural network được gọi là *convolution neural network* (mạng neural tích chập) .

Cơ bản mà nói, convolution neural network có thể được xem như một loại mạng neural mà sử dụng rất nhiều bản sao chép giống hệt nhau của cùng một neuron.
Điều này cho phép network này có thể có nhiều neurons và thực hiện tính toán mô hình lớn trong khi vẫn giữ được số lượng tham số thực sự - giá trị mô tả cách mà những neuron có - mà cần được học không thiên vị.
--&gt;

&lt;p&gt;In the last few years, deep neural networks have lead to breakthrough results on a variety of pattern recognition problems, such as computer vision and voice recognition. One of the essential components leading to these results has been a special kind of neural network called a convolutional neural network.&lt;/p&gt;

&lt;p&gt;At its most basic, convolutional neural networks can be thought of as a kind of neural network that uses many identical copies of the same neuron&lt;sup&gt;&lt;a href=&quot;#eef9:fn:1&quot; class=&quot;footnote&quot; id=&quot;eef9:fn-back:1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. This allows the network to have lots of neurons and express computationally large models while keeping the number of actual parameters – the values describing how neurons behave – that need to be learned fairly small.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-9x5-Conv2Conv2.png&quot; alt=&quot;Conv2Conv2&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;center&quot;&gt;&lt;em&gt;A 2D Convolutional Neural Network&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
This trick of having multiple copies of the same neuron is roughly analogous to the abstraction of functions in mathematics and computer science. When programming, we write a function once and use it in many places – not writing the same code a hundred times in different places makes it faster to program, and results in fewer bugs. Similarly, a convolutional neural network can learn a neuron once and use it in many places, making it easier to learn the model and reducing error.&lt;/p&gt;

&lt;!-- Kỹ thuật để sao chép nhiều lần của cùng một neuron được hiểu đại khác tương tự như quá trình trừu tượng hóa một hàm trong toán học và khoa học máy tính. Khi lập trình, chúng ta viết hàm một lần và sử dụng chúng ở nhiều nơi - chúng ta không viết cùng một đoạn code hàng trăm lần khác nhau
khiến cho chúng nhanh hơn để lập trình và kết quả ít lỗi. Tương tự, một mạng neurol tích chập có thể học một neuron một lần và sử dụng
nó ở nhiều nơi, làm cho chúng dễ dàng hơn để học mô hình và giảm thiểu lỗi.

# Cấu trúc của những mạng neurol tích chập (Convolution neural networks)
--&gt;

&lt;h1 id=&quot;structure-of-convolutional-neural-networks&quot;&gt;Structure of Convolutional Neural Networks&lt;/h1&gt;

&lt;p&gt;Suppose you want a neural network to look at audio samples and predict whether a human is speaking or not. Maybe you want to do more analysis if someone is speaking.&lt;/p&gt;

&lt;p&gt;You get audio samples at different points in time. The samples are evenly spaced.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-xs.png&quot; alt=&quot;Conv-9-xs&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The simplest way to try and classify them with a neural network is to just connect them all to a fully-connected layer.
There are a bunch of different neurons, and every input connects to every neuron.&lt;/p&gt;

&lt;p style=&quot;width: 90%; padding: 20px 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-F.png&quot; alt=&quot;Conv-9-F&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;width: 100%; padding: 20px 10px;&quot; class=&quot;center&quot;&gt;&lt;/p&gt;

&lt;p&gt;A more sophisticated approach notices a kind of &lt;em&gt;symmetry&lt;/em&gt; in the properties it’s useful to look for in the data. We care a lot about local properties of the data: What frequency of sounds are there around a given time? Are they increasing or decreasing? And so on.&lt;/p&gt;

&lt;p&gt;We care about the same properties at all points in time. It’s useful to know the frequencies at the beginning, it’s useful to know the frequencies in the middle, and it’s also useful to know the frequencies at the end. Again, note that these are local properties, in that we only need to look at a small window of the audio sample in order to determine them.&lt;/p&gt;

&lt;p&gt;So, we can create a group of neurons, $A$, that look at small time segments of our data&lt;sup&gt;&lt;a href=&quot;#eef9:fn:2&quot; class=&quot;footnote&quot; id=&quot;eef9:fn-back:2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; $A$ looks at all such segments, computing certain features.
Then, the output of this convolutional layer is fed into a fully-connected layer, $F$.&lt;/p&gt;

&lt;p style=&quot;width: 90%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-Conv2.png&quot; alt=&quot;Conv-9-Conv2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above example, $A$ only looked at segments consisting of two points. This isn’t realistic. Usually, a convolution layer’s window would be much larger.&lt;/p&gt;

&lt;p&gt;In the following example, $A$ looks at 3 points.
That isn’t realistic either – sadly, it’s tricky to visualize $A$ connecting to lots of points.&lt;/p&gt;

&lt;p style=&quot;width: 90%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-Conv3.png&quot; alt=&quot;Conv-9-Conv3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One very nice property of convolutional layers is that they’re composable. You can feed the output of one convolutional layer into another. With each layer, the network can detect higher-level, more abstract features.&lt;/p&gt;

&lt;p&gt;In the following example, we have a new group of neurons, $B$. $B$ is used to create another convolutional layer stacked on top of the previous one.&lt;/p&gt;

&lt;p style=&quot;width: 90%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-Conv2Conv2.png&quot; alt=&quot;Conv-9-Conv2Conv2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Convolutional layers are often interweaved with pooling layers. In particular, there is a kind of layer called a max-pooling layer that is extremely popular.&lt;/p&gt;

&lt;p&gt;Often, from a high level perspective, we don’t care about the precise point in time a feature is present. If a shift in frequency occurs slightly earlier or later, does it matter?&lt;/p&gt;

&lt;p&gt;A max-pooling layer takes the maximum of features over small blocks of a previous layer. The output tells us if a feature was present in a region of the previous layer, but not precisely where.&lt;/p&gt;

&lt;p&gt;Max-pooling layers kind of “zoom out”. They allow later convolutional layers to work on larger sections of the data, because a small patch after the pooling layer corresponds to a much larger patch before it. They also make us invariant to some very small transformations of the data.&lt;/p&gt;

&lt;p style=&quot;width: 90%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-Conv2Max2Conv2.png&quot; alt=&quot;Conv-9-Conv2Max2Conv2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p style=&quot;width: 40%; padding: 20 10px; float:right;&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-unit.png&quot; alt=&quot;Conv2-unit&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In our previous examples, we’ve used 1-dimensional convolutional layers. However, convolutional layers can work on higher-dimensional data as well. In fact, the most famous successes of convolutional neural networks are applying 2D convolutional neural networks to recognizing images.&lt;/p&gt;

&lt;p&gt;In a 2-dimensional convolutional layer, instead of looking at segments, $A$ will now look at patches.&lt;/p&gt;

&lt;p&gt;For each patch, $A$ will compute features. For example, it might learn to detect the presence of an edge. Or it might learn to detect a texture. Or perhaps a contrast between two colors.&lt;/p&gt;

&lt;p style=&quot;width: 80%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-9x5-Conv2.png&quot; alt=&quot;Conv2-9x5-Conv2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the previous example, we fed the output of our convolutional layer into a fully-connected layer. But we can also compose two convolutional layers, as we did in the one dimensional case.&lt;/p&gt;

&lt;p style=&quot;width: 80%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-9x5-Conv2Conv2.png&quot; alt=&quot;Conv2-9x5-Conv2Conv2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We can also do max pooling in two dimensions. Here, we take the maximum of features over a small patch.&lt;/p&gt;

&lt;p&gt;What this really boils down to is that, when considering an entire image, we don’t care about the exact position of an edge, down to a pixel. It’s enough to know where it is to within a few pixels.&lt;/p&gt;

&lt;p style=&quot;width:80%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-9x5-Conv2Max2Conv2.png&quot; alt=&quot;Conv2-9x5-Conv2Max2Conv2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Three-dimensional convolutional networks are also sometimes used, for data like videos or volumetric data (eg. 3D medical scans). However, they are not very widely used, and much harder to visualize.&lt;/p&gt;

&lt;p&gt;Now, we previously said that $A$ was a group of neurons. We should be a bit more precise about this: what is $A$ exactly?&lt;/p&gt;

&lt;p&gt;In traditional convolutional layers, $A$ is a bunch of neurons in parallel, that all get the same inputs and compute different features.&lt;/p&gt;

&lt;p&gt;For example, in a 2-dimensional convolutional layer, one neuron might detect horizontal edges, another might detect vertical edges, and another might detect green-red color contrasts.&lt;/p&gt;

&lt;p style=&quot;width: 80%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-A.png&quot; alt=&quot;Conv-A&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That said, in the recent paper ‘Network in Network’ (&lt;a href=&quot;https://arxiv.org/abs/1312.4400&quot;&gt;Lin et al. (2013)&lt;/a&gt;), a new “Mlpconv” layer is proposed.
In this model, $A$ would have multiple layers of neurons, with the final layer outputting higher level features for the region. In the paper, the model achieves some very impressive results, setting new state of the art on a number of benchmark datasets.&lt;/p&gt;

&lt;p style=&quot;width: 80%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-A-NIN.png&quot; alt=&quot;Conv-A-NIN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That said, for the purposes of this post, we will focus on standard convolutional layers. There’s already enough for us to consider there!&lt;/p&gt;

&lt;h1 id=&quot;results-of-convolutional-neural-networks&quot;&gt;Results of Convolutional Neural Networks&lt;/h1&gt;

&lt;p&gt;Earlier, we alluded to recent breakthroughs in computer vision using convolutional neural networks. Before we go on, I’d like to briefly discuss some of these results as motivation.&lt;/p&gt;

&lt;p&gt;In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton blew existing image classification results out of the water (&lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&quot;&gt;Krizehvsky et al. (2012)&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Their progress was the result of combining together a bunch of different pieces.
They used GPUs to train a very large, deep, neural network.
They used a new kind of neuron (ReLUs) and a new technique to reduce a problem called ‘overfitting’ (DropOut).
They used a very large dataset with lots of image categories (&lt;a href=&quot;http://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt;). And, of course, it was a convolutional neural network.&lt;/p&gt;

&lt;p&gt;Their architecture, illustrated below, was very deep.
It has 5 convolutional layers&lt;sup&gt;&lt;a href=&quot;#eef9:fn:3&quot; class=&quot;footnote&quot; id=&quot;eef9:fn-back:3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, with pooling interspersed, and three fully-connected layers.
The early layers are split over the two GPUs.&lt;/p&gt;

&lt;p style=&quot;width: 80%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/KSH-arch.png&quot; alt=&quot;KSH-arch&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;em&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&quot;&gt;From Krizehvsky et al. (2012)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;They trained their network to classify images into a thousand different categories.&lt;/p&gt;

&lt;p&gt;Randomly guessing, one would guess the correct answer 0.1% of the time.
Krizhevsky, et al.’s model is able to give the right answer 63% of the time.
Further, one of the top 5 answers it gives is right 85% of the time!&lt;/p&gt;

&lt;p style=&quot;padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/KSH-results.png&quot; alt=&quot;KSH-results&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;center&quot;&gt;&lt;em&gt;Top: 4 correctly classified examples.
Bottom: 4 incorrectly classified examples.
Each example has an image, followed by its label, followed by the top 5 guesses with probabilities.
From &lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&quot;&gt;Krizehvsky et al. (2012)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Even some of its errors seem pretty reasonable to me!&lt;/p&gt;

&lt;p&gt;We can also examine what the first layer of the network learns to do.&lt;/p&gt;

&lt;p&gt;Recall that the convolutional layers were split between the two GPUs. Information doesn’t go back and forth each layer, so the split sides are disconnected in a real way. It turns out that, every time the model is run, the two sides specialize.&lt;/p&gt;

&lt;p style=&quot;padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/KSH-filters.png&quot; alt=&quot;KSH-filters&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;center&quot;&gt;&lt;em&gt;Filters learned by the first convolutional layer. The top half corresponds to the layer on one GPU, the bottom on the other. From &lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&quot;&gt;Krizehvsky et al. (2012)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Neurons in one side focus on black and white, learning to detect edges of different orientations and sizes.
Neurons on the other side specialize on color and texture, detecting color contrasts and patterns&lt;sup&gt;&lt;a href=&quot;#eef9:fn:4&quot; class=&quot;footnote&quot; id=&quot;eef9:fn-back:4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.
Remember that the neurons are &lt;em&gt;randomly&lt;/em&gt; initialized. No human went and set them to be edge detectors, or to split in this way. It arose simply from training the network to classify images.&lt;/p&gt;

&lt;p&gt;These remarkable results (and other exciting results around that time) were only the beginning. They were quickly followed by a lot of other work testing modified approaches and gradually improving the results, or applying them to other areas. And, in addition to the neural networks community, many in the computer vision community have adopted deep convolutional neural networks.&lt;/p&gt;

&lt;p&gt;Convolutional neural networks are an essential tool in computer vision and modern pattern recognition.&lt;/p&gt;

&lt;h1 id=&quot;formalizing-convolutional-neural-networks&quot;&gt;Formalizing Convolutional Neural Networks&lt;/h1&gt;

&lt;p&gt;Consider a 1-dimensional convolutional layer with inputs ${x_n}$ and outputs  ${y_n}$&lt;/p&gt;

&lt;p style=&quot;padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv-9-Conv2-XY.png&quot; alt=&quot;Conv-9-Conv2-XY&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s relatively easy to describe the outputs in terms of the inputs:&lt;/p&gt;

\[y_n = A(x_n,x_{n+1},...)\]

&lt;p&gt;For example, in the above:&lt;/p&gt;

\[y_0 = A(x_0,x_1)\]

\[y_1 = A(x_1,x_2)\]

&lt;p&gt;Similarly, if we consider a 2-dimensional convolutional layer, with inputs ${x_n,m}$ and outputs ${y_n,m}$:&lt;/p&gt;

&lt;p style=&quot;width: 80%; padding: 20 10px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-08-06-ConvNets-Modular/Conv2-5x5-Conv2-XY.png&quot; alt=&quot;Conv2-5x5-Conv2-XY&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We can, again, write down the outputs in terms of the inputs:&lt;/p&gt;

\[y_{n,m} = A\begin{pmatrix}
           x_{n, m}, &amp;amp; x_{n+1, m} &amp;amp; ..., \\
           x_{n, m+1}, &amp;amp; x_{n+1, m+1}, &amp;amp; ..., \\
           &amp;amp; ...,
           \end{pmatrix}\]

&lt;p&gt;For example:&lt;/p&gt;

\[y_{0, 0} = A\begin{pmatrix}
    x_{0, 0}, &amp;amp;x_{1, 0}, \\
    x_{0,1}, &amp;amp; x_{1, 1}
    \end{pmatrix}\]

\[y_{1, 0} = A\begin{pmatrix}
    x_{1, 0}, &amp;amp; x_{2, 0}, \\
    x_{1,1}, &amp;amp; x_{2, 1}
    \end{pmatrix}\]

&lt;p&gt;If one combines this with the equation for $A(x)$,&lt;/p&gt;

\[A(x) = \sigma(Wx + b)\]

&lt;p&gt;one has everything they need to implement a convolutional neural network, at least in theory.&lt;/p&gt;

&lt;p&gt;In practice, this is often not best way to think about convolutional neural networks.
There is an alternative formulation, in terms of a mathematical operation called &lt;em&gt;convolution&lt;/em&gt;, that is often more helpful.&lt;/p&gt;

&lt;p&gt;The convolution operation is a powerful tool. In mathematics, it comes up in diverse contexts, ranging from the study of partial differential equations to probability theory. In part because of its role in PDEs, convolution is very important in the physical sciences. It also has an important role in many applied areas, like computer graphics and signal processing.&lt;/p&gt;

&lt;p&gt;For,[^1] us, convolution will provide a number of benefits. Firstly, it will allow us to create much more efficient implementations of convolutional layers than the naive perspective might suggest. Secondly, it will remove a lot of messiness from our formulation, handling all the bookkeeping presently showing up in the indexing of xs – the present formulation may not seem messy yet, but that’s only because we haven’t got into the tricky cases yet. Finally, convolution will give us a significantly different perspective for reasoning about convolutional layers.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I admire the elegance of your method of computation; it must be nice to ride through these fields upon the horse of true mathematics while the like of us have to make our way laboriously on foot.  — Albert Einstein&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h1&gt;

&lt;ol class=&quot;footnotelist&quot;&gt;
&lt;li id=&quot;eef9:fn:1&quot; class=&quot;footnotebody&quot; value=&quot;1&quot;&gt;
        It should be noted that not all neural networks that use multiple copies of the same neuron are convolutional neural networks. Convolutional neural networks are just one type of neural network that uses the more general trick, weight-tying. Other kinds of neural network that do this are recurrent neural networks and recursive neural networks.
   &lt;a href=&quot;#eef9:fn-back:1&quot; class=&quot;backlink&quot;&gt;⏎&lt;/a&gt;&lt;/li&gt;
&lt;li id=&quot;eef9:fn:2&quot; class=&quot;footnotebody&quot; value=&quot;2&quot;&gt;
      Groups of neurons, like $A$, that appear in multiple places are sometimes called modules, and networks that use them are sometimes called modular neural networks
   &lt;a href=&quot;#eef9:fn-back:2&quot; class=&quot;backlink&quot;&gt;⏎&lt;/a&gt;&lt;/li&gt;
&lt;li id=&quot;eef9:fn:3&quot; class=&quot;footnotebody&quot; value=&quot;3&quot;&gt;
     Groups of neurons, like $A$, that appear in multiple places are sometimes called modules, and networks that use them are sometimes called modular neural networks
  &lt;a href=&quot;#eef9:fn-back:3&quot; class=&quot;backlink&quot;&gt;⏎&lt;/a&gt;&lt;/li&gt;
&lt;li id=&quot;eef9:fn:4&quot; class=&quot;footnotebody&quot; value=&quot;4&quot;&gt;
       They also test using 7 in the paper.
    &lt;a href=&quot;#eef9:fn-back:4&quot; class=&quot;backlink&quot;&gt;⏎&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Christopher Olah</name></author><category term="deep-learning" /><summary type="html">Source : https://colah.github.io/posts/2014-07-Conv-Nets-Modular/</summary></entry><entry><title type="html">A Recipe for Training Neural Networks</title><link href="http://localhost:4000/blog/deep-learning/2019/04/25/recipe-for-training-neural-network.html" rel="alternate" type="text/html" title="A Recipe for Training Neural Networks" /><published>2019-04-25T07:04:17+07:00</published><updated>2019-04-25T07:04:17+07:00</updated><id>http://localhost:4000/blog/deep-learning/2019/04/25/recipe-for-training-neural-network</id><content type="html" xml:base="http://localhost:4000/blog/deep-learning/2019/04/25/recipe-for-training-neural-network.html">&lt;!--more--&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-neural-net-training-is-a-leaky-abstraction&quot; id=&quot;markdown-toc-1-neural-net-training-is-a-leaky-abstraction&quot;&gt;1) Neural net training is a leaky abstraction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-neural-net-training-fails-silently&quot; id=&quot;markdown-toc-2-neural-net-training-fails-silently&quot;&gt;2) Neural net training fails silently&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-recipe&quot; id=&quot;markdown-toc-the-recipe&quot;&gt;The recipe&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1-become-one-with-the-data&quot; id=&quot;markdown-toc-1-become-one-with-the-data&quot;&gt;1. Become one with the data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines&quot; id=&quot;markdown-toc-2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines&quot;&gt;2. Set up the end-to-end training/evaluation skeleton + get dumb baselines&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-overfit&quot; id=&quot;markdown-toc-3-overfit&quot;&gt;3. Overfit&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-regularize&quot; id=&quot;markdown-toc-4-regularize&quot;&gt;4. Regularize&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5-tune&quot; id=&quot;markdown-toc-5-tune&quot;&gt;5. Tune&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#6-squeeze-out-the-juice&quot; id=&quot;markdown-toc-6-squeeze-out-the-juice&quot;&gt;6. Squeeze out the juice&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;1-neural-net-training-is-a-leaky-abstraction&quot;&gt;1) Neural net training is a leaky abstraction&lt;/h4&gt;

&lt;p&gt;It is allegedly easy to get started with training neural nets. Numerous libraries and frameworks take pride in displaying 30-line miracle snippets that solve your data problems, giving the (false) impression that this stuff is plug and play. It’s common see things like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; your_data = # plug your awesome dataset here
&amp;gt;&amp;gt;&amp;gt; model = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)
# conquer world here
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These libraries and examples activate the part of our brain that is familiar with standard software - a place where clean APIs and abstractions are often attainable. &lt;a href=&quot;http://docs.python-requests.org/en/master/&quot;&gt;Requests&lt;/a&gt; library to demonstrate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
&amp;gt;&amp;gt;&amp;gt; r.status_code
200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That’s cool! A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are familiar with and expect. Unfortunately, neural nets are nothing like that. They are not “off-the-shelf” technology the second you deviate slightly from training an ImageNet classifier. I’ve tried to make this point in my post &lt;a href=&quot;https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b&quot;&gt;“Yes you should understand backprop”&lt;/a&gt; by picking on backpropagation and calling it a “leaky abstraction”, but the situation is unfortunately much more dire. Backprop + SGD does not magically make your network work. Batch norm does not magically make it converge faster. RNNs don’t magically let you “plug in” text. And just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail. Which brings me to…&lt;/p&gt;

&lt;h4 id=&quot;2-neural-net-training-fails-silently&quot;&gt;2) Neural net training fails silently&lt;/h4&gt;

&lt;p&gt;When you break or misconfigure code you will often get some kind of an exception. You plugged in an integer where something expected a string. The function only expected 3 arguments. This import failed. That key does not exist. The number of elements in the two lists isn’t equal. In addition, it’s often possible to create unit tests for a certain functionality.&lt;/p&gt;

&lt;p&gt;This is just a start when it comes to training neural nets. Everything could be correct syntactically, but the whole thing isn’t arranged properly, and it’s really hard to tell. The “possible error surface” is large, logical (as opposed to syntactic), and very tricky to unit test. For example, perhaps you forgot to flip your labels when you left-right flipped the image during data augmentation. Your net can still (shockingly) work pretty well because your network can internally learn to detect flipped images and then it left-right flips its predictions. Or maybe your autoregressive model accidentally takes the thing it’s trying to predict as an input due to an off-by-one bug. Or you tried to clip your gradients but instead clipped the loss, causing the outlier examples to be ignored during training. Or you initialized your weights from a pretrained checkpoint but didn’t use the original mean. Or you just screwed up the settings for regularization strengths, learning rate, its decay rate, model size, etc. Therefore, your misconfigured neural net will throw exceptions only if you’re lucky; Most of the time it will train but silently work a bit worse.&lt;/p&gt;

&lt;p&gt;As a result, (and this is reeaally difficult to over-emphasize) &lt;strong&gt;a “fast and furious” approach to training neural networks does not work&lt;/strong&gt; and only leads to suffering. Now, suffering is a perfectly natural part of getting a neural network to work well, but it can be mitigated by being thorough, defensive, paranoid, and obsessed with visualizations of basically every possible thing. The qualities that in my experience correlate most strongly to success in deep learning are patience and attention to detail.&lt;/p&gt;

&lt;h2 id=&quot;the-recipe&quot;&gt;The recipe&lt;/h2&gt;

&lt;p&gt;In light of the above two facts, I have developed a specific process for myself that I follow when applying a neural net to a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration.&lt;/p&gt;

&lt;h4 id=&quot;1-become-one-with-the-data&quot;&gt;1. Become one with the data&lt;/h4&gt;

&lt;p&gt;The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are the labels?&lt;/p&gt;

&lt;p&gt;In addition, since the neural net is effectively a compressed/compiled version of your dataset, you’ll be able to look at your network (mis)predictions and understand where they might be coming from. And if your network is giving you some prediction that doesn’t seem consistent with what you’ve seen in the data, something is off.&lt;/p&gt;

&lt;p&gt;Once you get a qualitative sense it is also a good idea to write some simple code to search/filter/sort by whatever you can think of (e.g. type of label, size of annotations, number of annotations, etc.) and visualize their distributions and the outliers along any axis. The outliers especially almost always uncover some bugs in data quality or preprocessing.&lt;/p&gt;

&lt;h4 id=&quot;2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines&quot;&gt;2. Set up the end-to-end training/evaluation skeleton + get dumb baselines&lt;/h4&gt;

&lt;p&gt;Now that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it is best to pick some simple model that you couldn’t possibly have screwed up somehow - e.g. a linear classifier, or a very tiny ConvNet. We’ll want to train it, visualize the losses, any other metrics (e.g. accuracy), model predictions, and perform a series of ablation experiments with explicit hypotheses along the way.&lt;/p&gt;

&lt;p&gt;Tips &amp;amp; tricks for this stage:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;fix random seed&lt;/strong&gt;. Always use a fixed random seed to guarantee that when you run the code twice you will get the same outcome. This removes a factor of variation and will help keep you sane.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;simplify&lt;/strong&gt;. Make sure to disable any unnecessary fanciness. As an example, definitely turn off any data augmentation at this stage. Data augmentation is a regularization strategy that we may incorporate later, but for now it is just another opportunity to introduce some dumb bug.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;add significant digits to your eval&lt;/strong&gt;. When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;verify loss @ init&lt;/strong&gt;. Verify that your loss starts at the correct loss value. E.g. if you initialize your final layer correctly you should measure &lt;code&gt;-log(1/n_classes)&lt;/code&gt; on a softmax at initialization. The same default values can be derived for L2 regression, Huber losses, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;init well&lt;/strong&gt;. Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate “hockey stick” loss curves where in the first few iteration your network is basically just learning the bias.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;human baseline&lt;/strong&gt;. Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it. Alternatively, annotate the test data twice and for each example treat one annotation as prediction and the second as ground truth.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;input-indepent baseline&lt;/strong&gt;. Train an input-independent baseline, (e.g. easiest is to just set all your inputs to zero). This should perform worse than when you actually plug in your data without zeroing it out. Does it? i.e. does your model learn to extract any information out of the input at all?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;overfit one batch&lt;/strong&gt;. Overfit a single batch of only a few examples (e.g. as little as two). To do so we increase the capacity of our model (e.g. add layers or filters) and verify that we can reach the lowest achievable loss (e.g. zero). I also like to visualize in the same plot both the label and the prediction and ensure that they end up aligning perfectly once we reach the minimum loss. If they do not, there is a bug somewhere and we cannot continue to the next stage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;verify decreasing training loss&lt;/strong&gt;. At this stage you will hopefully be underfitting on your dataset because you’re working with a toy model. Try to increase its capacity just a bit. Did your training loss go down as it should?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;visualize just before the net&lt;/strong&gt;. The unambiguously correct place to visualize your data is immediately before your &lt;code&gt;y_hat = model(x)&lt;/code&gt; (or &lt;code&gt;sess.run&lt;/code&gt; in tf). That is - you want to visualize &lt;em&gt;exactly&lt;/em&gt; what goes into your network, decoding that raw tensor of data and labels into visualizations. This is the only “source of truth”. I can’t count the number of times this has saved me and revealed problems in data preprocessing and augmentation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;visualize prediction dynamics&lt;/strong&gt;. I like to visualize model predictions on a fixed test batch during the course of training. The “dynamics” of how these predictions move will give you incredibly good intuition for how the training progresses. Many times it is possible to feel the network “struggle” to fit your data if it wiggles too much in some way, revealing instabilities. Very low or very high learning rates are also easily noticeable in the amount of jitter.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;use backprop to chart dependencies&lt;/strong&gt;. Your deep learning code will often contain complicated, vectorized, and broadcasted operations. A relatively common bug I’ve come across a few times is that people get this wrong (e.g. they use &lt;code&gt;view&lt;/code&gt; instead of &lt;code&gt;transpose/permute&lt;/code&gt; somewhere) and inadvertently mix information across the batch dimension. It is a depressing fact that your network will typically still train okay because it will learn to ignore data from the other examples. One way to debug this (and other related problems) is to set the loss to be something trivial like the sum of all outputs of example &lt;strong&gt;i&lt;/strong&gt;, run the backward pass all the way to the input, and ensure that you get a non-zero gradient only on the &lt;strong&gt;i-th&lt;/strong&gt; input. The same strategy can be used to e.g. ensure that your autoregressive model at time t only depends on 1..t-1.  More generally, gradients give you information about what depends on what in your network, which can be useful for debugging.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;generalize a special case&lt;/strong&gt;. This is a bit more of a general coding tip but I’ve often seen people create bugs when they bite off more than they can chew, writing a relatively general functionality from scratch. I like to write a very specific function to what I’m doing right now, get that to work, and then generalize it later making sure that I get the same result. Often this applies to vectorizing code, where I almost always write out the fully loopy version first and only then transform it to vectorized code one loop at a time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3-overfit&quot;&gt;3. Overfit&lt;/h4&gt;

&lt;p&gt;At this stage we should have a good understanding of the dataset and we have the full training + evaluation pipeline working. For any given model we can (reproducibly) compute a metric that we trust. We are also armed with our performance for an input-independent baseline, the performance of a few dumb baselines (we better beat these), and we have a rough sense of the performance of a human (we hope to reach this). The stage is now set for iterating on a good model.&lt;/p&gt;

&lt;p&gt;The approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration.&lt;/p&gt;

&lt;p&gt;A few tips &amp;amp; tricks for this stage:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;picking the model&lt;/strong&gt;. To reach a good training loss you’ll want to choose an appropriate architecture for the data. When it comes to choosing this my #1 advice is: &lt;strong&gt;Don’t be a hero&lt;/strong&gt;. I’ve seen a lot of people who are eager to get crazy and creative in stacking up the lego blocks of the neural net toolbox in various exotic architectures that make sense to them. Resist this temptation strongly in the early stages of your project. I always advise people to simply find the most related paper and copy paste their simplest architecture that achieves good performance. E.g. if you are classifying images don’t be a hero and just copy paste a ResNet-50 for your first run. You’re allowed to do something more custom later and beat this.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;adam is safe&lt;/strong&gt;. In the early stages of setting baselines I like to use Adam with a learning rate of &lt;a href=&quot;https://twitter.com/karpathy/status/801621764144971776?lang=en&quot;&gt;3e-4&lt;/a&gt;. In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate. For ConvNets a well-tuned SGD will almost always slightly outperform Adam, but the optimal learning rate region is much more narrow and problem-specific. (Note: If you are using RNNs and related sequence models it is more common to use Adam. At the initial stage of your project, again, don’t be a hero and follow whatever the most related papers do.)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;complexify only one at a time&lt;/strong&gt;. If you have multiple signals to plug into your classifier I would advise that you plug them in one by one and every time ensure that you get a performance boost you’d expect. Don’t throw the kitchen sink at your model at the start. There are other ways of building up complexity - e.g. you can try to plug in smaller images first and make them bigger later, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;do not trust learning rate decay defaults&lt;/strong&gt;. If you are re-purposing code from some other domain always be very careful with learning rate decay. Not only would you want to use different decay schedules for different problems, but - even worse - in a typical implementation the schedule will be based current epoch number, which can vary widely simply depending on the size of your dataset. E.g. ImageNet would decay by 10 on epoch 30. If you’re not training ImageNet then you almost certainly do not want this. If you’re not careful your code could secretely be driving your learning rate to zero too early, not allowing your model to converge. In my own work I always disable learning rate decays entirely (I use a constant LR) and tune this all the way at the very end.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4-regularize&quot;&gt;4. Regularize&lt;/h4&gt;

&lt;p&gt;Ideally, we are now at a place where we have a large model that is fitting at least the training set. Now it is time to regularize it and gain some validation accuracy by giving up some of the training accuracy. Some tips &amp;amp; tricks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;get more data&lt;/strong&gt;. First, the by far best and preferred way to regularize a model in any practical setting is to add more real training data. It is a very common mistake to spend a lot engineering cycles trying to squeeze juice out of a small dataset when you could instead be collecting more data. As far as I’m aware adding more data is pretty much the only guaranteed way to monotonically improve the performance of a well-configured neural network almost indefinitely. The other would be ensembles (if you can afford them), but that tops out after ~5 models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;data augment&lt;/strong&gt;. The next best thing to real data is half-fake data - try out more aggressive data augmentation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;creative augmentation&lt;/strong&gt;. If half-fake data doesn’t do it, fake data may also do something. People are finding creative ways of expanding datasets; For example, &lt;a href=&quot;https://openai.com/blog/learning-dexterity/&quot;&gt;domain randomization&lt;/a&gt;, use of &lt;a href=&quot;http://vladlen.info/publications/playing-data-ground-truth-computer-games/&quot;&gt;simulation&lt;/a&gt;, clever &lt;a href=&quot;https://arxiv.org/abs/1708.01642&quot;&gt;hybrids&lt;/a&gt; such as inserting (potentially simulated) data into scenes, or even GANs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;pretrain&lt;/strong&gt;. It rarely ever hurts to use a pretrained network if you can, even if you have enough data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;stick with supervised learning&lt;/strong&gt;. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;smaller input dimensionality&lt;/strong&gt;. Remove features that may contain spurious signal. Any added spurious input is just another opportunity to overfit if your dataset is small. Similarly, if low-level details don’t matter much try to input a smaller image.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;smaller model size&lt;/strong&gt;. In many cases you can use domain knowledge constraints on the network to decrease its size. As an example, it used to be trendy to use Fully Connected layers at the top of backbones for ImageNet but these have since been replaced with simple average pooling, eliminating a ton of parameters in the process.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;decrease the batch size&lt;/strong&gt;. Due to the normalization inside batch norm smaller batch sizes somewhat correspond to stronger regularization. This is because the batch empirical mean/std are more approximate versions of the full mean/std so the scale &amp;amp; offset “wiggles” your batch around more.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;drop&lt;/strong&gt;. Add dropout. Use dropout2d (spatial dropout) for ConvNets. Use this sparingly/carefully because dropout &lt;a href=&quot;https://arxiv.org/abs/1801.05134&quot;&gt;does not seem to play nice&lt;/a&gt; with batch normalization.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;weight decay&lt;/strong&gt;. Increase the weight decay penalty.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;early stopping&lt;/strong&gt;. Stop training based on your measured validation loss to catch your model just as it’s about to overfit.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;try a larger model&lt;/strong&gt;. I mention this last and only after early stopping but I’ve found a few times in the past that larger models will of course overfit much more eventually, but their “early stopped” performance can often be much better than that of smaller models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, to gain additional confidence that your network is a reasonable classifier, I like to visualize the network’s first-layer weights and ensure you get nice edges that make sense. If your first layer filters look like noise then something could be off. Similarly, activations inside the net can sometimes display odd artifacts and hint at problems.&lt;/p&gt;

&lt;h4 id=&quot;5-tune&quot;&gt;5. Tune&lt;/h4&gt;

&lt;p&gt;You should now be “in the loop” with your dataset exploring a wide model space for architectures that achieve low validation loss. A few tips and tricks for this step:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;random over grid search&lt;/strong&gt;. For simultaneously tuning multiple hyperparameters it may sound tempting to use grid search to ensure coverage of all settings, but keep in mind that it is &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf&quot;&gt;best to use random search instead&lt;/a&gt;. Intuitively, this is because neural nets are often much more sensitive to some parameters than others. In the limit, if a parameter &lt;strong&gt;a&lt;/strong&gt; matters but changing &lt;strong&gt;b&lt;/strong&gt; has no effect then you’d rather sample &lt;strong&gt;a&lt;/strong&gt; more throughly than at a few fixed points multiple times.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;hyper-parameter optimization&lt;/strong&gt;. There is a large number of fancy bayesian hyper-parameter optimization toolboxes around and a few of my friends have also reported success with them, but my personal experience is that the state of the art approach to exploring a nice and wide space of models and hyperparameters is to use an intern :). Just kidding.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;6-squeeze-out-the-juice&quot;&gt;6. Squeeze out the juice&lt;/h4&gt;

&lt;p&gt;Once you find the best types of architectures and hyper-parameters you can still use a few more tricks to squeeze out the last pieces of juice out of the system:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ensembles&lt;/strong&gt;. Model ensembles are a pretty much guaranteed way to gain 2% of accuracy on anything. If you can’t afford the computation at test time look into distilling your ensemble into a network using &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;&gt;dark knowledge&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;leave it training&lt;/strong&gt;. I’ve often seen people tempted to stop the model training when the validation loss seems to be leveling off. In my experience networks keep training for unintuitively long time. One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Once you make it here you’ll have all the ingredients for success: You have a deep understanding of the technology, the dataset and the problem, you’ve set up the entire training/evaluation infrastructure and achieved high confidence in its accuracy, and you’ve explored increasingly more complex models, gaining performance improvements in ways you’ve predicted each step of the way. You’re now ready to read a lot of papers, try a large number of experiments, and get your SOTA results. Good luck!&lt;/p&gt;

&lt;p&gt;Source : &lt;a href=&quot;http://karpathy.github.io/2019/04/25/recipe/&quot;&gt;Andrej Karpathy&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrej Karpathy</name></author><category term="deep-learning" /><summary type="html">A collection of practical advice for the process of achieving strong results with neural networks.</summary></entry><entry><title type="html">Attention? Attention!</title><link href="http://localhost:4000/blog/2018/06/24/attention-attention.html" rel="alternate" type="text/html" title="Attention? Attention!" /><published>2018-06-24T18:07:00+07:00</published><updated>2018-06-24T18:07:00+07:00</updated><id>http://localhost:4000/blog/2018/06/24/attention-attention</id><content type="html" xml:base="http://localhost:4000/blog/2018/06/24/attention-attention.html">&lt;p&gt;Source : &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&quot;&gt;Lilian&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#whats-wrong-with-seq2seq-model&quot; id=&quot;markdown-toc-whats-wrong-with-seq2seq-model&quot;&gt;What’s Wrong with Seq2Seq Model?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#born-for-translation&quot; id=&quot;markdown-toc-born-for-translation&quot;&gt;Born for Translation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#definition&quot; id=&quot;markdown-toc-definition&quot;&gt;Definition&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#a-family-of-attention-mechanisms&quot; id=&quot;markdown-toc-a-family-of-attention-mechanisms&quot;&gt;A Family of Attention Mechanisms&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#self-attention&quot; id=&quot;markdown-toc-self-attention&quot;&gt;Self-Attention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#soft-vs-hard-attention&quot; id=&quot;markdown-toc-soft-vs-hard-attention&quot;&gt;Soft vs Hard Attention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#global-vs-local-attention&quot; id=&quot;markdown-toc-global-vs-local-attention&quot;&gt;Global vs Local Attention&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#neural-turing-machines&quot; id=&quot;markdown-toc-neural-turing-machines&quot;&gt;Neural Turing Machines&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#reading-and-writing&quot; id=&quot;markdown-toc-reading-and-writing&quot;&gt;Reading and Writing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#attention-mechanisms&quot; id=&quot;markdown-toc-attention-mechanisms&quot;&gt;Attention Mechanisms&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pointer-network&quot; id=&quot;markdown-toc-pointer-network&quot;&gt;Pointer Network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#transformer&quot; id=&quot;markdown-toc-transformer&quot;&gt;Transformer&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#key-value-and-query&quot; id=&quot;markdown-toc-key-value-and-query&quot;&gt;Key, Value and Query&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#multi-head-self-attention&quot; id=&quot;markdown-toc-multi-head-self-attention&quot;&gt;Multi-Head Self-Attention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#encoder&quot; id=&quot;markdown-toc-encoder&quot;&gt;Encoder&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#decoder&quot; id=&quot;markdown-toc-decoder&quot;&gt;Decoder&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#full-architecture&quot; id=&quot;markdown-toc-full-architecture&quot;&gt;Full Architecture&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#snail&quot; id=&quot;markdown-toc-snail&quot;&gt;SNAIL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#self-attention-gan&quot; id=&quot;markdown-toc-self-attention-gan&quot;&gt;Self-Attention GAN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot; id=&quot;markdown-toc-references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Attention has been a fairly popular concept and a useful tool in the deep learning community in recent years. In this post, we are gonna look into how attention was invented, and various attention mechanisms and models, such as transformer and SNAIL.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Attention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence. Take the picture of a Shiba Inu in Fig. 1 as an example.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/shiba-example-attention.png&quot; alt=&quot;shiba&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 1. A Shiba Inu in a men’s outfit. The credit of the original photo goes to Instagram &lt;a href=&quot;https://www.instagram.com/mensweardog/?hl=en&quot;&gt;@mensweardog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Human visual attention allows us to focus on a certain region with “high resolution” (i.e. look at the pointy ear in the yellow box) while perceiving the surrounding image in “low resolution” (i.e. now how about the snowy background and the outfit?), and then adjust the focal point or do the inference accordingly. Given a small patch of an image, pixels in the rest provide clues what should be displayed there. We expect to see a pointy ear in the yellow box because we have seen a dog’s nose, another pointy ear on the right, and Shiba’s mystery eyes (stuff in the red boxes). However, the sweater and blanket at the bottom would not be as helpful as those doggy features.&lt;/p&gt;

&lt;p&gt;Similarly, we can explain the relationship between words in one sentence or close context. When we see “eating”, we expect to encounter a food word very soon. The color term describes the food, but probably not so much with “eating” directly.&lt;/p&gt;

&lt;p style=&quot;width: 65%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/sentence-example-attention.png&quot; alt=&quot;sentence&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 2. One word “attends” to other words in the same sentence differently.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In a nutshell, attention in deep learning can be broadly interpreted as a vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with (or “&lt;em&gt;attends to&lt;/em&gt;” as you may have read in many papers) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.&lt;/p&gt;

&lt;h2 id=&quot;whats-wrong-with-seq2seq-model&quot;&gt;What’s Wrong with Seq2Seq Model?&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;seq2seq&lt;/strong&gt; model was born in the field of language modeling (&lt;a href=&quot;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;Sutskever, et al. 2014&lt;/a&gt;). Broadly speaking, it aims to transform an input sequence (source) to a new one (target) and both sequences can be of arbitrary lengths. Examples of transformation tasks include machine translation between multiple languages in either text or audio, question-answer dialog generation, or even parsing sentences into grammar trees.&lt;/p&gt;

&lt;p&gt;The seq2seq model normally has an encoder-decoder architecture, composed of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;An &lt;strong&gt;encoder&lt;/strong&gt; processes the input sequence and compresses the information into a context vector (also known as sentence embedding or “thought” vector) of a &lt;em&gt;fixed length&lt;/em&gt;. This representation is expected to be a good summary of the meaning of the &lt;em&gt;whole&lt;/em&gt; source sequence.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;decoder&lt;/strong&gt; is initialized with the context vector to emit the transformed output. The early work only used the last state of the encoder network as the decoder initial state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both the encoder and decoder are recurrent neural networks, i.e. using &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;LSTM or GRU&lt;/a&gt; units.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/encoder-decoder-example.png&quot; alt=&quot;encoder-decoder model with additive attention layer&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 3. The encoder-decoder model, translating the sentence “she is eating a green apple” to Chinese. The visualization of both encoder and decoder is unrolled in time.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A critical and apparent disadvantage of this fixed-length context vector design is incapability of remembering long sentences. Often it has forgotten the first part once it completes processing the whole input. The attention mechanism was born (&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;) to resolve this problem.&lt;/p&gt;

&lt;h2 id=&quot;born-for-translation&quot;&gt;Born for Translation&lt;/h2&gt;

&lt;p&gt;The attention mechanism was born to help memorize long source sentences in neural machine translation (&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;NMT&lt;/a&gt;). Rather than building a single context vector out of the encoder’s last hidden state, the secret sauce invented by attention is to create shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element.&lt;/p&gt;

&lt;p&gt;While the context vector has access to the entire input sequence, we don’t need to worry about forgetting. The alignment between the source and target is learned and controlled by the context vector. Essentially the context vector consumes three pieces of information:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;encoder hidden states;&lt;/li&gt;
  &lt;li&gt;decoder hidden states;&lt;/li&gt;
  &lt;li&gt;alignment between source and target.&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/encoder-decoder-attention.png&quot; alt=&quot;encoder-decoder model with additive attention layer&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 4. The encoder-decoder model with additive attention mechanism in &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;Now let’s define the attention mechanism introduced in NMT in a scientific way. Say, we have a source sequence \(\mathbf{x}\) of length \(n\) and try to output a target sequence \(\mathbf{y}\) of length \(m\):&lt;/p&gt;

\[\begin{aligned}
\mathbf{x} &amp;amp;= [x_1, x_2, \dots, x_n] \\
\mathbf{y} &amp;amp;= [y_1, y_2, \dots, y_m]
\end{aligned}\]

&lt;p&gt;(Variables in bold indicate that they are vectors; same for everything else in this post.)&lt;/p&gt;

&lt;p&gt;The encoder is a &lt;a href=&quot;https://www.coursera.org/lecture/nlp-sequence-models/bidirectional-rnn-fyXnn&quot;&gt;bidirectional RNN&lt;/a&gt; (or other recurrent network setting of your choice) with a forward hidden state \(\overrightarrow{\boldsymbol{h}}_i\) and a backward one \(\overleftarrow{\boldsymbol{h}}_i\). A simple concatenation of two represents the encoder state. The motivation is to include both the preceding and following words in the annotation of one word.&lt;/p&gt;

\[\boldsymbol{h}_i = [\overrightarrow{\boldsymbol{h}}_i^\top; \overleftarrow{\boldsymbol{h}}_i^\top]^\top, i=1,\dots,n\]

&lt;p&gt;The decoder network has hidden state \(\boldsymbol{s}_t=f(\boldsymbol{s}_{t-1}, y_{t-1}, \mathbf{c}_t)\) for the output word at position t, \(t=1,\dots,m\), where the context vector \(\mathbf{c}_t\) is a sum of hidden states of the input sequence, weighted by alignment scores:&lt;/p&gt;

\[\begin{aligned}
\mathbf{c}_t &amp;amp;= \sum_{i=1}^n \alpha_{t,i} \boldsymbol{h}_i &amp;amp; \small{\text{; Context vector for output }y_t}\\
\alpha_{t,i} &amp;amp;= \text{align}(y_t, x_i) &amp;amp; \small{\text{; How well two words }y_t\text{ and }x_i\text{ are aligned.}}\\
&amp;amp;= \frac{\exp(\text{score}(\boldsymbol{s}_{t-1}, \boldsymbol{h}_i))}{\sum_{i'=1}^n \exp(\text{score}(\boldsymbol{s}_{t-1}, \boldsymbol{h}_{i'}))} &amp;amp; \small{\text{; Softmax of some predefined alignment score.}}.
\end{aligned}\]

&lt;p&gt;The alignment model assigns a score \(\alpha_{t,i}\) to the pair of input at position i and output at position t, \((y_t, x_i)\), based on how well they match. The set of \(\{\alpha_{t, i}\}\) are weights defining how much of each source hidden state should be considered for each output. In Bahdanau’s paper, the alignment score \(\alpha\) is parametrized by a &lt;strong&gt;feed-forward network&lt;/strong&gt; with a single hidden layer and this network is jointly trained with other parts of the model. The score function is therefore in the following form, given that tanh is used as the non-linear activation function:&lt;/p&gt;

\[\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \mathbf{v}_a^\top \tanh(\mathbf{W}_a[\boldsymbol{s}_t; \boldsymbol{h}_i])\]

&lt;p&gt;where both \(\mathbf{v}_a\) and \(\mathbf{W}_a\) are weight matrices to be learned in the alignment model.&lt;/p&gt;

&lt;p&gt;The matrix of alignment scores is a nice byproduct to explicitly show the correlation between source and target words.&lt;/p&gt;

&lt;p style=&quot;width: 65%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/bahdanau-fig3.png&quot; alt=&quot;alignment matrix&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 5. Alignment matrix of “L’accord sur l’Espace économique européen a été signé en août 1992” (French) and its English translation “The agreement on the European Economic Area was signed in August 1992”. (Image source: Fig 3 in &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Check out this nice &lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/seq2seq&quot;&gt;tutorial&lt;/a&gt; by Tensorflow team for more implementation instructions.&lt;/p&gt;

&lt;h2 id=&quot;a-family-of-attention-mechanisms&quot;&gt;A Family of Attention Mechanisms&lt;/h2&gt;

&lt;p&gt;With the help of the attention, the dependencies between source and target sequences are not restricted by the in-between distance anymore! Given the big improvement by attention in machine translation, it soon got extended into the computer vision field (&lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;Xu et al. 2015&lt;/a&gt;) and people started exploring various other forms of attention mechanisms (&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong, et al., 2015&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/1703.03906&quot;&gt;Britz et al., 2017&lt;/a&gt;; &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Below is a summary table of several popular attention mechanisms and corresponding alignment score functions:&lt;/p&gt;

&lt;table class=&quot;info&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Alignment score function&lt;/th&gt;
      &lt;th&gt;Citation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Content-base attention&lt;/td&gt;
      &lt;td&gt;$$\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \text{cosine}[\boldsymbol{s}_t, \boldsymbol{h}_i]$$&lt;/td&gt;
      &lt;td&gt;[Graves2014](https://arxiv.org/abs/1410.5401)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Additive(*)&lt;/td&gt;
      &lt;td&gt;$$\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \mathbf{v}_a^\top \tanh(\mathbf{W}_a[\boldsymbol{s}_t; \boldsymbol{h}_i])$$&lt;/td&gt;
      &lt;td&gt;[Bahdanau2015](https://arxiv.org/pdf/1409.0473.pdf)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Location-Base&lt;/td&gt;
      &lt;td&gt;$$\alpha_{t,i} = \text{softmax}(\mathbf{W}_a \boldsymbol{s}_t)$$&lt;br /&gt;Note: This simplifies the softmax alignment to only depend on the target position.&lt;/td&gt;
      &lt;td&gt;[Luong2015](https://arxiv.org/pdf/1508.04025.pdf)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;General&lt;/td&gt;
      &lt;td&gt;$$\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \boldsymbol{s}_t^\top\mathbf{W}_a\boldsymbol{h}_i$$&lt;br /&gt;where $$\mathbf{W}_a$$ is a trainable weight matrix in the attention layer.&lt;/td&gt;
      &lt;td&gt;[Luong2015](https://arxiv.org/pdf/1508.04025.pdf)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dot-Product&lt;/td&gt;
      &lt;td&gt;$$\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \boldsymbol{s}_t^\top\boldsymbol{h}_i$$&lt;/td&gt;
      &lt;td&gt;[Luong2015](https://arxiv.org/pdf/1508.4025.pdf)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Scaled Dot-Product(^)&lt;/td&gt;
      &lt;td&gt;$$\text{score}(\boldsymbol{s}_t, \boldsymbol{h}_i) = \frac{\boldsymbol{s}_t^\top\boldsymbol{h}_i}{\sqrt{n}}$$&lt;br /&gt;Note: very similar to the dot-product attention except for a scaling factor; where n is the dimension of the source hidden state.&lt;/td&gt;
      &lt;td&gt;[Vaswani2017](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;(*) Referred to as “concat” in Luong, et al., 2015 and as “additive attention” in Vaswani, et al., 2017.
(^) It adds a scaling factor \(1/\sqrt{n}\), motivated by the concern when the input is large, the softmax function may have an extremely small gradient, hard for efficient learning.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Here are a summary of broader categories of attention mechanisms:&lt;/p&gt;

&lt;table class=&quot;info&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Definition&lt;/th&gt;
      &lt;th&gt;Citation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Self-Attention(&amp;amp;)&lt;/td&gt;
      &lt;td&gt;Relating different positions of the same input sequence. Theoretically the self-attention can adopt any score functions above, but just replace the target sequence with the same input sequence.&lt;/td&gt;
      &lt;td&gt;[Cheng2016](https://arxiv.org/pdf/1601.06733.pdf)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Global/Soft&lt;/td&gt;
      &lt;td&gt;Attending to the entire input state space.&lt;/td&gt;
      &lt;td&gt;[Xu2015](http://proceedings.mlr.press/v37/xuc15.pdf)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Local/Hard&lt;/td&gt;
      &lt;td&gt;Attending to the part of input state space; i.e. a patch of the input image.&lt;/td&gt;
      &lt;td&gt;[Xu2015](http://proceedings.mlr.press/v37/xuc15.pdf); [Luong2015](https://arxiv.org/pdf/1508.04025.pdf)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;(&amp;amp;) Also, referred to as “intra-attention” in Cheng et al., 2016 and some other papers.&lt;/p&gt;

&lt;h3 id=&quot;self-attention&quot;&gt;Self-Attention&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Self-attention&lt;/strong&gt;, also known as &lt;strong&gt;intra-attention&lt;/strong&gt;, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;long short-term memory network&lt;/a&gt; paper used self-attention to do machine reading. In the example below, the self-attention mechanism enables us to learn the correlation between the current words and the previous part of the sentence.&lt;/p&gt;

&lt;p style=&quot;width: 70%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/cheng2016-fig1.png&quot; alt=&quot;intra-attention&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 6. The current word is in red and the size of the blue shade indicates the activation level. (Image source: &lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;Cheng et al., 2016&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;soft-vs-hard-attention&quot;&gt;Soft vs Hard Attention&lt;/h3&gt;

&lt;p&gt;In the &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;show, attend and tell&lt;/a&gt; paper, attention mechanism is applied to images to generate captions. The image is first encoded by a CNN to extract features. Then a LSTM decoder consumes the convolution features to produce descriptive words one by one, where the weights are learned through attention. The visualization of the attention weights clearly demonstrates which regions of the image the model is paying attention to so as to output a certain word.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/xu2015-fig6b.png&quot; alt=&quot;show-attend-and-tell&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 7. “A woman is throwing a frisbee in a park.” (Image source: Fig. 6(b) in &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;Xu et al. 2015&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This paper first proposed the distinction between “soft” vs “hard” attention, based on whether the attention has access to the entire image or only a patch:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Soft&lt;/strong&gt; Attention: the alignment weights are learned and placed “softly” over all patches in the source image; essentially the same type of attention as in &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;.
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Pro&lt;/em&gt;: the model is smooth and differentiable.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Con&lt;/em&gt;: expensive when the source input is large.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hard&lt;/strong&gt; Attention: only selects one patch of the image to attend to at a time.
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Pro&lt;/em&gt;: less calculation at the inference time.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Con&lt;/em&gt;: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (&lt;a href=&quot;https://arxiv.org/abs/1508.04025&quot;&gt;Luong, et al., 2015&lt;/a&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;global-vs-local-attention&quot;&gt;Global vs Local Attention&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong, et al., 2015&lt;/a&gt; proposed the “global” and “local” attention. The global attention is similar to the soft attention, while the local one is an interesting blend between &lt;a href=&quot;#soft-vs-hard-attention&quot;&gt;hard and soft&lt;/a&gt;, an improvement over the hard attention to make it differentiable: the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/luong2015-fig2-3.png&quot; alt=&quot;global-local-attention&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 8. Global vs local attention (Image source: Fig 2 &amp;amp; 3 in &lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong, et al., 2015&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;neural-turing-machines&quot;&gt;Neural Turing Machines&lt;/h2&gt;

&lt;p&gt;Alan Turing in &lt;a href=&quot;https://en.wikipedia.org/wiki/Turing_machine&quot;&gt;1936&lt;/a&gt; proposed a minimalistic model of computation. It is composed of a infinitely long tape and a head to interact with the tape. The tape has countless cells on it, each filled with a symbol: 0, 1 or blank (“ “). The operation head can read symbols, edit symbols and move left/right on the tape. Theoretically a Turing machine can simulate any computer algorithm, irrespective of how complex or expensive the procedure might be. The infinite memory gives a Turing machine an edge to be mathematically limitless. However, infinite memory is not feasible in real modern computers and then we only consider Turing machine as a mathematical model of computation.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/turing-machine.jpg&quot; alt=&quot;turing-machine&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 9. How a Turing machine looks like: a tape + a head that handles the tape. (Image source: http://aturingmachine.com/)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Neural Turing Machine&lt;/strong&gt; (&lt;strong&gt;NTM&lt;/strong&gt;, &lt;a href=&quot;https://arxiv.org/abs/1410.5401&quot;&gt;Graves, Wayne &amp;amp; Danihelka, 2014&lt;/a&gt;) is a model architecture for coupling a neural network with external memory storage. The memory mimics the Turing machine tape and the neural network controls the operation heads to read from or write to the tape. However, the memory in NTM is finite, and thus it probably looks more like a “Neural &lt;a href=&quot;https://en.wikipedia.org/wiki/Von_Neumann_architecture&quot;&gt;von Neumann&lt;/a&gt; Machine”.&lt;/p&gt;

&lt;p&gt;NTM contains two major components, a &lt;em&gt;controller&lt;/em&gt; neural network and a &lt;em&gt;memory&lt;/em&gt; bank. 
Controller: is in charge of executing operations on the memory. It can be any type of neural network, feed-forward or recurrent.
Memory: stores processed information. It is a matrix of size \(N \times M\), containing N vector rows and each has \(M\) dimensions.&lt;/p&gt;

&lt;p&gt;In one update iteration, the controller processes the input and interacts with the memory bank accordingly to generate output. The interaction is handled by a set of parallel &lt;em&gt;read&lt;/em&gt; and &lt;em&gt;write&lt;/em&gt; heads. Both read and write operations are “blurry” by softly attending to all the memory addresses.&lt;/p&gt;

&lt;p style=&quot;width: 60%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/NTM.png&quot; alt=&quot;turing-machine&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig 10. Neural Turing Machine Architecture.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;reading-and-writing&quot;&gt;Reading and Writing&lt;/h3&gt;

&lt;p&gt;When reading from the memory at time t, an attention vector of size \(N\), \(\mathbf{w}_t\) controls how much attention to assign to different memory locations (matrix rows). The read vector \(\mathbf{r}_t\) is a sum weighted by attention intensity:&lt;/p&gt;

\[\mathbf{r}_t = \sum_{i=1}^N w_t(i)\mathbf{M}_t(i)\text{, where }\sum_{i=1}^N w_t(i)=1, \forall i: 0 \leq w_t(i) \leq 1\]

&lt;p&gt;where \(w_t(i)\) is the \(i\)-th element in \(\mathbf{w}_t\) and \(\mathbf{M}_t(i)\) is the \(i\)-th row vector in the memory.&lt;/p&gt;

&lt;p&gt;When writing into the memory at time t, as inspired by the input and forget gates in LSTM, a write head first wipes off some old content according to an erase vector \(\mathbf{e}_t\) and then adds new information by an add vector \(\mathbf{a}_t\).&lt;/p&gt;

\[\begin{aligned}
\tilde{\mathbf{M}}_t(i) &amp;amp;= \mathbf{M}_{t-1}(i) [\mathbf{1} - w_t(i)\mathbf{e}_t] &amp;amp;\scriptstyle{\text{; erase}}\\
\mathbf{M}_t(i) &amp;amp;= \tilde{\mathbf{M}}_t(i) + w_t(i) \mathbf{a}_t &amp;amp;\scriptstyle{\text{; add}}
\end{aligned}\]

&lt;h3 id=&quot;attention-mechanisms&quot;&gt;Attention Mechanisms&lt;/h3&gt;

&lt;p&gt;In Neural Turing Machine, how to generate the attention distribution \(\mathbf{w}_t\) depends on the addressing mechanisms: NTM uses a mixture of content-based and location-based addressings.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Content-based addressing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The content-addressing creates attention vectors based on the similarity between the key vector \(\mathbf{k}_t\) extracted by the controller from the input and memory rows. The content-based attention scores are computed as cosine similarity and then normalized by softmax. In addition, NTM adds a strength multiplier \(\beta_t\) to amplify or attenuate the focus of the distribution.&lt;/p&gt;

\[w_t^c(i) 
= \text{softmax}(\beta_t \cdot \text{cosine}[\mathbf{k}_t, \mathbf{M}_t(i)])
= \frac{\exp(\beta_t \frac{\mathbf{k}_t \cdot \mathbf{M}_t(i)}{\|\mathbf{k}_t\| \cdot \|\mathbf{M}_t(i)\|})}{\sum_{j=1}^N \exp(\beta_t \frac{\mathbf{k}_t \cdot \mathbf{M}_t(j)}{\|\mathbf{k}_t\| \cdot \|\mathbf{M}_t(j)\|})}\]

&lt;p&gt;&lt;strong&gt;Interpolation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Then an interpolation gate scalar \(g_t\) is used to blend the newly generated content-based attention vector with the attention weights in the last time step:&lt;/p&gt;

\[\mathbf{w}_t^g = g_t \mathbf{w}_t^c + (1 - g_t) \mathbf{w}_{t-1}\]

&lt;p&gt;&lt;strong&gt;Location-based addressing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The location-based addressing sums up the values at different positions in the attention vector, weighted by a weighting distribution over allowable integer shifts. It is equivalent to a 1-d convolution with a kernel \(\mathbf{s}_t(.)\), a function of the position offset. There are multiple ways to define this distribution. See Fig. 11. for inspiration.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/shift-weighting.png&quot; alt=&quot;shift-weighting&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 11. Two ways to represent the shift weighting distribution \(\mathbf{s}_t\).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Finally the attention distribution is enhanced by a sharpening scalar \(\gamma_t \geq 1\).&lt;/p&gt;

\[\begin{aligned}
\tilde{w}_t(i) &amp;amp;= \sum_{j=1}^N w_t^g(j) s_t(i-j) &amp;amp; \scriptstyle{\text{; circular convolution}}\\
w_t(i) &amp;amp;= \frac{\tilde{w}_t(i)^{\gamma_t}}{\sum_{j=1}^N \tilde{w}_t(j)^{\gamma_t}} &amp;amp; \scriptstyle{\text{; sharpen}}
\end{aligned}\]

&lt;p&gt;The complete process of generating the attention vector \(\mathbf{w}_t\) at time step t is illustrated in Fig. 12. All the parameters produced by the controller are unique for each head. If there are multiple read and write heads in parallel, the controller would output multiple sets.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/NTM-flow-addressing.png&quot; alt=&quot;NTM-flow-addressing&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 12. Flow diagram of the addressing mechanisms in Neural Turing Machine. (Image source: &lt;a href=&quot;https://arxiv.org/abs/1410.5401&quot;&gt;Graves, Wayne &amp;amp; Danihelka, 2014&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;pointer-network&quot;&gt;Pointer Network&lt;/h2&gt;

&lt;p&gt;In problems like sorting or travelling salesman, both input and output are sequential data. Unfortunately, they cannot be easily solved by classic seq-2-seq or NMT models, given that the discrete categories of output elements are not determined in advance, but depends on the variable input size. The &lt;strong&gt;Pointer Net&lt;/strong&gt; (&lt;strong&gt;Ptr-Net&lt;/strong&gt;; &lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;Vinyals, et al. 2015&lt;/a&gt;) is proposed to resolve this type of problems: When the output elements correspond to &lt;em&gt;positions&lt;/em&gt; in an input sequence. Rather than using attention to blend hidden units of an encoder into a context vector (See Fig. 8), the Pointer Net applies attention over the input elements to pick one as the output at each decoder step.&lt;/p&gt;

&lt;p style=&quot;width: 75%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/ptr-net.png&quot; alt=&quot;pointer network&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 13. The architecture of a Pointer Network model. (Image source: &lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;Vinyals, et al. 2015&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The Ptr-Net outputs a sequence of integer indices, \(\boldsymbol{c} = (c_1, \dots, c_m)\) given a sequence of input vectors \(\boldsymbol{x} = (x_1, \dots, x_n)\) and \(1 \leq c_i \leq n\). The model still embraces an encoder-decoder framework. The encoder and decoder hidden states are denoted as \((\boldsymbol{h}_1, \dots, \boldsymbol{h}_n)\) and \((\boldsymbol{s}_1, \dots, \boldsymbol{s}_m)\), respectively. Note that \(\mathbf{s}_i\) is the output gate after cell activation in the decoder. The Ptr-Net applies additive attention between states and then normalizes it by softmax to model the output conditional probability:&lt;/p&gt;

\[\begin{aligned}
y_i &amp;amp;= p(c_i \vert c_1, \dots, c_{i-1}, \boldsymbol{x}) \\
    &amp;amp;= \text{softmax}(\text{score}(\boldsymbol{s}_t; \boldsymbol{h}_i)) = \text{softmax}(\mathbf{v}_a^\top \tanh(\mathbf{W}_a[\boldsymbol{s}_t; \boldsymbol{h}_i]))
\end{aligned}\]

&lt;p&gt;The attention mechanism is simplified, as Ptr-Net does not blend the encoder states into the output with attention weights. In this way, the output only responds to the positions but not the input content.&lt;/p&gt;

&lt;h2 id=&quot;transformer&quot;&gt;Transformer&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;“Attention is All you Need”&lt;/a&gt;
 (Vaswani, et al., 2017), without a doubt, is one of the most impactful and interesting paper in 2017. It presented a lot of improvements to the soft attention and make it possible to do seq2seq modeling &lt;em&gt;without&lt;/em&gt; recurrent network units. The proposed “&lt;strong&gt;transformer&lt;/strong&gt;” model is entirely built on the self-attention mechanisms without using sequence-aligned recurrent architecture.&lt;/p&gt;

&lt;p&gt;The secret recipe is carried in its model architecture.&lt;/p&gt;

&lt;h3 id=&quot;key-value-and-query&quot;&gt;Key, Value and Query&lt;/h3&gt;

&lt;p&gt;The major component in the transformer is the unit of &lt;em&gt;multi-head self-attention mechanism&lt;/em&gt;. The transformer views the encoded representation of the input as a set of &lt;strong&gt;key&lt;/strong&gt;-&lt;strong&gt;value&lt;/strong&gt; pairs, \((\mathbf{K}, \mathbf{V})\), both of dimension \(n\) (input sequence length); in the context of NMT, both the keys and values are the encoder hidden states. In the decoder, the previous output is compressed into a &lt;strong&gt;query&lt;/strong&gt; (\(\mathbf{Q}\) of dimension \(m\)) and the next output is produced by mapping this query and the set of keys and values.&lt;/p&gt;

&lt;p&gt;The transformer adopts the &lt;a href=&quot;#summary&quot;&gt;scaled dot-product attention&lt;/a&gt;: the output is a weighted sum of the values, where the weight assigned to each value is determined by the dot-product of the query with all the keys:&lt;/p&gt;

\[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{n}})\mathbf{V}\]

&lt;h3 id=&quot;multi-head-self-attention&quot;&gt;Multi-Head Self-Attention&lt;/h3&gt;

&lt;p style=&quot;width: 40%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/multi-head-attention.png&quot; alt=&quot;multi-head scaled dot-product attention&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em style=&quot;text-align: center;&quot;&gt;Fig. 14. Multi-head scaled dot-product attention mechanism. (Image source: Fig 2 in &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Rather than only computing the attention once, the multi-head mechanism runs through the scaled dot-product attention multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions. I assume the motivation is because ensembling always helps? ;) According to the paper, &lt;em style=&quot;text-align: center;&quot;&gt;“multi-head attention allows the model to jointly attend to information from different representation &lt;strong&gt;subspaces&lt;/strong&gt; at different positions. With a single attention head, averaging inhibits this.”&lt;/em&gt;&lt;/p&gt;

\[\begin{aligned}
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &amp;amp;= [\text{head}_1; \dots; \text{head}_h]\mathbf{W}^O \\
\text{where head}_i &amp;amp;= \text{Attention}(\mathbf{Q}\mathbf{W}^Q_i, \mathbf{K}\mathbf{W}^K_i, \mathbf{V}\mathbf{W}^V_i)
\end{aligned}\]

&lt;p&gt;where \(\mathbf{W}^Q_i\), \(\mathbf{W}^K_i\), \(\mathbf{W}^V_i\), and \(\mathbf{W}^O\) are parameter matrices to be learned.&lt;/p&gt;

&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;

&lt;p style=&quot;width: 60%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/transformer-encoder.png&quot; alt=&quot;Transformer encoder&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 15. The transformer’s encoder. (Image source: &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The encoder generates an attention-based representation with capability to locate a specific piece of information from a potentially infinitely-large context.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A stack of N=6 identical layers.&lt;/li&gt;
  &lt;li&gt;Each layer has a &lt;strong&gt;multi-head self-attention layer&lt;/strong&gt; and a simple position-wise &lt;strong&gt;fully connected feed-forward network&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Each sub-layer adopts a &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&lt;strong&gt;residual&lt;/strong&gt;&lt;/a&gt; connection and a layer &lt;strong&gt;normalization&lt;/strong&gt;.
All the sub-layers output data of the same dimension \(d_\text{model} = 512\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;

&lt;p style=&quot;width: 58%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/transformer-decoder.png&quot; alt=&quot;Transformer decoder&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 16. The transformer’s decoder. (Image source: &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The decoder is able to retrieval from the encoded representation.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A stack of N = 6 identical layers&lt;/li&gt;
  &lt;li&gt;Each layer has two sub-layers of multi-head attention mechanisms and one sub-layer of fully-connected feed-forward network.&lt;/li&gt;
  &lt;li&gt;Similar to the encoder, each sub-layer adopts a residual connection and a layer normalization.&lt;/li&gt;
  &lt;li&gt;The first multi-head attention sub-layer is &lt;strong&gt;modified&lt;/strong&gt; to prevent positions from attending to subsequent positions, as we don’t want to look into the future of the target sequence when predicting the current position.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;full-architecture&quot;&gt;Full Architecture&lt;/h3&gt;

&lt;p&gt;Finally here is the complete view of the transformer’s architecture:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Both the source and target sequences first go through embedding layers to produce data of the same dimension \(d_\text{model} =512\).&lt;/li&gt;
  &lt;li&gt;To preserve the position information, a sinusoid-wave-based positional encoding is applied and summed with the embedding output.&lt;/li&gt;
  &lt;li&gt;A softmax and linear layer are added to the final decoder output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/transformer.png&quot; alt=&quot;Transformer model&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em style=&quot;text-align: center;&quot;&gt;Fig. 17. The full model architecture of the transformer. (Image source: Fig 1 &amp;amp; 2 in &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Try to implement the transformer model is an interesting experience, here is mine: &lt;a href=&quot;https://github.com/lilianweng/transformer-tensorflow&quot;&gt;lilianweng/transformer-tensorflow&lt;/a&gt;. Read the comments in the code if you are interested.&lt;/p&gt;

&lt;h2 id=&quot;snail&quot;&gt;SNAIL&lt;/h2&gt;

&lt;p&gt;The transformer has no recurrent or convolutional structure, even with the positional encoding added to the embedding vector, the sequential order is only weakly incorporated. For problems sensitive to the positional dependency like &lt;a href=&quot;/blog&quot;&gt;reinforcement learning&lt;/a&gt;, this can be a big problem.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Simple Neural Attention &lt;a href=&quot;http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/&quot;&gt;Meta-Learner&lt;/a&gt;&lt;/strong&gt; (&lt;strong&gt;SNAIL&lt;/strong&gt;) (&lt;a href=&quot;http://metalearning.ml/papers/metalearn17_mishra.pdf&quot;&gt;Mishra et al., 2017&lt;/a&gt;) was developed partially to resolve the problem with &lt;a href=&quot;#full-architecture&quot;&gt;positioning&lt;/a&gt; in the transformer model by combining the self-attention mechanism in transformer with &lt;a href=&quot;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&quot;&gt;temporal convolutions&lt;/a&gt;. It has been demonstrated to be good at both supervised learning and reinforcement learning tasks.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/snail.png&quot; alt=&quot;SNAIL&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 18. SNAIL model architecture (Image source: &lt;a href=&quot;http://metalearning.ml/papers/metalearn17_mishra.pdf&quot;&gt;Mishra et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;SNAIL was born in the field of meta-learning, which is another big topic worthy of a post by itself. But in simple words, the meta-learning model is expected to be generalizable to novel, unseen tasks in the similar distribution. Read &lt;a href=&quot;http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/&quot;&gt;this&lt;/a&gt; nice introduction if interested.&lt;/p&gt;

&lt;h2 id=&quot;self-attention-gan&quot;&gt;Self-Attention GAN&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Self-Attention GAN&lt;/em&gt; (&lt;strong&gt;SAGAN&lt;/strong&gt;; &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;Zhang et al., 2018&lt;/a&gt;) adds self-attention layers into &lt;a href=&quot;/blog&quot;&gt;GAN&lt;/a&gt; to enable both the generator and the discriminator to better model relationships between spatial regions.&lt;/p&gt;

&lt;p&gt;The classic &lt;a href=&quot;https://arxiv.org/abs/1511.06434&quot;&gt;DCGAN&lt;/a&gt; (Deep Convolutional GAN) represents both discriminator and generator as multi-layer convolutional networks. However, the representation capacity of the network is restrained by the filter size, as the feature of one pixel is limited to a small local region. In order to connect regions far apart, the features have to be dilute through layers of convolutional operations and the dependencies are not guaranteed to be maintained.&lt;/p&gt;

&lt;p&gt;As the (soft) self-attention in the vision context is designed to explicitly learn the relationship between one pixel and all other positions, even regions far apart, it can easily capture global dependencies. Hence GAN equipped with self-attention is expected to &lt;em&gt;handle details better&lt;/em&gt;, hooray!&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/conv-vs-self-attention.png&quot; alt=&quot;Conv vs self-attention on images&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 19. Convolution operation and self-attention have access to regions of very different sizes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The SAGAN adopts the &lt;a href=&quot;https://arxiv.org/pdf/1711.07971.pdf&quot;&gt;non-local neural network&lt;/a&gt; to apply the attention computation. The convolutional image feature maps \(\mathbf{x}\) is branched out into three copies, corresponding to the concepts of &lt;a href=&quot;#key-value-and-query&quot;&gt;key, value, and query&lt;/a&gt; in the transformer:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Key: \(f(\mathbf{x}) = \mathbf{W}_f \mathbf{x}\)&lt;/li&gt;
  &lt;li&gt;Query: \(g(\mathbf{x}) = \mathbf{W}_g \mathbf{x}\)&lt;/li&gt;
  &lt;li&gt;Value: \(h(\mathbf{x}) = \mathbf{W}_h \mathbf{x}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then we apply the dot-product attention to output the self-attention feature maps:&lt;/p&gt;

\[\begin{aligned}
\alpha_{i,j} &amp;amp;= \text{softmax}(f(\mathbf{x}_i)^\top g(\mathbf{x}_j)) \\
\mathbf{o}_j &amp;amp;= \mathbf{W}_v \Big( \sum_{i=1}^N \alpha_{i,j} h(\mathbf{x}_i) \Big)
\end{aligned}\]

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/SAGAN.png&quot; alt=&quot;SAGAN&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 20. The self-attention mechanism in SAGAN. (Image source: Fig. 2 in &lt;a href=&quot;https://arxiv.org/abs/1805.08318&quot;&gt;Zhang et al., 2018&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note that \(\alpha_{i,j}\) is one entry in the attention map, indicating how much attention the model should pay to the \(i\)-th position when synthesizing the \(j\)-th location. \(\mathbf{W}_f\), \(\mathbf{W}_g\), and \(\mathbf{W}_h\) are all 1x1 convolution filters. If you feel that 1x1 conv sounds like a weird concept (i.e., isn’t it just to multiply the whole feature map with one number?), watch this short &lt;a href=&quot;https://www.coursera.org/lecture/convolutional-neural-networks/networks-in-networks-and-1x1-convolutions-ZTb8x&quot;&gt;tutorial&lt;/a&gt; by Andrew Ng. The output \(\mathbf{o}_j\) is a column vector of the final output \(\mathbf{o}= (\mathbf{o}_1, \mathbf{o}_2, \dots, \mathbf{o}_j, \dots, \mathbf{o}_N)\).&lt;/p&gt;

&lt;p&gt;Furthermore, the output of the attention layer is multiplied by a scale parameter and added back to the original input feature map:&lt;/p&gt;

\[\mathbf{y} = \mathbf{x}_i + \gamma \mathbf{o}_i\]

&lt;p&gt;While the scaling parameter \(\gamma\) is increased gradually from 0 during the training, the network is configured to first rely on the cues in the local regions and then gradually learn to assign more weight to the regions that are further away.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/transformer/SAGAN-examples.png&quot; alt=&quot;SAGAN examples&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Fig. 21. 128×128 example images generated by SAGAN for different classes. (Image source: Partial Fig. 6 in &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;Zhang et al., 2018&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Cited as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{weng2018attention,
  title   = &quot;Attention? Attention!&quot;,
  author  = &quot;Weng, Lilian&quot;,
  journal = &quot;lilianweng.github.io/lil-log&quot;,
  year    = &quot;2018&quot;,
  url     = &quot;http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/&quot;&gt;“Attention and Memory in Deep Learning and NLP.”&lt;/a&gt; - Jan 3, 2016 by Denny Britz&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://github.com/tensorflow/nmt&quot;&gt;“Neural Machine Translation (seq2seq) Tutorial”&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;“Neural machine translation by jointly learning to align and translate.”&lt;/a&gt; ICLR 2015.&lt;/p&gt;

&lt;p&gt;[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;“Show, attend and tell: Neural image caption generation with visual attention.”&lt;/a&gt; ICML, 2015.&lt;/p&gt;

&lt;p&gt;[5] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. &lt;a href=&quot;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;“Sequence to sequence learning with neural networks.”&lt;/a&gt; NIPS 2014.&lt;/p&gt;

&lt;p&gt;[6] Thang Luong, Hieu Pham, Christopher D. Manning. &lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;“Effective Approaches to Attention-based Neural Machine Translation.”&lt;/a&gt; EMNLP 2015.&lt;/p&gt;

&lt;p&gt;[7] Denny Britz, Anna Goldie, Thang Luong, and Quoc Le. &lt;a href=&quot;https://arxiv.org/abs/1703.03906&quot;&gt;“Massive exploration of neural machine translation architectures.”&lt;/a&gt; ACL 2017.&lt;/p&gt;

&lt;p&gt;[8] Ashish Vaswani, et al. &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;“Attention is all you need.”&lt;/a&gt; NIPS 2017.&lt;/p&gt;

&lt;p&gt;[9] Jianpeng Cheng, Li Dong, and Mirella Lapata. &lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;“Long short-term memory-networks for machine reading.”&lt;/a&gt; EMNLP 2016.&lt;/p&gt;

&lt;p&gt;[10] Xiaolong Wang, et al. &lt;a href=&quot;https://arxiv.org/pdf/1711.07971.pdf&quot;&gt;“Non-local Neural Networks.”&lt;/a&gt; CVPR 2018&lt;/p&gt;

&lt;p&gt;[11] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;“Self-Attention Generative Adversarial Networks.”&lt;/a&gt; arXiv preprint arXiv:1805.08318 (2018).&lt;/p&gt;

&lt;p&gt;[12] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. &lt;a href=&quot;https://arxiv.org/abs/1707.03141&quot;&gt;“A simple neural attentive meta-learner.”&lt;/a&gt; ICLR 2018.&lt;/p&gt;

&lt;p&gt;[13] &lt;a href=&quot;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&quot;&gt;“WaveNet: A Generative Model for Raw Audio”&lt;/a&gt; - Sep 8, 2016 by DeepMind.&lt;/p&gt;

&lt;p&gt;[14]  Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. &lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;“Pointer networks.”&lt;/a&gt; NIPS 2015.&lt;/p&gt;

&lt;p&gt;[15] Alex Graves, Greg Wayne, and Ivo Danihelka. &lt;a href=&quot;https://arxiv.org/abs/1410.5401&quot;&gt;“Neural turing machines.”&lt;/a&gt; arXiv preprint arXiv:1410.5401 (2014).&lt;/p&gt;</content><author><name>Lilian</name></author><category term="attention" /><category term="transformer" /><category term="rnn" /><summary type="html">Source : Lilian</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/blog/transformer.png" /><media:content medium="image" url="http://localhost:4000/blog/transformer.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Understand Convolution</title><link href="http://localhost:4000/blog/2014/07/08/understand-convolution.html" rel="alternate" type="text/html" title="Understand Convolution" /><published>2014-07-08T18:04:17+07:00</published><updated>2014-07-08T18:04:17+07:00</updated><id>http://localhost:4000/blog/2014/07/08/understand-convolution</id><content type="html" xml:base="http://localhost:4000/blog/2014/07/08/understand-convolution.html">&lt;p&gt;Source : &lt;a href=&quot;https://colah.github.io/posts/2014-07-Understanding-Convolutions/&quot;&gt;https://colah.github.io/posts/2014-07-Understanding-Convolutions/&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#lessons-from-a-dropped-ball&quot; id=&quot;markdown-toc-lessons-from-a-dropped-ball&quot;&gt;Lessons from a Dropped Ball&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#visualizing-convolutions&quot; id=&quot;markdown-toc-visualizing-convolutions&quot;&gt;Visualizing Convolutions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#higher-dimensional-convolutions&quot; id=&quot;markdown-toc-higher-dimensional-convolutions&quot;&gt;Higher Dimensional Convolutions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#convolutional-neural-networks&quot; id=&quot;markdown-toc-convolutional-neural-networks&quot;&gt;Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#acknowledgments&quot; id=&quot;markdown-toc-acknowledgments&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;lessons-from-a-dropped-ball&quot;&gt;Lessons from a Dropped Ball&lt;/h1&gt;

&lt;p&gt;Imagine we drop a ball from some height onto the ground, where it only has one dimension of motion.
&lt;em&gt;How likely is it that a ball will go a distance $c$ if you drop it and then drop it again from above the point at which it landed?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let’s break this down.
After the first drop, it will land $a$ units away from the starting point with probability $f(a)$, where $f$ is the probability distribution.&lt;/p&gt;

&lt;p&gt;Now after this first drop, we pick the ball up and drop it from another height above the point where it first landed.
The probability of the ball rolling $b$ units away from the new starting point is $g(b)$, where $g$ may be a different probability distribution if it’s dropped from a different height.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-fagb.png&quot; alt=&quot;ProbConv-fagb&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we fix the result of the first drop so we know the ball went distance $a$, for the ball to go a total distance $c$,
the distance traveled in the second drop is also fixed at $b$, where $a + b = c$.
So the probability of this happening is simply $f(a) ⋅ g(b)$.&lt;sup&gt;&lt;a href=&quot;#031a:fn:1&quot; class=&quot;footnote&quot; id=&quot;031a:fn-back:1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Let’s think about this with a specific discrete example.
We want the total distance $c$ to be $3$. If the first time it rolls, $a=2$, the second time it must roll $b=1$ in order to reach our total distance $a+b=3$.
The probability of this is $f(2)⋅g(1)$.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-split-21.png&quot; alt=&quot;ProbConv-split-21&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, this isn’t the only way we could get to a total distance of 3.
The ball could roll 1 units the first time, and 2 the second. 
Or 0 units the first time and all 3 the second. 
It could go any $a$ and $b$, as long as they add to 3.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-splits-12-03.png&quot; alt=&quot;ProbConv-splits-12-03&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The probabilities are $f(1)⋅g(2)$ and $f(0)⋅g(3)$, respectively.&lt;/p&gt;

&lt;p&gt;In order to find the total likelihood of the ball reaching a total distance of $c$, we can’t consider only one possible way of reaching $c$.
Instead, we consider &lt;em&gt;all the possible ways&lt;/em&gt; of partitioning $c$ into two drops $a$ and $b$ and sum over the &lt;em&gt;probability of each way.&lt;/em&gt;&lt;/p&gt;

\[...  f(0)⋅g(3) + f(1)⋅g(2) + f(2)⋅g(1)  ...\]

&lt;p&gt;We already know that the probability for each case of a+b=c is simply $f(a)⋅g(b)$.
So, summing over every solution to $a+b=c$, we can denote the total likelihood as:&lt;/p&gt;

\[\sum_{a+b=c} f(a)⋅g(b)\]

&lt;p&gt;Turns out, we’re doing a convolution! In particular, the convolution of $f$ and $g$, evluated at $c$ is defined:&lt;/p&gt;

\[(f∗g)(c)=\sum_{a+b=c} f(a)⋅g(b)\]

&lt;p&gt;If we substitute $b=c−a$, we get:&lt;/p&gt;

\[(f∗g)(c)=\sum_{a} f(a)⋅g(c−a)\]

&lt;p&gt;This is the standard definition&lt;sup&gt;&lt;a href=&quot;#031a:fn:2&quot; class=&quot;footnote&quot; id=&quot;031a:fn-back:2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; of convolution.&lt;/p&gt;

&lt;p&gt;To make this a bit more concrete, we can think about this in terms of positions the ball might land.
After the first drop, it will land at an intermediate position $a$ with probability $f(a)$.
If it lands at $a$, it has probability $g(c−a)$ of landing at $a$ position $c$.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-OnePath.png&quot; alt=&quot;ProbConv-OnePath&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To get the convolution, we consider all intermediate positions.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-SumPaths.png&quot; alt=&quot;ProbConv-SumPaths&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;visualizing-convolutions&quot;&gt;Visualizing Convolutions&lt;/h1&gt;

&lt;p&gt;There’s a very nice trick that helps one think about convolutions more easily.&lt;/p&gt;

&lt;p&gt;First, an observation.
Suppose the probability that a ball lands a certain distance $x$ from where it started is $f(x)$.
Then, afterwards, the probability that it started a distance $x$ from where it landed is $f(−x)$.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-Reverse.png&quot; alt=&quot;ProbConv-Reverse&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we know the ball lands at a position c after the second drop, what is the probability that the previous position was $a$?&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-BackProb.png&quot; alt=&quot;ProbConv-BackProb&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So the probability that the previous position was $a$ is $g(−(a−c))=g(c−a)$.&lt;/p&gt;

&lt;p&gt;Now, consider the probability each intermediate position contributes to the ball finally landing at $c$.
We know the probability of the first drop putting the ball into the intermediate position $a$ is $f(a)$.
We also know that the probability of it having been in $a$, if it lands at $c$ is $g(c−a)$.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-Intermediate.png&quot; alt=&quot;ProbConv-Intermediate&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Summing over the as, we get the convolution.&lt;/p&gt;

&lt;p&gt;The advantage of this approach is that it allows us to visualize the evaluation of a convolution at a value $c$ in a single picture. By shifting the bottom half around, we can evaluate the convolution at other values of $c$.
This allows us to understand the convolution as a whole.&lt;/p&gt;

&lt;p&gt;For example, we can see that it peaks when the distributions align.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-Intermediate-Align.png&quot; alt=&quot;ProbConv-Intermediate-Align&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And shrinks as the intersection between the distributions gets smaller.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-Intermediate-Sep.png&quot; alt=&quot;ProbConv-Intermediate-Sep&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By using this trick in an animation, it really becomes possible to visually understand convolutions.&lt;/p&gt;

&lt;p&gt;Below, we’re able to visualize the convolution of two box functions:&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/Wiki-BoxConvAnim.gif&quot; alt=&quot;Wiki-BoxConvAnim&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;center&quot;&gt;&lt;em&gt;From Wikipedia&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Armed with this perspective, a lot of things become more intuitive.&lt;/p&gt;

&lt;p&gt;Let’s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy.&lt;/p&gt;

&lt;h1 id=&quot;higher-dimensional-convolutions&quot;&gt;Higher Dimensional Convolutions&lt;/h1&gt;

&lt;p&gt;Convolutions are an extremely general idea. We can also use them in a higher number of dimensions.&lt;/p&gt;

&lt;p&gt;Let’s consider our example of a falling ball again. Now, as it falls, it’s position shifts not only in one dimension, but in two.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/ProbConv-TwoDim.png&quot; alt=&quot;ProbConv-TwoDim&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Convolution is the same as before:
\((f∗g)(c)=\sum_{a+b=c} f(a)⋅g(b)\)&lt;/p&gt;

&lt;p&gt;Except, now $a$, $b$ and $c$ are vectors. To be more explicit,&lt;/p&gt;

\[(f∗g)(c_1,c_2) = \sum_{
    \begin{matrix} a_1+b_1=c_1 \\
    a_2+b_2=c_2
    \end{matrix}} f(a_1,a_2)⋅g(b_1,b_2)\]

&lt;p&gt;Just like one-dimensional convolutions, we can think of a two-dimensional convolution as sliding one function on top of another, multiplying and adding.&lt;/p&gt;

&lt;p&gt;One common application of this is image processing. We can think of images as two-dimensional functions. Many important image transformations are convolutions where you convolve the image function with a very small, local function called a “kernel.”&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/RiverTrain-ImageConvDiagram.png&quot; alt=&quot;RiverTrain-ImageConvDiagram&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;center&quot;&gt;&lt;em&gt;From the &lt;a href=&quot;http://intellabs.github.io/RiverTrail/tutorial/&quot;&gt;River Trail documentation&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The kernel slides to every position of the image and computes a new pixel as a weighted sum of the pixels it floats over.&lt;/p&gt;

&lt;p&gt;For example, by averaging a $3 \times 3$ box of pixels, we can blur an image. To do this, our kernel takes the value $1/9$ on each pixel in the box,&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/Gimp-Blur.png&quot; alt=&quot;Gimp-Blur&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;center&quot;&gt;&lt;em&gt;Derived from the Gimp documentation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We can also detect edges by taking the values $−1$ and $1$ on two adjacent pixels, and zero everywhere else. That is, we subtract two adjacent pixels. When side by side pixels are similar, this is gives us approximately zero. On edges, however, adjacent pixels are very different in the direction perpendicular to the edge.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/Gimp-Edge.png&quot; alt=&quot;Gimp-Edge&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;center&quot;&gt;&lt;em&gt;Derived from the Gimp documentation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The gimp documentation has many other examples.&lt;/p&gt;

&lt;h1 id=&quot;convolutional-neural-networks&quot;&gt;Convolutional Neural Networks&lt;/h1&gt;

&lt;p&gt;So, how does convolution relate to convolutional neural networks?&lt;/p&gt;

&lt;p&gt;Consider a 1-dimensional convolutional layer with inputs ${x_n}$ and outputs ${y_n}$, like we discussed in the previous post:&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/Conv-9-Conv2-XY.png&quot; alt=&quot;Conv-9-Conv2-XY&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we observed, we can describe the outputs in terms of the inputs:&lt;/p&gt;

\[y_{n} = A(x_{n},x_{n+1},...)\]

&lt;p&gt;Generally, $A$ would be multiple neurons. But suppose it is a single neuron for a moment.&lt;/p&gt;

&lt;p&gt;Recall that a typical neuron in a neural network is described by:&lt;/p&gt;

\[\sigma (w_0 x_0 + w_1 x_1 + w_2 x_2 ... +b)\]

&lt;p&gt;Where $x_0, x_1…$ are the inputs. The weights, $w0$, $w1$, … describe how the neuron connects to its inputs.
A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to.
The weights are the heart of the neuron, controlling its behavior&lt;sup&gt;&lt;a href=&quot;#031a:fn:3&quot; class=&quot;footnote&quot; id=&quot;031a:fn-back:3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.
Saying that multiple neurons are identical is the same thing as saying that the weights are the same.&lt;/p&gt;

&lt;p&gt;It’s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.&lt;/p&gt;

&lt;p&gt;Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, $W$:&lt;/p&gt;

\[y=\sigma (Wx+b)\]

&lt;p&gt;For example, we get:&lt;/p&gt;

\[y_0=\sigma (W_{0,0} x_0 + W_{0,1} x_1 + W_{0,2} x_2...)\]

\[y1=\sigma (W_{1,0} x_0 + W_{1,1} x{1}+W_{1,2} x_2...)\]

&lt;p&gt;Each row of the matrix describes the weights connecting a neuron to its inputs.&lt;/p&gt;

&lt;p&gt;Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/Conv-9-Conv2-XY-W.png&quot; alt=&quot;Conv-9-Conv2-XY-W&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Which corresponds to the equations:&lt;/p&gt;

\[y_0=sigma(W_0 x_0 + W_1 x_1 − b)

y_1=sigma(W_0 x_1 + W_1 x_2 − b)\]

&lt;p&gt;So while, normally, a weight matrix connects every input to every neuron with different weights:&lt;/p&gt;

\[W=\begin{bmatrix}
    W_{0,0} &amp;amp; W_{1,0} &amp;amp; W_{2,0} &amp;amp; W_{3,0} &amp;amp; ...\\
    W_{0,1} &amp;amp; W_{1,1} &amp;amp; W_{2,1} &amp;amp; W_{3,1} &amp;amp; ...\\
    W_{0,2} &amp;amp; W_{1,2} &amp;amp; W_{2,2} &amp;amp; W_{3,2} &amp;amp; ...\\
    W_{0,3} &amp;amp; W_{1,3} &amp;amp; W_{2,3} &amp;amp; W_{3,3} &amp;amp; ...\\
    ... &amp;amp; ... &amp;amp; ... &amp;amp; ... &amp;amp; ... &amp;amp;
\end{bmatrix}\]

&lt;p&gt;The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons don’t connect to many possible inputs, there’s lots of zeros.&lt;/p&gt;

\[W=\begin{bmatrix}
    w_{0} &amp;amp; w_{1} &amp;amp; 0 &amp;amp; 0 &amp;amp; ...\\
    0 &amp;amp; w_{0} &amp;amp; w_{1} &amp;amp; 0 &amp;amp; ...\\
    0 &amp;amp; 0 &amp;amp; w_{0} &amp;amp; w_{1} &amp;amp; ...\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; w_{0} &amp;amp; ...\\
    ... &amp;amp; ... &amp;amp; ... &amp;amp; ... &amp;amp; ... &amp;amp;
\end{bmatrix}\]

&lt;p&gt;Multiplying by the above matrix is the same thing as convolving with $[…0,w_1,w_0,0…]$. The function sliding to different positions corresponds to having neurons at those positions.&lt;/p&gt;

&lt;p&gt;What about two-dimensional convolutional layers?&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/blog/assets/images/2014-07-08-understand-convolution/Conv2-5x5-Conv2-XY.png&quot; alt=&quot;Conv2-5x5-Conv2-XY&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.&lt;/p&gt;

&lt;p&gt;Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;We introduced a lot of mathematical machinery in this blog post, but it may not be obvious what we gained. Convolution is obviously a useful tool in probability theory and computer graphics, but what do we gain from phrasing convolutional neural networks in terms of convolutions?&lt;/p&gt;

&lt;p&gt;The first advantage is that we have some very powerful language for describing the wiring of networks. The examples we’ve dealt with so far haven’t been complicated enough for this benefit to become clear, but convolutions will allow us to get rid of huge amounts of unpleasant book-keeping for us.&lt;/p&gt;

&lt;p&gt;Secondly, convolutions come with significant implementational advantages.
Many libraries provide highly efficient convolution routines.
Further, while convolution naively appears to be an $O(n2)$ operation, using some rather deep mathematical insights, 
it is possible to create a $O(nlog(n))$ implementation. We will discuss this in much greater detail in a future post.&lt;/p&gt;

&lt;p&gt;In fact, the use of highly-efficient parallel convolution implementations on GPUs has been essential to recent progress in computer vision.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h1&gt;

&lt;ol class=&quot;footnotelist&quot;&gt;
&lt;li id=&quot;031a:fn:1&quot; class=&quot;footnotebody&quot; value=&quot;1&quot;&gt;
        We want the probability of the ball rolling a units the first time and also rolling b units the second time. The distributions $P(A)=f(a)$ and $P(b)=g(b)$ are independent, with both distributions centered at 0. So  $P(a,b)=P(a)∗P(b)=f(a)⋅g(b)$
   &lt;a href=&quot;#031a:fn-back:1&quot; class=&quot;backlink&quot;&gt;⏎&lt;/a&gt;&lt;/li&gt;
&lt;li id=&quot;031a:fn:2&quot; class=&quot;footnotebody&quot; value=&quot;2&quot;&gt;
      The non-standard definition, which I haven’t previously seen, seems to have a lot of benefits. In future posts, we will find this definition very helpful because it lends itself to generalization to new algebraic structures. But it also has the advantage that it makes a lot of algebraic properties of convolutions really obvious.
      &lt;br /&gt;
      For example, convolution is a commutative operation. That is, $f∗g=g∗f$. Why?
      $$
      \sum_{a+b=c} f(a)⋅g(b)  = \sum_{b+a=c} g(b)⋅f(a)
      $$
      Convolution is also associative. That is, $(f∗g)∗h=f∗(g∗h)$. Why?
      $$
      \sum_{(a+b)+c=d} (f(a)⋅g(b))⋅h(c)  = \sum_{a+(b+c)=d} f(a)⋅(g(b)⋅h(c))
      $$
      
   &lt;a href=&quot;#031a:fn-back:2&quot; class=&quot;backlink&quot;&gt;⏎&lt;/a&gt;&lt;/li&gt;
&lt;li id=&quot;031a:fn:3&quot; class=&quot;footnotebody&quot; value=&quot;3&quot;&gt;
     There’s also the bias, which is the “threshold” for whether the neuron fires, but it’s much simpler and I don’t want to clutter this section talking about it.
  &lt;a href=&quot;#031a:fn-back:3&quot; class=&quot;backlink&quot;&gt;⏎&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Minh-Thanh Hoang</name></author><category term="deep" /><category term="learning" /><summary type="html">Source : https://colah.github.io/posts/2014-07-Understanding-Convolutions/</summary></entry></feed>